{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning Model for Images.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTntG28PxoNd"
      },
      "source": [
        "# Eduonix Assignment Project\r\n",
        "\r\n",
        "## Deep Learning Model for Images\r\n",
        "\r\n",
        "\r\n",
        "*By  Mohammed Azharuddin*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-n3TLq2ySqq"
      },
      "source": [
        "Project List :\r\n",
        "\r\n",
        "You are given the CIFAR-10 dataset http://www.cs.toronto.edu/~kriz/cifar.html \r\n",
        "\r\n",
        "and you want to develop a deep learning classifier that automatically classifies the images based on their appearance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdYB1CBfwzUm"
      },
      "source": [
        "#importing the libraties\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "import tensorflow as tf\r\n",
        "import keras\r\n",
        "from keras.datasets import cifar10\r\n",
        "from __future__ import print_function\r\n",
        "from keras.preprocessing.image import ImageDataGenerator\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\r\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\r\n",
        "from keras import optimizers\r\n",
        "from keras.layers.core import Lambda\r\n",
        "from keras import backend as K\r\n",
        "from keras import regularizers\r\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e_N9MrQwzaQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b01cf88-67ad-4af0-b013-5629770c2b6b"
      },
      "source": [
        "# Extracting the data from keras.dataset cifar10\r\n",
        "# Dataset contains 60000 32_x32 color images in 10 classes. \r\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TUzjcSNwzei",
        "outputId": "c44d2eef-0bd7-48eb-fb12-7790fbac43a2"
      },
      "source": [
        "class cifar10vgg:\r\n",
        "    def __init__(self,train=True):\r\n",
        "        self.num_classes = 10\r\n",
        "        self.weight_decay = 0.0005\r\n",
        "        self.x_shape = [32,32,3]\r\n",
        "\r\n",
        "        self.model = self.build_model()\r\n",
        "        if train:\r\n",
        "            self.model = self.train(self.model)\r\n",
        "        else:\r\n",
        "            self.model.load_weights('cifar10vgg.h5')\r\n",
        "\r\n",
        "\r\n",
        "    def build_model(self):\r\n",
        "        # Build the network of vgg for 10 classes with massive dropout and weight decay as described in the paper.\r\n",
        "\r\n",
        "        model = Sequential()\r\n",
        "        weight_decay = self.weight_decay\r\n",
        "\r\n",
        "        model.add(Conv2D(64, (3, 3), padding='same',\r\n",
        "                         input_shape=self.x_shape,kernel_regularizer=regularizers.l2(weight_decay)))\r\n",
        "        model.add(Activation('relu'))\r\n",
        "        model.add(BatchNormalization())\r\n",
        "        model.add(Dropout(0.3))\r\n",
        "\r\n",
        "        model.add(Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\r\n",
        "        model.add(Activation('relu'))\r\n",
        "        model.add(BatchNormalization())\r\n",
        "\r\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "\r\n",
        "        model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\r\n",
        "        model.add(Activation('relu'))\r\n",
        "        model.add(BatchNormalization())\r\n",
        "        model.add(Dropout(0.4))\r\n",
        "\r\n",
        "        model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\r\n",
        "        model.add(Activation('relu'))\r\n",
        "        model.add(BatchNormalization())\r\n",
        "\r\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "\r\n",
        "        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\r\n",
        "        model.add(Activation('relu'))\r\n",
        "        model.add(BatchNormalization())\r\n",
        "        model.add(Dropout(0.4))\r\n",
        "\r\n",
        "        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\r\n",
        "        model.add(Activation('relu'))\r\n",
        "        model.add(BatchNormalization())\r\n",
        "        model.add(Dropout(0.4))\r\n",
        "\r\n",
        "        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\r\n",
        "        model.add(Activation('relu'))\r\n",
        "        model.add(BatchNormalization())\r\n",
        "\r\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "\r\n",
        "\r\n",
        "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\r\n",
        "        model.add(Activation('relu'))\r\n",
        "        model.add(BatchNormalization())\r\n",
        "        model.add(Dropout(0.4))\r\n",
        "\r\n",
        "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\r\n",
        "        model.add(Activation('relu'))\r\n",
        "        model.add(BatchNormalization())\r\n",
        "        model.add(Dropout(0.4))\r\n",
        "\r\n",
        "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\r\n",
        "        model.add(Activation('relu'))\r\n",
        "        model.add(BatchNormalization())\r\n",
        "\r\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "\r\n",
        "\r\n",
        "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\r\n",
        "        model.add(Activation('relu'))\r\n",
        "        model.add(BatchNormalization())\r\n",
        "        model.add(Dropout(0.4))\r\n",
        "\r\n",
        "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\r\n",
        "        model.add(Activation('relu'))\r\n",
        "        model.add(BatchNormalization())\r\n",
        "        model.add(Dropout(0.4))\r\n",
        "\r\n",
        "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\r\n",
        "        model.add(Activation('relu'))\r\n",
        "        model.add(BatchNormalization())\r\n",
        "\r\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "        model.add(Dropout(0.5))\r\n",
        "\r\n",
        "        model.add(Flatten())\r\n",
        "        model.add(Dense(512,kernel_regularizer=regularizers.l2(weight_decay)))\r\n",
        "        model.add(Activation('relu'))\r\n",
        "        model.add(BatchNormalization())\r\n",
        "\r\n",
        "        model.add(Dropout(0.5))\r\n",
        "        model.add(Dense(self.num_classes))\r\n",
        "        model.add(Activation('softmax'))\r\n",
        "        return model\r\n",
        "\r\n",
        "\r\n",
        "    def normalize(self,X_train,X_test):\r\n",
        "        #this function normalize inputs for zero mean and unit variance\r\n",
        "        # it is used when training a model.\r\n",
        "        # Input: training set and test set\r\n",
        "        # Output: normalized training set and test set according to the trianing set statistics.\r\n",
        "        mean = np.mean(X_train,axis=(0,1,2,3))\r\n",
        "        std = np.std(X_train, axis=(0, 1, 2, 3))\r\n",
        "        X_train = (X_train-mean)/(std+1e-7)\r\n",
        "        X_test = (X_test-mean)/(std+1e-7)\r\n",
        "        return X_train, X_test\r\n",
        "\r\n",
        "    def normalize_production(self,x):\r\n",
        "        #this function is used to normalize instances in production according to saved training set statistics\r\n",
        "        # Input: X - a training set\r\n",
        "        # Output X - a normalized training set according to normalization constants.\r\n",
        "\r\n",
        "        #these values produced during first training and are general for the standard cifar10 training set normalization\r\n",
        "        mean = 120.707\r\n",
        "        std = 64.15\r\n",
        "        return (x-mean)/(std+1e-7)\r\n",
        "\r\n",
        "    def predict(self,x,normalize=True,batch_size=50):\r\n",
        "        if normalize:\r\n",
        "            x = self.normalize_production(x)\r\n",
        "        return self.model.predict(x,batch_size)\r\n",
        "\r\n",
        "    def train(self,model):\r\n",
        "\r\n",
        "        #training parameters\r\n",
        "        batch_size = 128\r\n",
        "        maxepoches = 250\r\n",
        "        learning_rate = 0.1\r\n",
        "        lr_decay = 1e-6\r\n",
        "        lr_drop = 20\r\n",
        "        # The data, shuffled and split between train and test sets:\r\n",
        "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\n",
        "        x_train = x_train.astype('float32')\r\n",
        "        x_test = x_test.astype('float32')\r\n",
        "        x_train, x_test = self.normalize(x_train, x_test)\r\n",
        "\r\n",
        "        y_train = keras.utils.to_categorical(y_train, self.num_classes)\r\n",
        "        y_test = keras.utils.to_categorical(y_test, self.num_classes)\r\n",
        "\r\n",
        "        def lr_scheduler(epoch):\r\n",
        "            return learning_rate * (0.5 ** (epoch // lr_drop))\r\n",
        "        reduce_lr = keras.callbacks.LearningRateScheduler(lr_scheduler)\r\n",
        "\r\n",
        "        #data augmentation\r\n",
        "        datagen = ImageDataGenerator(\r\n",
        "            featurewise_center=False,  # set input mean to 0 over the dataset\r\n",
        "            samplewise_center=False,  # set each sample mean to 0\r\n",
        "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\r\n",
        "            samplewise_std_normalization=False,  # divide each input by its std\r\n",
        "            zca_whitening=False,  # apply ZCA whitening\r\n",
        "            rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\r\n",
        "            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\r\n",
        "            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\r\n",
        "            horizontal_flip=True,  # randomly flip images\r\n",
        "            vertical_flip=False)  # randomly flip images\r\n",
        "        # (std, mean, and principal components if ZCA whitening is applied).\r\n",
        "        datagen.fit(x_train)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        #optimization details\r\n",
        "        sgd = optimizers.SGD(lr=learning_rate, decay=lr_decay, momentum=0.9, nesterov=True)\r\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy'])\r\n",
        "\r\n",
        "\r\n",
        "        # training process in a for loop with learning rate drop every 25 epoches.\r\n",
        "\r\n",
        "        historytemp = model.fit_generator(datagen.flow(x_train, y_train,\r\n",
        "                                         batch_size=batch_size),\r\n",
        "                            steps_per_epoch=x_train.shape[0] // batch_size,\r\n",
        "                            epochs=maxepoches,\r\n",
        "                            validation_data=(x_test, y_test),callbacks=[reduce_lr],verbose=1)\r\n",
        "        model.save_weights('cifar10vgg.h5')\r\n",
        "        return model\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "\r\n",
        "\r\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\n",
        "    x_train = x_train.astype('float32')\r\n",
        "    x_test = x_test.astype('float32')\r\n",
        "\r\n",
        "    y_train = keras.utils.to_categorical(y_train, 10)\r\n",
        "    y_test = keras.utils.to_categorical(y_test, 10)\r\n",
        "\r\n",
        "    model = cifar10vgg()\r\n",
        "\r\n",
        "    predicted_x = model.predict(x_test)\r\n",
        "    residuals = np.argmax(predicted_x,1)!=np.argmax(y_test,1)\r\n",
        "\r\n",
        "    loss = sum(residuals)/len(residuals)\r\n",
        "    print(\"the validation 0/1 loss is: \",loss)\r\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "390/390 [==============================] - 43s 85ms/step - loss: 21.7564 - accuracy: 0.1391 - val_loss: 14.4448 - val_accuracy: 0.1494\n",
            "Epoch 2/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 12.1917 - accuracy: 0.2696 - val_loss: 7.9448 - val_accuracy: 0.1564\n",
            "Epoch 3/250\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 6.4821 - accuracy: 0.3663 - val_loss: 4.6727 - val_accuracy: 0.2974\n",
            "Epoch 4/250\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 3.8775 - accuracy: 0.4346 - val_loss: 2.9899 - val_accuracy: 0.4261\n",
            "Epoch 5/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 2.6012 - accuracy: 0.5083 - val_loss: 2.2382 - val_accuracy: 0.5076\n",
            "Epoch 6/250\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 1.9941 - accuracy: 0.5716 - val_loss: 1.7176 - val_accuracy: 0.6218\n",
            "Epoch 7/250\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 1.7195 - accuracy: 0.6128 - val_loss: 1.5431 - val_accuracy: 0.6622\n",
            "Epoch 8/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.5793 - accuracy: 0.6511 - val_loss: 1.4184 - val_accuracy: 0.7042\n",
            "Epoch 9/250\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 1.5083 - accuracy: 0.6725 - val_loss: 1.4638 - val_accuracy: 0.6980\n",
            "Epoch 10/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.4922 - accuracy: 0.6881 - val_loss: 1.5481 - val_accuracy: 0.6732\n",
            "Epoch 11/250\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 1.4639 - accuracy: 0.7020 - val_loss: 1.5943 - val_accuracy: 0.6698\n",
            "Epoch 12/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.4618 - accuracy: 0.7111 - val_loss: 1.8096 - val_accuracy: 0.6410\n",
            "Epoch 13/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.4590 - accuracy: 0.7180 - val_loss: 1.4379 - val_accuracy: 0.7261\n",
            "Epoch 14/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 1.4594 - accuracy: 0.7251 - val_loss: 1.3695 - val_accuracy: 0.7495\n",
            "Epoch 15/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.4601 - accuracy: 0.7263 - val_loss: 1.3907 - val_accuracy: 0.7537\n",
            "Epoch 16/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.4568 - accuracy: 0.7338 - val_loss: 1.5946 - val_accuracy: 0.6935\n",
            "Epoch 17/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.4757 - accuracy: 0.7321 - val_loss: 1.4430 - val_accuracy: 0.7436\n",
            "Epoch 18/250\n",
            "390/390 [==============================] - 31s 81ms/step - loss: 1.4676 - accuracy: 0.7367 - val_loss: 1.3806 - val_accuracy: 0.7716\n",
            "Epoch 19/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.4787 - accuracy: 0.7376 - val_loss: 1.4612 - val_accuracy: 0.7492\n",
            "Epoch 20/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.4887 - accuracy: 0.7385 - val_loss: 1.4272 - val_accuracy: 0.7660\n",
            "Epoch 21/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.3716 - accuracy: 0.7768 - val_loss: 1.2331 - val_accuracy: 0.7961\n",
            "Epoch 22/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.2339 - accuracy: 0.7911 - val_loss: 1.2437 - val_accuracy: 0.7747\n",
            "Epoch 23/250\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 1.1930 - accuracy: 0.7907 - val_loss: 1.2731 - val_accuracy: 0.7639\n",
            "Epoch 24/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.1917 - accuracy: 0.7902 - val_loss: 1.2343 - val_accuracy: 0.7783\n",
            "Epoch 25/250\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 1.1825 - accuracy: 0.7959 - val_loss: 1.0709 - val_accuracy: 0.8344\n",
            "Epoch 26/250\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 1.1817 - accuracy: 0.7960 - val_loss: 1.1602 - val_accuracy: 0.8064\n",
            "Epoch 27/250\n",
            "390/390 [==============================] - 31s 81ms/step - loss: 1.1888 - accuracy: 0.7930 - val_loss: 1.1165 - val_accuracy: 0.8212\n",
            "Epoch 28/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.1897 - accuracy: 0.7990 - val_loss: 1.1681 - val_accuracy: 0.8027\n",
            "Epoch 29/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 1.1789 - accuracy: 0.8060 - val_loss: 1.0808 - val_accuracy: 0.8373\n",
            "Epoch 30/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.1952 - accuracy: 0.7996 - val_loss: 1.1347 - val_accuracy: 0.8226\n",
            "Epoch 31/250\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 1.1950 - accuracy: 0.8035 - val_loss: 1.1638 - val_accuracy: 0.8136\n",
            "Epoch 32/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.2036 - accuracy: 0.8034 - val_loss: 1.1707 - val_accuracy: 0.8134\n",
            "Epoch 33/250\n",
            "390/390 [==============================] - 31s 81ms/step - loss: 1.2031 - accuracy: 0.8015 - val_loss: 1.1493 - val_accuracy: 0.8157\n",
            "Epoch 34/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.1962 - accuracy: 0.8041 - val_loss: 1.2180 - val_accuracy: 0.8057\n",
            "Epoch 35/250\n",
            "390/390 [==============================] - 31s 81ms/step - loss: 1.2010 - accuracy: 0.8063 - val_loss: 1.2592 - val_accuracy: 0.7845\n",
            "Epoch 36/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.2023 - accuracy: 0.8093 - val_loss: 1.2285 - val_accuracy: 0.8064\n",
            "Epoch 37/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 1.2147 - accuracy: 0.8049 - val_loss: 1.1520 - val_accuracy: 0.8247\n",
            "Epoch 38/250\n",
            "390/390 [==============================] - 33s 83ms/step - loss: 1.2123 - accuracy: 0.8073 - val_loss: 1.2069 - val_accuracy: 0.8088\n",
            "Epoch 39/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 1.2199 - accuracy: 0.8088 - val_loss: 1.1813 - val_accuracy: 0.8204\n",
            "Epoch 40/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 1.2071 - accuracy: 0.8116 - val_loss: 1.1658 - val_accuracy: 0.8264\n",
            "Epoch 41/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 1.1359 - accuracy: 0.8291 - val_loss: 0.9618 - val_accuracy: 0.8731\n",
            "Epoch 42/250\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 1.0308 - accuracy: 0.8504 - val_loss: 0.9806 - val_accuracy: 0.8547\n",
            "Epoch 43/250\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.9881 - accuracy: 0.8496 - val_loss: 0.9598 - val_accuracy: 0.8578\n",
            "Epoch 44/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.9669 - accuracy: 0.8485 - val_loss: 0.9935 - val_accuracy: 0.8363\n",
            "Epoch 45/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.9539 - accuracy: 0.8511 - val_loss: 0.8906 - val_accuracy: 0.8678\n",
            "Epoch 46/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.9443 - accuracy: 0.8496 - val_loss: 0.9389 - val_accuracy: 0.8517\n",
            "Epoch 47/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.9485 - accuracy: 0.8501 - val_loss: 0.9485 - val_accuracy: 0.8471\n",
            "Epoch 48/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.9397 - accuracy: 0.8510 - val_loss: 0.8785 - val_accuracy: 0.8727\n",
            "Epoch 49/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.9377 - accuracy: 0.8518 - val_loss: 0.9612 - val_accuracy: 0.8455\n",
            "Epoch 50/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.9431 - accuracy: 0.8527 - val_loss: 0.9110 - val_accuracy: 0.8632\n",
            "Epoch 51/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.9385 - accuracy: 0.8528 - val_loss: 0.9141 - val_accuracy: 0.8610\n",
            "Epoch 52/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.9440 - accuracy: 0.8506 - val_loss: 0.8828 - val_accuracy: 0.8729\n",
            "Epoch 53/250\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.9470 - accuracy: 0.8524 - val_loss: 0.9360 - val_accuracy: 0.8589\n",
            "Epoch 54/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.9447 - accuracy: 0.8556 - val_loss: 0.9352 - val_accuracy: 0.8544\n",
            "Epoch 55/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.9476 - accuracy: 0.8522 - val_loss: 0.9312 - val_accuracy: 0.8565\n",
            "Epoch 56/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.9478 - accuracy: 0.8525 - val_loss: 0.9518 - val_accuracy: 0.8566\n",
            "Epoch 57/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.9412 - accuracy: 0.8566 - val_loss: 0.9125 - val_accuracy: 0.8622\n",
            "Epoch 58/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.9541 - accuracy: 0.8537 - val_loss: 0.9646 - val_accuracy: 0.8523\n",
            "Epoch 59/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.9510 - accuracy: 0.8577 - val_loss: 0.9314 - val_accuracy: 0.8622\n",
            "Epoch 60/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.9560 - accuracy: 0.8530 - val_loss: 0.9068 - val_accuracy: 0.8703\n",
            "Epoch 61/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.8924 - accuracy: 0.8753 - val_loss: 0.8150 - val_accuracy: 0.8920\n",
            "Epoch 62/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.8277 - accuracy: 0.8867 - val_loss: 0.8110 - val_accuracy: 0.8905\n",
            "Epoch 63/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.8035 - accuracy: 0.8889 - val_loss: 0.7759 - val_accuracy: 0.8969\n",
            "Epoch 64/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.7825 - accuracy: 0.8890 - val_loss: 0.7849 - val_accuracy: 0.8873\n",
            "Epoch 65/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.7670 - accuracy: 0.8911 - val_loss: 0.7399 - val_accuracy: 0.8982\n",
            "Epoch 66/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.7584 - accuracy: 0.8897 - val_loss: 0.7733 - val_accuracy: 0.8838\n",
            "Epoch 67/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.7466 - accuracy: 0.8909 - val_loss: 0.7576 - val_accuracy: 0.8839\n",
            "Epoch 68/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.7386 - accuracy: 0.8922 - val_loss: 0.7521 - val_accuracy: 0.8853\n",
            "Epoch 69/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.7369 - accuracy: 0.8913 - val_loss: 0.7346 - val_accuracy: 0.8895\n",
            "Epoch 70/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.7288 - accuracy: 0.8935 - val_loss: 0.7582 - val_accuracy: 0.8852\n",
            "Epoch 71/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.7275 - accuracy: 0.8936 - val_loss: 0.7444 - val_accuracy: 0.8865\n",
            "Epoch 72/250\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.7270 - accuracy: 0.8918 - val_loss: 0.7610 - val_accuracy: 0.8835\n",
            "Epoch 73/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.7339 - accuracy: 0.8904 - val_loss: 0.7366 - val_accuracy: 0.8924\n",
            "Epoch 74/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.7316 - accuracy: 0.8897 - val_loss: 0.7364 - val_accuracy: 0.8879\n",
            "Epoch 75/250\n",
            "390/390 [==============================] - 33s 83ms/step - loss: 0.7318 - accuracy: 0.8910 - val_loss: 0.7331 - val_accuracy: 0.8906\n",
            "Epoch 76/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.7263 - accuracy: 0.8919 - val_loss: 0.7872 - val_accuracy: 0.8791\n",
            "Epoch 77/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.7347 - accuracy: 0.8895 - val_loss: 0.7522 - val_accuracy: 0.8886\n",
            "Epoch 78/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.7308 - accuracy: 0.8929 - val_loss: 0.7541 - val_accuracy: 0.8888\n",
            "Epoch 79/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.7322 - accuracy: 0.8915 - val_loss: 0.7374 - val_accuracy: 0.8937\n",
            "Epoch 80/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.7253 - accuracy: 0.8947 - val_loss: 0.7677 - val_accuracy: 0.8820\n",
            "Epoch 81/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.6910 - accuracy: 0.9032 - val_loss: 0.6624 - val_accuracy: 0.9119\n",
            "Epoch 82/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.6474 - accuracy: 0.9136 - val_loss: 0.6708 - val_accuracy: 0.9097\n",
            "Epoch 83/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.6265 - accuracy: 0.9180 - val_loss: 0.6431 - val_accuracy: 0.9134\n",
            "Epoch 84/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.6161 - accuracy: 0.9211 - val_loss: 0.6510 - val_accuracy: 0.9107\n",
            "Epoch 85/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.6053 - accuracy: 0.9219 - val_loss: 0.6504 - val_accuracy: 0.9066\n",
            "Epoch 86/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.5903 - accuracy: 0.9237 - val_loss: 0.6323 - val_accuracy: 0.9120\n",
            "Epoch 87/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.5876 - accuracy: 0.9209 - val_loss: 0.6344 - val_accuracy: 0.9108\n",
            "Epoch 88/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.5835 - accuracy: 0.9235 - val_loss: 0.6226 - val_accuracy: 0.9115\n",
            "Epoch 89/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.5781 - accuracy: 0.9231 - val_loss: 0.6376 - val_accuracy: 0.9079\n",
            "Epoch 90/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.5748 - accuracy: 0.9233 - val_loss: 0.6631 - val_accuracy: 0.9028\n",
            "Epoch 91/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.5712 - accuracy: 0.9229 - val_loss: 0.6299 - val_accuracy: 0.9070\n",
            "Epoch 92/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.5677 - accuracy: 0.9232 - val_loss: 0.6520 - val_accuracy: 0.9004\n",
            "Epoch 93/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.5694 - accuracy: 0.9198 - val_loss: 0.6318 - val_accuracy: 0.9063\n",
            "Epoch 94/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.5596 - accuracy: 0.9214 - val_loss: 0.6165 - val_accuracy: 0.9107\n",
            "Epoch 95/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.5579 - accuracy: 0.9243 - val_loss: 0.6147 - val_accuracy: 0.9077\n",
            "Epoch 96/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.5588 - accuracy: 0.9220 - val_loss: 0.6193 - val_accuracy: 0.9073\n",
            "Epoch 97/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.5543 - accuracy: 0.9229 - val_loss: 0.6410 - val_accuracy: 0.9013\n",
            "Epoch 98/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.5521 - accuracy: 0.9244 - val_loss: 0.6017 - val_accuracy: 0.9090\n",
            "Epoch 99/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.5482 - accuracy: 0.9258 - val_loss: 0.6054 - val_accuracy: 0.9092\n",
            "Epoch 100/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.5546 - accuracy: 0.9241 - val_loss: 0.6729 - val_accuracy: 0.8914\n",
            "Epoch 101/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.5310 - accuracy: 0.9298 - val_loss: 0.5685 - val_accuracy: 0.9209\n",
            "Epoch 102/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.4959 - accuracy: 0.9390 - val_loss: 0.5784 - val_accuracy: 0.9177\n",
            "Epoch 103/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.4919 - accuracy: 0.9399 - val_loss: 0.5854 - val_accuracy: 0.9159\n",
            "Epoch 104/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.4765 - accuracy: 0.9431 - val_loss: 0.5789 - val_accuracy: 0.9173\n",
            "Epoch 105/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.4754 - accuracy: 0.9421 - val_loss: 0.5782 - val_accuracy: 0.9176\n",
            "Epoch 106/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.4618 - accuracy: 0.9470 - val_loss: 0.5680 - val_accuracy: 0.9205\n",
            "Epoch 107/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.4604 - accuracy: 0.9447 - val_loss: 0.5590 - val_accuracy: 0.9203\n",
            "Epoch 108/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.4509 - accuracy: 0.9482 - val_loss: 0.5759 - val_accuracy: 0.9145\n",
            "Epoch 109/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.4521 - accuracy: 0.9462 - val_loss: 0.5618 - val_accuracy: 0.9180\n",
            "Epoch 110/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.4516 - accuracy: 0.9451 - val_loss: 0.5509 - val_accuracy: 0.9202\n",
            "Epoch 111/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.4505 - accuracy: 0.9459 - val_loss: 0.5588 - val_accuracy: 0.9187\n",
            "Epoch 112/250\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.4402 - accuracy: 0.9482 - val_loss: 0.5546 - val_accuracy: 0.9187\n",
            "Epoch 113/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.4388 - accuracy: 0.9464 - val_loss: 0.5561 - val_accuracy: 0.9155\n",
            "Epoch 114/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.4371 - accuracy: 0.9467 - val_loss: 0.5594 - val_accuracy: 0.9169\n",
            "Epoch 115/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.4333 - accuracy: 0.9475 - val_loss: 0.5556 - val_accuracy: 0.9155\n",
            "Epoch 116/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.4339 - accuracy: 0.9453 - val_loss: 0.5677 - val_accuracy: 0.9109\n",
            "Epoch 117/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.4319 - accuracy: 0.9470 - val_loss: 0.5678 - val_accuracy: 0.9158\n",
            "Epoch 118/250\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.4249 - accuracy: 0.9480 - val_loss: 0.5364 - val_accuracy: 0.9201\n",
            "Epoch 119/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.4269 - accuracy: 0.9484 - val_loss: 0.5379 - val_accuracy: 0.9206\n",
            "Epoch 120/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.4207 - accuracy: 0.9486 - val_loss: 0.5489 - val_accuracy: 0.9160\n",
            "Epoch 121/250\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.4097 - accuracy: 0.9528 - val_loss: 0.5257 - val_accuracy: 0.9241\n",
            "Epoch 122/250\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.3929 - accuracy: 0.9560 - val_loss: 0.5330 - val_accuracy: 0.9198\n",
            "Epoch 123/250\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.3845 - accuracy: 0.9604 - val_loss: 0.5221 - val_accuracy: 0.9242\n",
            "Epoch 124/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.3828 - accuracy: 0.9601 - val_loss: 0.5223 - val_accuracy: 0.9269\n",
            "Epoch 125/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.3760 - accuracy: 0.9615 - val_loss: 0.5235 - val_accuracy: 0.9263\n",
            "Epoch 126/250\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.3756 - accuracy: 0.9606 - val_loss: 0.4944 - val_accuracy: 0.9326\n",
            "Epoch 127/250\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.3712 - accuracy: 0.9608 - val_loss: 0.5090 - val_accuracy: 0.9266\n",
            "Epoch 128/250\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.3702 - accuracy: 0.9611 - val_loss: 0.5078 - val_accuracy: 0.9303\n",
            "Epoch 129/250\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.3561 - accuracy: 0.9663 - val_loss: 0.5121 - val_accuracy: 0.9275\n",
            "Epoch 130/250\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.3610 - accuracy: 0.9626 - val_loss: 0.5226 - val_accuracy: 0.9220\n",
            "Epoch 131/250\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.3627 - accuracy: 0.9616 - val_loss: 0.5140 - val_accuracy: 0.9245\n",
            "Epoch 132/250\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.3586 - accuracy: 0.9632 - val_loss: 0.5224 - val_accuracy: 0.9241\n",
            "Epoch 133/250\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.3506 - accuracy: 0.9640 - val_loss: 0.5251 - val_accuracy: 0.9226\n",
            "Epoch 134/250\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.3501 - accuracy: 0.9640 - val_loss: 0.5050 - val_accuracy: 0.9264\n",
            "Epoch 135/250\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.3440 - accuracy: 0.9668 - val_loss: 0.5030 - val_accuracy: 0.9263\n",
            "Epoch 136/250\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.3482 - accuracy: 0.9638 - val_loss: 0.5184 - val_accuracy: 0.9236\n",
            "Epoch 137/250\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.3513 - accuracy: 0.9613 - val_loss: 0.4961 - val_accuracy: 0.9289\n",
            "Epoch 138/250\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.3402 - accuracy: 0.9642 - val_loss: 0.5217 - val_accuracy: 0.9232\n",
            "Epoch 139/250\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.3439 - accuracy: 0.9643 - val_loss: 0.5004 - val_accuracy: 0.9275\n",
            "Epoch 140/250\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.3379 - accuracy: 0.9662 - val_loss: 0.5075 - val_accuracy: 0.9240\n",
            "Epoch 141/250\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.3333 - accuracy: 0.9663 - val_loss: 0.5015 - val_accuracy: 0.9264\n",
            "Epoch 142/250\n",
            "390/390 [==============================] - 33s 83ms/step - loss: 0.3259 - accuracy: 0.9684 - val_loss: 0.4833 - val_accuracy: 0.9308\n",
            "Epoch 143/250\n",
            "390/390 [==============================] - 33s 83ms/step - loss: 0.3216 - accuracy: 0.9699 - val_loss: 0.4896 - val_accuracy: 0.9302\n",
            "Epoch 144/250\n",
            "390/390 [==============================] - 33s 83ms/step - loss: 0.3135 - accuracy: 0.9723 - val_loss: 0.4903 - val_accuracy: 0.9297\n",
            "Epoch 145/250\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.3123 - accuracy: 0.9719 - val_loss: 0.4845 - val_accuracy: 0.9340\n",
            "Epoch 146/250\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.3117 - accuracy: 0.9723 - val_loss: 0.4948 - val_accuracy: 0.9294\n",
            "Epoch 147/250\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.3074 - accuracy: 0.9729 - val_loss: 0.4935 - val_accuracy: 0.9278\n",
            "Epoch 148/250\n",
            "390/390 [==============================] - 33s 83ms/step - loss: 0.3099 - accuracy: 0.9727 - val_loss: 0.5050 - val_accuracy: 0.9282\n",
            "Epoch 149/250\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.3060 - accuracy: 0.9734 - val_loss: 0.4985 - val_accuracy: 0.9284\n",
            "Epoch 150/250\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.3044 - accuracy: 0.9733 - val_loss: 0.4921 - val_accuracy: 0.9299\n",
            "Epoch 151/250\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.3074 - accuracy: 0.9712 - val_loss: 0.4872 - val_accuracy: 0.9321\n",
            "Epoch 152/250\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.3002 - accuracy: 0.9737 - val_loss: 0.4844 - val_accuracy: 0.9317\n",
            "Epoch 153/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2967 - accuracy: 0.9752 - val_loss: 0.4891 - val_accuracy: 0.9313\n",
            "Epoch 154/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2964 - accuracy: 0.9738 - val_loss: 0.5012 - val_accuracy: 0.9279\n",
            "Epoch 155/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2965 - accuracy: 0.9749 - val_loss: 0.4915 - val_accuracy: 0.9293\n",
            "Epoch 156/250\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.3006 - accuracy: 0.9721 - val_loss: 0.4861 - val_accuracy: 0.9304\n",
            "Epoch 157/250\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.2983 - accuracy: 0.9740 - val_loss: 0.4850 - val_accuracy: 0.9298\n",
            "Epoch 158/250\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.2930 - accuracy: 0.9742 - val_loss: 0.4921 - val_accuracy: 0.9264\n",
            "Epoch 159/250\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.2908 - accuracy: 0.9754 - val_loss: 0.4878 - val_accuracy: 0.9291\n",
            "Epoch 160/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2885 - accuracy: 0.9756 - val_loss: 0.4826 - val_accuracy: 0.9288\n",
            "Epoch 161/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2905 - accuracy: 0.9746 - val_loss: 0.4818 - val_accuracy: 0.9300\n",
            "Epoch 162/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2805 - accuracy: 0.9784 - val_loss: 0.4821 - val_accuracy: 0.9310\n",
            "Epoch 163/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2845 - accuracy: 0.9757 - val_loss: 0.4802 - val_accuracy: 0.9298\n",
            "Epoch 164/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2754 - accuracy: 0.9790 - val_loss: 0.4869 - val_accuracy: 0.9300\n",
            "Epoch 165/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2809 - accuracy: 0.9775 - val_loss: 0.4872 - val_accuracy: 0.9289\n",
            "Epoch 166/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2740 - accuracy: 0.9790 - val_loss: 0.4925 - val_accuracy: 0.9284\n",
            "Epoch 167/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2770 - accuracy: 0.9779 - val_loss: 0.4863 - val_accuracy: 0.9283\n",
            "Epoch 168/250\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.2760 - accuracy: 0.9782 - val_loss: 0.4849 - val_accuracy: 0.9278\n",
            "Epoch 169/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2737 - accuracy: 0.9785 - val_loss: 0.4861 - val_accuracy: 0.9287\n",
            "Epoch 170/250\n",
            "390/390 [==============================] - 33s 83ms/step - loss: 0.2762 - accuracy: 0.9774 - val_loss: 0.4826 - val_accuracy: 0.9303\n",
            "Epoch 171/250\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.2748 - accuracy: 0.9783 - val_loss: 0.4798 - val_accuracy: 0.9296\n",
            "Epoch 172/250\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.2710 - accuracy: 0.9792 - val_loss: 0.4783 - val_accuracy: 0.9304\n",
            "Epoch 173/250\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.2708 - accuracy: 0.9783 - val_loss: 0.4797 - val_accuracy: 0.9302\n",
            "Epoch 174/250\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.2659 - accuracy: 0.9810 - val_loss: 0.4831 - val_accuracy: 0.9286\n",
            "Epoch 175/250\n",
            "390/390 [==============================] - 33s 83ms/step - loss: 0.2645 - accuracy: 0.9809 - val_loss: 0.4844 - val_accuracy: 0.9291\n",
            "Epoch 176/250\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.2639 - accuracy: 0.9809 - val_loss: 0.4746 - val_accuracy: 0.9317\n",
            "Epoch 177/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2707 - accuracy: 0.9787 - val_loss: 0.4893 - val_accuracy: 0.9289\n",
            "Epoch 178/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2672 - accuracy: 0.9803 - val_loss: 0.4837 - val_accuracy: 0.9291\n",
            "Epoch 179/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2674 - accuracy: 0.9783 - val_loss: 0.4883 - val_accuracy: 0.9284\n",
            "Epoch 180/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2611 - accuracy: 0.9805 - val_loss: 0.4720 - val_accuracy: 0.9316\n",
            "Epoch 181/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2608 - accuracy: 0.9813 - val_loss: 0.4746 - val_accuracy: 0.9321\n",
            "Epoch 182/250\n",
            "390/390 [==============================] - 33s 86ms/step - loss: 0.2550 - accuracy: 0.9831 - val_loss: 0.4749 - val_accuracy: 0.9309\n",
            "Epoch 183/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2593 - accuracy: 0.9820 - val_loss: 0.4769 - val_accuracy: 0.9310\n",
            "Epoch 184/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2611 - accuracy: 0.9814 - val_loss: 0.4731 - val_accuracy: 0.9314\n",
            "Epoch 185/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2620 - accuracy: 0.9806 - val_loss: 0.4722 - val_accuracy: 0.9312\n",
            "Epoch 186/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2567 - accuracy: 0.9825 - val_loss: 0.4764 - val_accuracy: 0.9304\n",
            "Epoch 187/250\n",
            "390/390 [==============================] - 33s 86ms/step - loss: 0.2560 - accuracy: 0.9822 - val_loss: 0.4789 - val_accuracy: 0.9300\n",
            "Epoch 188/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2576 - accuracy: 0.9814 - val_loss: 0.4786 - val_accuracy: 0.9309\n",
            "Epoch 189/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2555 - accuracy: 0.9823 - val_loss: 0.4791 - val_accuracy: 0.9303\n",
            "Epoch 190/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2547 - accuracy: 0.9816 - val_loss: 0.4785 - val_accuracy: 0.9310\n",
            "Epoch 191/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2518 - accuracy: 0.9830 - val_loss: 0.4777 - val_accuracy: 0.9315\n",
            "Epoch 192/250\n",
            "390/390 [==============================] - 34s 86ms/step - loss: 0.2526 - accuracy: 0.9834 - val_loss: 0.4806 - val_accuracy: 0.9313\n",
            "Epoch 193/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2582 - accuracy: 0.9814 - val_loss: 0.4706 - val_accuracy: 0.9318\n",
            "Epoch 194/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2554 - accuracy: 0.9826 - val_loss: 0.4750 - val_accuracy: 0.9314\n",
            "Epoch 195/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2516 - accuracy: 0.9831 - val_loss: 0.4731 - val_accuracy: 0.9313\n",
            "Epoch 196/250\n",
            "390/390 [==============================] - 33s 86ms/step - loss: 0.2513 - accuracy: 0.9833 - val_loss: 0.4793 - val_accuracy: 0.9311\n",
            "Epoch 197/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2532 - accuracy: 0.9825 - val_loss: 0.4799 - val_accuracy: 0.9308\n",
            "Epoch 198/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2539 - accuracy: 0.9831 - val_loss: 0.4757 - val_accuracy: 0.9313\n",
            "Epoch 199/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2523 - accuracy: 0.9832 - val_loss: 0.4796 - val_accuracy: 0.9309\n",
            "Epoch 200/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2503 - accuracy: 0.9837 - val_loss: 0.4754 - val_accuracy: 0.9333\n",
            "Epoch 201/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2485 - accuracy: 0.9838 - val_loss: 0.4737 - val_accuracy: 0.9329\n",
            "Epoch 202/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2505 - accuracy: 0.9828 - val_loss: 0.4755 - val_accuracy: 0.9316\n",
            "Epoch 203/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2501 - accuracy: 0.9823 - val_loss: 0.4772 - val_accuracy: 0.9317\n",
            "Epoch 204/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2474 - accuracy: 0.9835 - val_loss: 0.4741 - val_accuracy: 0.9337\n",
            "Epoch 205/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2492 - accuracy: 0.9837 - val_loss: 0.4756 - val_accuracy: 0.9326\n",
            "Epoch 206/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2475 - accuracy: 0.9840 - val_loss: 0.4772 - val_accuracy: 0.9316\n",
            "Epoch 207/250\n",
            "390/390 [==============================] - 33s 86ms/step - loss: 0.2506 - accuracy: 0.9820 - val_loss: 0.4750 - val_accuracy: 0.9323\n",
            "Epoch 208/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2496 - accuracy: 0.9828 - val_loss: 0.4756 - val_accuracy: 0.9303\n",
            "Epoch 209/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2493 - accuracy: 0.9826 - val_loss: 0.4739 - val_accuracy: 0.9322\n",
            "Epoch 210/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2506 - accuracy: 0.9828 - val_loss: 0.4731 - val_accuracy: 0.9309\n",
            "Epoch 211/250\n",
            "390/390 [==============================] - 34s 86ms/step - loss: 0.2495 - accuracy: 0.9829 - val_loss: 0.4763 - val_accuracy: 0.9313\n",
            "Epoch 212/250\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.2450 - accuracy: 0.9837 - val_loss: 0.4712 - val_accuracy: 0.9321\n",
            "Epoch 213/250\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.2480 - accuracy: 0.9833 - val_loss: 0.4733 - val_accuracy: 0.9313\n",
            "Epoch 214/250\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.2498 - accuracy: 0.9830 - val_loss: 0.4766 - val_accuracy: 0.9307\n",
            "Epoch 215/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2478 - accuracy: 0.9838 - val_loss: 0.4761 - val_accuracy: 0.9325\n",
            "Epoch 216/250\n",
            "390/390 [==============================] - 33s 86ms/step - loss: 0.2463 - accuracy: 0.9836 - val_loss: 0.4739 - val_accuracy: 0.9320\n",
            "Epoch 217/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2429 - accuracy: 0.9838 - val_loss: 0.4688 - val_accuracy: 0.9337\n",
            "Epoch 218/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2468 - accuracy: 0.9844 - val_loss: 0.4729 - val_accuracy: 0.9325\n",
            "Epoch 219/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2440 - accuracy: 0.9843 - val_loss: 0.4759 - val_accuracy: 0.9318\n",
            "Epoch 220/250\n",
            "390/390 [==============================] - 33s 86ms/step - loss: 0.2462 - accuracy: 0.9832 - val_loss: 0.4705 - val_accuracy: 0.9322\n",
            "Epoch 221/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2457 - accuracy: 0.9838 - val_loss: 0.4708 - val_accuracy: 0.9333\n",
            "Epoch 222/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2442 - accuracy: 0.9842 - val_loss: 0.4698 - val_accuracy: 0.9330\n",
            "Epoch 223/250\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.2469 - accuracy: 0.9827 - val_loss: 0.4721 - val_accuracy: 0.9322\n",
            "Epoch 224/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2459 - accuracy: 0.9832 - val_loss: 0.4713 - val_accuracy: 0.9328\n",
            "Epoch 225/250\n",
            "390/390 [==============================] - 33s 86ms/step - loss: 0.2423 - accuracy: 0.9846 - val_loss: 0.4730 - val_accuracy: 0.9319\n",
            "Epoch 226/250\n",
            "390/390 [==============================] - 33s 86ms/step - loss: 0.2428 - accuracy: 0.9840 - val_loss: 0.4723 - val_accuracy: 0.9324\n",
            "Epoch 227/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2441 - accuracy: 0.9837 - val_loss: 0.4714 - val_accuracy: 0.9328\n",
            "Epoch 228/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2385 - accuracy: 0.9863 - val_loss: 0.4726 - val_accuracy: 0.9328\n",
            "Epoch 229/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2400 - accuracy: 0.9858 - val_loss: 0.4734 - val_accuracy: 0.9319\n",
            "Epoch 230/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2426 - accuracy: 0.9852 - val_loss: 0.4729 - val_accuracy: 0.9323\n",
            "Epoch 231/250\n",
            "390/390 [==============================] - 34s 86ms/step - loss: 0.2449 - accuracy: 0.9832 - val_loss: 0.4716 - val_accuracy: 0.9326\n",
            "Epoch 232/250\n",
            "390/390 [==============================] - 34s 86ms/step - loss: 0.2440 - accuracy: 0.9845 - val_loss: 0.4748 - val_accuracy: 0.9315\n",
            "Epoch 233/250\n",
            "390/390 [==============================] - 34s 86ms/step - loss: 0.2423 - accuracy: 0.9852 - val_loss: 0.4712 - val_accuracy: 0.9333\n",
            "Epoch 234/250\n",
            "390/390 [==============================] - 34s 86ms/step - loss: 0.2415 - accuracy: 0.9844 - val_loss: 0.4716 - val_accuracy: 0.9335\n",
            "Epoch 235/250\n",
            "390/390 [==============================] - 33s 86ms/step - loss: 0.2394 - accuracy: 0.9854 - val_loss: 0.4719 - val_accuracy: 0.9330\n",
            "Epoch 236/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2410 - accuracy: 0.9849 - val_loss: 0.4736 - val_accuracy: 0.9325\n",
            "Epoch 237/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2434 - accuracy: 0.9839 - val_loss: 0.4719 - val_accuracy: 0.9323\n",
            "Epoch 238/250\n",
            "390/390 [==============================] - 33s 86ms/step - loss: 0.2420 - accuracy: 0.9845 - val_loss: 0.4712 - val_accuracy: 0.9329\n",
            "Epoch 239/250\n",
            "390/390 [==============================] - 34s 86ms/step - loss: 0.2440 - accuracy: 0.9843 - val_loss: 0.4726 - val_accuracy: 0.9322\n",
            "Epoch 240/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2456 - accuracy: 0.9840 - val_loss: 0.4728 - val_accuracy: 0.9322\n",
            "Epoch 241/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2422 - accuracy: 0.9846 - val_loss: 0.4738 - val_accuracy: 0.9318\n",
            "Epoch 242/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2438 - accuracy: 0.9841 - val_loss: 0.4719 - val_accuracy: 0.9325\n",
            "Epoch 243/250\n",
            "390/390 [==============================] - 34s 86ms/step - loss: 0.2440 - accuracy: 0.9849 - val_loss: 0.4727 - val_accuracy: 0.9324\n",
            "Epoch 244/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2433 - accuracy: 0.9838 - val_loss: 0.4739 - val_accuracy: 0.9324\n",
            "Epoch 245/250\n",
            "390/390 [==============================] - 33s 86ms/step - loss: 0.2405 - accuracy: 0.9842 - val_loss: 0.4742 - val_accuracy: 0.9326\n",
            "Epoch 246/250\n",
            "390/390 [==============================] - 34s 86ms/step - loss: 0.2407 - accuracy: 0.9858 - val_loss: 0.4737 - val_accuracy: 0.9322\n",
            "Epoch 247/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2442 - accuracy: 0.9835 - val_loss: 0.4719 - val_accuracy: 0.9326\n",
            "Epoch 248/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2432 - accuracy: 0.9839 - val_loss: 0.4728 - val_accuracy: 0.9323\n",
            "Epoch 249/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2405 - accuracy: 0.9860 - val_loss: 0.4723 - val_accuracy: 0.9328\n",
            "Epoch 250/250\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.2439 - accuracy: 0.9832 - val_loss: 0.4749 - val_accuracy: 0.9318\n",
            "the validation 0/1 loss is:  0.0682\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKPoXBJowzkA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jXjgINAwzn0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}