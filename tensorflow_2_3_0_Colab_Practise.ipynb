{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow 2.3.0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTI344pw67iU"
      },
      "source": [
        "# multi-class classification with Keras\r\n",
        "import pandas as pd\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\r\n",
        "from keras.utils import np_utils\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "import h5py\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNFKN--7DjxP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "outputId": "15a8e38d-0967-4c77-ba8f-fc916b072126"
      },
      "source": [
        "!pip install h5py\r\n",
        "from sklearn.datasets import load_iris\r\n",
        "\r\n",
        "d=load_iris()\r\n",
        "X=d.data\r\n",
        "z=pd.DataFrame(X)\r\n",
        "# Y=d.target\r\n",
        "z.loc[[2,3,6,]]\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.15.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     0    1    2    3\n",
              "2  4.7  3.2  1.3  0.2\n",
              "3  4.6  3.1  1.5  0.2\n",
              "6  4.6  3.4  1.4  0.3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFTRuW1F9cqn"
      },
      "source": [
        "from sklearn.datasets import load_iris\r\n",
        "\r\n",
        "d=load_iris()\r\n",
        "X=d.data\r\n",
        "Y=d.target\r\n",
        "\r\n",
        "# encode class values as integers\r\n",
        "encoder = LabelEncoder()\r\n",
        "encoder.fit(Y)\r\n",
        "encoded_Y = encoder.transform(Y)\r\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\r\n",
        "dummy_y = np_utils.to_categorical(encoded_Y)\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT8G5p59AQfk"
      },
      "source": [
        "encoder = LabelEncoder()\r\n",
        "encoder.fit(Y)\r\n",
        "encoded_Y = encoder.transform(Y)\r\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\r\n",
        "dummy_y = np_utils.to_categorical(encoded_Y)\r\n",
        "dummy_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "EzgA_Fv0_YlW",
        "outputId": "9e3db46e-5dd0-4e84-ba88-4c06696e77d4"
      },
      "source": [
        "from sklearn.datasets import load_iris\r\n",
        "d=load_iris()\r\n",
        "x=d.data\r\n",
        "y=d.target\r\n",
        "\r\n",
        "l=LabelEncoder()\r\n",
        "l.fit(y)\r\n",
        "newy = l.transform(y)\r\n",
        "e=np_utils.to_categorical(newy)\r\n",
        "e"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-316a3771c123>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnewy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'LabelEncoder' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sfF4gZm99X9",
        "outputId": "832620d4-7e93-49d4-f1ac-9509fa8aa017"
      },
      "source": [
        "# encode class values as integers\r\n",
        "encoder = LabelEncoder()\r\n",
        "encoder.fit(Y)\r\n",
        "encoded_Y = encoder.transform(Y)\r\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\r\n",
        "dummy_y = np_utils.to_categorical(encoded_Y)\r\n",
        "Y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTIT4HacgoF5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obfpstD962ON",
        "outputId": "e7fa3e40-f722-412d-9a3c-f0adc2a3f65f"
      },
      "source": [
        "# multi-class classification with Keras\r\n",
        "import pandas\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\r\n",
        "from keras.utils import np_utils\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.datasets import load_iris\r\n",
        "\r\n",
        "# load dataset\r\n",
        "d=load_iris()\r\n",
        "X=d.data\r\n",
        "Y=d.target\r\n",
        "\r\n",
        "\r\n",
        "# encode class values as integers\r\n",
        "encoder = LabelEncoder()\r\n",
        "encoder.fit(Y)\r\n",
        "encoded_Y = encoder.transform(Y)\r\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\r\n",
        "dummy_y = np_utils.to_categorical(encoded_Y)\r\n",
        "\r\n",
        "# define baseline model\r\n",
        "def baseline_model():\r\n",
        "\t# create model\r\n",
        "\tmodel = Sequential()\r\n",
        "\tmodel.add(Dense(8, input_dim=4, activation='relu'))\r\n",
        "\tmodel.add(Dense(3, activation='softmax'))\r\n",
        "\t# Compile model\r\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\treturn model\r\n",
        "\r\n",
        "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\r\n",
        "kfold = KFold(n_splits=10, shuffle=True)\r\n",
        "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\r\n",
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fddc3008ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fddc6be78c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fddc92c6268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fddc2772b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fddc41167b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fddc1e66400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Baseline: 98.00% (4.27%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3uEXdOXd1Nu",
        "outputId": "6ee8965d-009d-4130-f1e2-e77ac2c305e7"
      },
      "source": [
        "# multi-class classification with Keras\r\n",
        "import pandas\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\r\n",
        "from keras.utils import np_utils\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.datasets import load_iris\r\n",
        "\r\n",
        "# load dataset\r\n",
        "d=load_iris()\r\n",
        "X=d.data\r\n",
        "Y=d.target\r\n",
        "\r\n",
        "\r\n",
        "# encode class values as integers\r\n",
        "encoder = LabelEncoder()\r\n",
        "encoder.fit(Y)\r\n",
        "encoded_Y = encoder.transform(Y)\r\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\r\n",
        "dummy_y = np_utils.to_categorical(encoded_Y)\r\n",
        "\r\n",
        "# define baseline model\r\n",
        "def baseline_model():\r\n",
        "\t# create model\r\n",
        "\tmodel = Sequential()\r\n",
        "\tmodel.add(Dense(8, input_dim=4, activation='relu'))\r\n",
        "\tmodel.add(Dense(3, activation='softmax'))\r\n",
        "\t# Compile model\r\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\treturn model\r\n",
        "\r\n",
        "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\r\n",
        "kfold = KFold(n_splits=10, shuffle=True)\r\n",
        "results = cross_val_score(estimator, X, Y, cv=kfold)\r\n",
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fddc0d65730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fddbfc3a378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fddbeb81d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fddbda2bb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fddbc8f17b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fddc8a9e9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fddc1e668c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fddbf3ffea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fddc52551e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fddbe303d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Baseline: 97.33% (4.42%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNjWrDGCfn0X",
        "outputId": "67a52c96-1e6a-469d-b1d3-6099cf4284a3"
      },
      "source": [
        "from sklearn.datasets import load_iris\r\n",
        "\r\n",
        "d=load_iris()\r\n",
        "x=d.data\r\n",
        "y=d.target\r\n",
        "\r\n",
        "x_train, \r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeRzzHtzhA4A"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import keras\r\n",
        "from keras import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from sklearn.datasets import load_iris\r\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck0RJfyQq4-n"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "\r\n",
        "L=LabelEncoder()\r\n",
        "L.fit(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yhLtgBHnLYv",
        "outputId": "965aaee8-adb7-4df2-f4e2-a54870986670"
      },
      "source": [
        "d=load_iris()\r\n",
        "x=d.data\r\n",
        "y=d.target\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "x_train, x_test, y_train, y_test= train_test_split(x,y,test_size=0.3, random_state=1)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "model = Sequential([Dense(20, input_dim=len(x_train[0, :]), activation='relu'), \r\n",
        "                    Dense(10, activation='relu'),\r\n",
        "                    Dense(5, activation='relu'),\r\n",
        "                    Dense(1)])\r\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\r\n",
        "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=50, batch_size=20)\r\n",
        "print(model.summary())\r\n",
        "print(history.params)\r\n",
        "t, ac = model.evaluate(x_test, y_test, verbose=2)\r\n",
        "print(round(t, 2))\r\n",
        "print(round(ac,2))\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "6/6 [==============================] - 1s 35ms/step - loss: 1.3576 - accuracy: 0.3289 - val_loss: 0.9976 - val_accuracy: 0.3111\n",
            "Epoch 2/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.9768 - accuracy: 0.3876 - val_loss: 0.8064 - val_accuracy: 0.3111\n",
            "Epoch 3/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.8671 - accuracy: 0.3506 - val_loss: 0.6117 - val_accuracy: 0.6444\n",
            "Epoch 4/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.6342 - accuracy: 0.6184 - val_loss: 0.4564 - val_accuracy: 0.7111\n",
            "Epoch 5/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5288 - accuracy: 0.6374 - val_loss: 0.3382 - val_accuracy: 0.7111\n",
            "Epoch 6/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.3972 - accuracy: 0.6198 - val_loss: 0.2692 - val_accuracy: 0.6000\n",
            "Epoch 7/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.3155 - accuracy: 0.5647 - val_loss: 0.2287 - val_accuracy: 0.4444\n",
            "Epoch 8/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2541 - accuracy: 0.4481 - val_loss: 0.1999 - val_accuracy: 0.4444\n",
            "Epoch 9/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2499 - accuracy: 0.3252 - val_loss: 0.1790 - val_accuracy: 0.4444\n",
            "Epoch 10/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1916 - accuracy: 0.4078 - val_loss: 0.1611 - val_accuracy: 0.5111\n",
            "Epoch 11/50\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1770 - accuracy: 0.5340 - val_loss: 0.1416 - val_accuracy: 0.6444\n",
            "Epoch 12/50\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1332 - accuracy: 0.6564 - val_loss: 0.1214 - val_accuracy: 0.6889\n",
            "Epoch 13/50\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1231 - accuracy: 0.6429 - val_loss: 0.1041 - val_accuracy: 0.7111\n",
            "Epoch 14/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0970 - accuracy: 0.6304 - val_loss: 0.0890 - val_accuracy: 0.7111\n",
            "Epoch 15/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0835 - accuracy: 0.6452 - val_loss: 0.0782 - val_accuracy: 0.7111\n",
            "Epoch 16/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0768 - accuracy: 0.6768 - val_loss: 0.0696 - val_accuracy: 0.7111\n",
            "Epoch 17/50\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0643 - accuracy: 0.6193 - val_loss: 0.0643 - val_accuracy: 0.7111\n",
            "Epoch 18/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0621 - accuracy: 0.6627 - val_loss: 0.0601 - val_accuracy: 0.7111\n",
            "Epoch 19/50\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0563 - accuracy: 0.6555 - val_loss: 0.0588 - val_accuracy: 0.7111\n",
            "Epoch 20/50\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0448 - accuracy: 0.6392 - val_loss: 0.0573 - val_accuracy: 0.7111\n",
            "Epoch 21/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0453 - accuracy: 0.6431 - val_loss: 0.0559 - val_accuracy: 0.7111\n",
            "Epoch 22/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0476 - accuracy: 0.6080 - val_loss: 0.0526 - val_accuracy: 0.7111\n",
            "Epoch 23/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0408 - accuracy: 0.6585 - val_loss: 0.0486 - val_accuracy: 0.7111\n",
            "Epoch 24/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0407 - accuracy: 0.6629 - val_loss: 0.0500 - val_accuracy: 0.7111\n",
            "Epoch 25/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0384 - accuracy: 0.6894 - val_loss: 0.0511 - val_accuracy: 0.7111\n",
            "Epoch 26/50\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0390 - accuracy: 0.5881 - val_loss: 0.0491 - val_accuracy: 0.7111\n",
            "Epoch 27/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0411 - accuracy: 0.6513 - val_loss: 0.0451 - val_accuracy: 0.7111\n",
            "Epoch 28/50\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0386 - accuracy: 0.6338 - val_loss: 0.0430 - val_accuracy: 0.7111\n",
            "Epoch 29/50\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0436 - accuracy: 0.6341 - val_loss: 0.0438 - val_accuracy: 0.7111\n",
            "Epoch 30/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0384 - accuracy: 0.6137 - val_loss: 0.0499 - val_accuracy: 0.7111\n",
            "Epoch 31/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0351 - accuracy: 0.6309 - val_loss: 0.0455 - val_accuracy: 0.7111\n",
            "Epoch 32/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0436 - accuracy: 0.5711 - val_loss: 0.0415 - val_accuracy: 0.7111\n",
            "Epoch 33/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0414 - accuracy: 0.6517 - val_loss: 0.0418 - val_accuracy: 0.7111\n",
            "Epoch 34/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0370 - accuracy: 0.6686 - val_loss: 0.0436 - val_accuracy: 0.7111\n",
            "Epoch 35/50\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0429 - accuracy: 0.6538 - val_loss: 0.0475 - val_accuracy: 0.7111\n",
            "Epoch 36/50\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0380 - accuracy: 0.6110 - val_loss: 0.0475 - val_accuracy: 0.7111\n",
            "Epoch 37/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0374 - accuracy: 0.6731 - val_loss: 0.0424 - val_accuracy: 0.7111\n",
            "Epoch 38/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0353 - accuracy: 0.6096 - val_loss: 0.0412 - val_accuracy: 0.7111\n",
            "Epoch 39/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0311 - accuracy: 0.6474 - val_loss: 0.0424 - val_accuracy: 0.7111\n",
            "Epoch 40/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0366 - accuracy: 0.6791 - val_loss: 0.0455 - val_accuracy: 0.7111\n",
            "Epoch 41/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0306 - accuracy: 0.6662 - val_loss: 0.0471 - val_accuracy: 0.7111\n",
            "Epoch 42/50\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0370 - accuracy: 0.6750 - val_loss: 0.0468 - val_accuracy: 0.7111\n",
            "Epoch 43/50\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 0.0326 - accuracy: 0.6306 - val_loss: 0.0444 - val_accuracy: 0.7111\n",
            "Epoch 44/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0361 - accuracy: 0.7182 - val_loss: 0.0416 - val_accuracy: 0.7111\n",
            "Epoch 45/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0410 - accuracy: 0.6225 - val_loss: 0.0445 - val_accuracy: 0.7111\n",
            "Epoch 46/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0443 - accuracy: 0.6422 - val_loss: 0.0487 - val_accuracy: 0.7111\n",
            "Epoch 47/50\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0399 - accuracy: 0.6210 - val_loss: 0.0467 - val_accuracy: 0.7111\n",
            "Epoch 48/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0419 - accuracy: 0.6542 - val_loss: 0.0454 - val_accuracy: 0.7111\n",
            "Epoch 49/50\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0346 - accuracy: 0.6538 - val_loss: 0.0437 - val_accuracy: 0.7111\n",
            "Epoch 50/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0413 - accuracy: 0.6485 - val_loss: 0.0417 - val_accuracy: 0.7111\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_8 (Dense)              (None, 20)                100       \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                210       \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 5)                 55        \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 6         \n",
            "=================================================================\n",
            "Total params: 371\n",
            "Trainable params: 371\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "{'verbose': 1, 'epochs': 50, 'steps': 6}\n",
            "2/2 - 0s - loss: 0.0417 - accuracy: 0.7111\n",
            "0.04\n",
            "0.71\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voU2WoCUnQ2m"
      },
      "source": [
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "_hQ8F4hurhD-",
        "outputId": "dfb32d2c-ca4b-429e-f856-949ed9c61cd3"
      },
      "source": [
        "from sklearn.datasets import load_iris\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from keras.utils import np_utils\r\n",
        "\r\n",
        "d=load_iris()\r\n",
        "x=d.data\r\n",
        "y=d.target\r\n",
        "\r\n",
        "#Label Encoding target variable\r\n",
        "L=LabelEncoder()\r\n",
        "L.fit(y)\r\n",
        "t_y=L.transform(y)\r\n",
        "y=np_utils.to_categorical(t_y)\r\n",
        "x_train, x_test, y_train, y_test= train_test_split(x,y,test_size=0.3, random_state=1)\r\n",
        "\r\n",
        "model = Sequential([Dense(20, input_dim=len(x_train[0, :]), activation='relu'), \r\n",
        "                    Dense(10, activation='relu'),\r\n",
        "                    Dense(5, activation='relu'),\r\n",
        "                    Dense(1, activation='relu')])\r\n",
        "optimizer=SGD(lr=0.01, momentum=0.9)\r\n",
        "model.compile(optimizer=optimizer, loss='mse', metrics=['accuracy'])\r\n",
        "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=50, batch_size=20)\r\n",
        "print(model.summary())\r\n",
        "print(history.params)\r\n",
        "t, ac = model.evaluate(x_test, y_test, verbose=2)\r\n",
        "print(round(t, 2))\r\n",
        "print(round(ac,2))\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-2da99b2e692c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m                     \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                     Dense(1, activation='relu')])\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SGD' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dtk08r-BsTHu",
        "outputId": "3485b5be-9e9a-496b-ae00-ed41a8a70f47"
      },
      "source": [
        "# multi-class classification with Keras\r\n",
        "import pandas\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\r\n",
        "from keras.utils import np_utils\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.datasets import load_iris\r\n",
        "\r\n",
        "# load dataset\r\n",
        "\r\n",
        "d=load_iris()\r\n",
        "X=d.data\r\n",
        "Y=d.target\r\n",
        "# encode class values as integers\r\n",
        "dummy_y = d.target\r\n",
        "\r\n",
        "# define baseline model\r\n",
        "def baseline_model():\r\n",
        "\t# create model\r\n",
        "\tmodel = Sequential()\r\n",
        "\tmodel.add(Dense(8, input_dim=4, activation='relu'))\r\n",
        "\tmodel.add(Dense(3, activation='softmax'))\r\n",
        "\t# Compile model\r\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\treturn model\r\n",
        "\r\n",
        "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\r\n",
        "kfold = KFold(n_splits=10, shuffle=True)\r\n",
        "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\r\n",
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 162 calls to <function Model.make_test_function.<locals>.test_function at 0x7f055e4f97b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f055e3f5d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f055e2a2ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f055e1c91e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0561f93840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f055a65b488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f055a65b400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Baseline: 96.00% (4.42%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoqfdWnrxOPu"
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\r\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCiRGR0Ltqy1",
        "outputId": "d6bb6689-cd7f-4b11-e3db-425f8df384b7"
      },
      "source": [
        "# multi-class classification with Keras\r\n",
        "import pandas\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\r\n",
        "from keras.utils import np_utils\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.datasets import load_iris\r\n",
        "\r\n",
        "# load dataset\r\n",
        "\r\n",
        "d=load_iris()\r\n",
        "X=d.data\r\n",
        "Y=d.target\r\n",
        "# encode class values as integers\r\n",
        "dummy_y = d.target\r\n",
        "\r\n",
        "# define baseline model\r\n",
        "def baseline_model():\r\n",
        "\t# create model\r\n",
        "\tmodel = Sequential()\r\n",
        "\tmodel.add(Dense(8, input_dim=4, activation='relu'))\r\n",
        "\tmodel.add(Dense(3, activation='softmax'))\r\n",
        "\t# Compile model\r\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\treturn model\r\n",
        "\r\n",
        "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=1)\r\n",
        "kfold = KFold(n_splits=10, shuffle=True)\r\n",
        "results = cross_val_score(estimator, X, Y, cv=kfold)\r\n",
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "27/27 [==============================] - 1s 1ms/step - loss: 1.1180 - accuracy: 0.7487\n",
            "Epoch 2/200\n",
            "27/27 [==============================] - 0s 901us/step - loss: 1.1784 - accuracy: 0.6626\n",
            "Epoch 3/200\n",
            "27/27 [==============================] - 0s 963us/step - loss: 1.0025 - accuracy: 0.6985\n",
            "Epoch 4/200\n",
            "27/27 [==============================] - 0s 926us/step - loss: 0.9227 - accuracy: 0.7074\n",
            "Epoch 5/200\n",
            "27/27 [==============================] - 0s 938us/step - loss: 0.8054 - accuracy: 0.7475\n",
            "Epoch 6/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.8249 - accuracy: 0.6957\n",
            "Epoch 7/200\n",
            "27/27 [==============================] - 0s 997us/step - loss: 0.8052 - accuracy: 0.6610\n",
            "Epoch 8/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.7950 - accuracy: 0.6273\n",
            "Epoch 9/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.7233 - accuracy: 0.6820\n",
            "Epoch 10/200\n",
            "27/27 [==============================] - 0s 941us/step - loss: 0.7178 - accuracy: 0.6687\n",
            "Epoch 11/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6807 - accuracy: 0.7151\n",
            "Epoch 12/200\n",
            "27/27 [==============================] - 0s 959us/step - loss: 0.6544 - accuracy: 0.7324\n",
            "Epoch 13/200\n",
            "27/27 [==============================] - 0s 931us/step - loss: 0.6208 - accuracy: 0.7500\n",
            "Epoch 14/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6053 - accuracy: 0.8013\n",
            "Epoch 15/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5496 - accuracy: 0.7962\n",
            "Epoch 16/200\n",
            "27/27 [==============================] - 0s 985us/step - loss: 0.5893 - accuracy: 0.8071\n",
            "Epoch 17/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5483 - accuracy: 0.7886\n",
            "Epoch 18/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5477 - accuracy: 0.8149\n",
            "Epoch 19/200\n",
            "27/27 [==============================] - 0s 977us/step - loss: 0.4996 - accuracy: 0.8745\n",
            "Epoch 20/200\n",
            "27/27 [==============================] - 0s 956us/step - loss: 0.5111 - accuracy: 0.8227\n",
            "Epoch 21/200\n",
            "27/27 [==============================] - 0s 884us/step - loss: 0.4924 - accuracy: 0.8628\n",
            "Epoch 22/200\n",
            "27/27 [==============================] - 0s 982us/step - loss: 0.4386 - accuracy: 0.8994\n",
            "Epoch 23/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4731 - accuracy: 0.9241\n",
            "Epoch 24/200\n",
            "27/27 [==============================] - 0s 944us/step - loss: 0.4195 - accuracy: 0.9131\n",
            "Epoch 25/200\n",
            "27/27 [==============================] - 0s 978us/step - loss: 0.4364 - accuracy: 0.9229\n",
            "Epoch 26/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3914 - accuracy: 0.8804\n",
            "Epoch 27/200\n",
            "27/27 [==============================] - 0s 903us/step - loss: 0.4252 - accuracy: 0.9220\n",
            "Epoch 28/200\n",
            "27/27 [==============================] - 0s 967us/step - loss: 0.4402 - accuracy: 0.9407\n",
            "Epoch 29/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4011 - accuracy: 0.9166\n",
            "Epoch 30/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3723 - accuracy: 0.9448\n",
            "Epoch 31/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3834 - accuracy: 0.9189\n",
            "Epoch 32/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3612 - accuracy: 0.9133\n",
            "Epoch 33/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3336 - accuracy: 0.9268\n",
            "Epoch 34/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3754 - accuracy: 0.9177\n",
            "Epoch 35/200\n",
            "27/27 [==============================] - 0s 981us/step - loss: 0.3450 - accuracy: 0.9414\n",
            "Epoch 36/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3352 - accuracy: 0.9719\n",
            "Epoch 37/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3287 - accuracy: 0.9541\n",
            "Epoch 38/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3506 - accuracy: 0.9298\n",
            "Epoch 39/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3687 - accuracy: 0.9637\n",
            "Epoch 40/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3339 - accuracy: 0.9704\n",
            "Epoch 41/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3268 - accuracy: 0.9434\n",
            "Epoch 42/200\n",
            "27/27 [==============================] - 0s 942us/step - loss: 0.3206 - accuracy: 0.9710\n",
            "Epoch 43/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3292 - accuracy: 0.9526\n",
            "Epoch 44/200\n",
            "27/27 [==============================] - 0s 904us/step - loss: 0.3340 - accuracy: 0.9167\n",
            "Epoch 45/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2966 - accuracy: 0.9842\n",
            "Epoch 46/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3171 - accuracy: 0.9458\n",
            "Epoch 47/200\n",
            "27/27 [==============================] - 0s 895us/step - loss: 0.2843 - accuracy: 0.9644\n",
            "Epoch 48/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2968 - accuracy: 0.9623\n",
            "Epoch 49/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2587 - accuracy: 0.9649\n",
            "Epoch 50/200\n",
            "27/27 [==============================] - 0s 924us/step - loss: 0.2740 - accuracy: 0.9725\n",
            "Epoch 51/200\n",
            "27/27 [==============================] - 0s 936us/step - loss: 0.2734 - accuracy: 0.9450\n",
            "Epoch 52/200\n",
            "27/27 [==============================] - 0s 877us/step - loss: 0.2599 - accuracy: 0.9631\n",
            "Epoch 53/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2607 - accuracy: 0.9719\n",
            "Epoch 54/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2819 - accuracy: 0.9605\n",
            "Epoch 55/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2660 - accuracy: 0.9308\n",
            "Epoch 56/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2521 - accuracy: 0.9714\n",
            "Epoch 57/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2613 - accuracy: 0.9453\n",
            "Epoch 58/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2568 - accuracy: 0.9617\n",
            "Epoch 59/200\n",
            "27/27 [==============================] - 0s 967us/step - loss: 0.2325 - accuracy: 0.9600\n",
            "Epoch 60/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2406 - accuracy: 0.9640\n",
            "Epoch 61/200\n",
            "27/27 [==============================] - 0s 976us/step - loss: 0.2369 - accuracy: 0.9689\n",
            "Epoch 62/200\n",
            "27/27 [==============================] - 0s 970us/step - loss: 0.2516 - accuracy: 0.9541\n",
            "Epoch 63/200\n",
            "27/27 [==============================] - 0s 987us/step - loss: 0.2548 - accuracy: 0.9633\n",
            "Epoch 64/200\n",
            "27/27 [==============================] - 0s 987us/step - loss: 0.2305 - accuracy: 0.9511\n",
            "Epoch 65/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2303 - accuracy: 0.9655\n",
            "Epoch 66/200\n",
            "27/27 [==============================] - 0s 957us/step - loss: 0.2175 - accuracy: 0.9510\n",
            "Epoch 67/200\n",
            "27/27 [==============================] - 0s 912us/step - loss: 0.2231 - accuracy: 0.9625\n",
            "Epoch 68/200\n",
            "27/27 [==============================] - 0s 940us/step - loss: 0.2261 - accuracy: 0.9540\n",
            "Epoch 69/200\n",
            "27/27 [==============================] - 0s 957us/step - loss: 0.2091 - accuracy: 0.9896\n",
            "Epoch 70/200\n",
            "27/27 [==============================] - 0s 984us/step - loss: 0.2194 - accuracy: 0.9824\n",
            "Epoch 71/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2307 - accuracy: 0.9592\n",
            "Epoch 72/200\n",
            "27/27 [==============================] - 0s 986us/step - loss: 0.1905 - accuracy: 0.9766\n",
            "Epoch 73/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2221 - accuracy: 0.9585\n",
            "Epoch 74/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1996 - accuracy: 0.9492\n",
            "Epoch 75/200\n",
            "27/27 [==============================] - 0s 980us/step - loss: 0.2015 - accuracy: 0.9461\n",
            "Epoch 76/200\n",
            "27/27 [==============================] - 0s 989us/step - loss: 0.1956 - accuracy: 0.9651\n",
            "Epoch 77/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2164 - accuracy: 0.9571\n",
            "Epoch 78/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1818 - accuracy: 0.9701\n",
            "Epoch 79/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1934 - accuracy: 0.9665\n",
            "Epoch 80/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2130 - accuracy: 0.9621\n",
            "Epoch 81/200\n",
            "27/27 [==============================] - 0s 980us/step - loss: 0.1852 - accuracy: 0.9849\n",
            "Epoch 82/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2151 - accuracy: 0.9414\n",
            "Epoch 83/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1766 - accuracy: 0.9665\n",
            "Epoch 84/200\n",
            "27/27 [==============================] - 0s 950us/step - loss: 0.1670 - accuracy: 0.9849\n",
            "Epoch 85/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1666 - accuracy: 0.9861\n",
            "Epoch 86/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1693 - accuracy: 0.9802\n",
            "Epoch 87/200\n",
            "27/27 [==============================] - 0s 991us/step - loss: 0.1593 - accuracy: 0.9716\n",
            "Epoch 88/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1711 - accuracy: 0.9672\n",
            "Epoch 89/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1890 - accuracy: 0.9572\n",
            "Epoch 90/200\n",
            "27/27 [==============================] - 0s 954us/step - loss: 0.1822 - accuracy: 0.9614\n",
            "Epoch 91/200\n",
            "27/27 [==============================] - 0s 948us/step - loss: 0.1697 - accuracy: 0.9798\n",
            "Epoch 92/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1550 - accuracy: 0.9901\n",
            "Epoch 93/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1730 - accuracy: 0.9811\n",
            "Epoch 94/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1990 - accuracy: 0.9369\n",
            "Epoch 95/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1709 - accuracy: 0.9780\n",
            "Epoch 96/200\n",
            "27/27 [==============================] - 0s 961us/step - loss: 0.1501 - accuracy: 0.9682\n",
            "Epoch 97/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1447 - accuracy: 0.9832\n",
            "Epoch 98/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1710 - accuracy: 0.9714\n",
            "Epoch 99/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1233 - accuracy: 0.9863\n",
            "Epoch 100/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1248 - accuracy: 0.9884\n",
            "Epoch 101/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1649 - accuracy: 0.9530\n",
            "Epoch 102/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1578 - accuracy: 0.9740\n",
            "Epoch 103/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1462 - accuracy: 0.9822\n",
            "Epoch 104/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1856 - accuracy: 0.9511\n",
            "Epoch 105/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1585 - accuracy: 0.9621\n",
            "Epoch 106/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1387 - accuracy: 0.9748\n",
            "Epoch 107/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1376 - accuracy: 0.9802\n",
            "Epoch 108/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1537 - accuracy: 0.9587\n",
            "Epoch 109/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1279 - accuracy: 0.9660\n",
            "Epoch 110/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1064 - accuracy: 0.9893\n",
            "Epoch 111/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1237 - accuracy: 0.9697\n",
            "Epoch 112/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1480 - accuracy: 0.9803\n",
            "Epoch 113/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1357 - accuracy: 0.9843\n",
            "Epoch 114/200\n",
            "27/27 [==============================] - 0s 951us/step - loss: 0.1467 - accuracy: 0.9737\n",
            "Epoch 115/200\n",
            "27/27 [==============================] - 0s 961us/step - loss: 0.1221 - accuracy: 0.9863\n",
            "Epoch 116/200\n",
            "27/27 [==============================] - 0s 918us/step - loss: 0.1551 - accuracy: 0.9627\n",
            "Epoch 117/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1369 - accuracy: 0.9729\n",
            "Epoch 118/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1557 - accuracy: 0.9648\n",
            "Epoch 119/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1253 - accuracy: 0.9727\n",
            "Epoch 120/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1101 - accuracy: 0.9867\n",
            "Epoch 121/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1094 - accuracy: 0.9896\n",
            "Epoch 122/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1101 - accuracy: 0.9911\n",
            "Epoch 123/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1200 - accuracy: 0.9762\n",
            "Epoch 124/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0975 - accuracy: 0.9959\n",
            "Epoch 125/200\n",
            "27/27 [==============================] - 0s 975us/step - loss: 0.1242 - accuracy: 0.9676\n",
            "Epoch 126/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1108 - accuracy: 0.9838\n",
            "Epoch 127/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1202 - accuracy: 0.9853\n",
            "Epoch 128/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1087 - accuracy: 0.9856\n",
            "Epoch 129/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0999 - accuracy: 0.9709\n",
            "Epoch 130/200\n",
            "27/27 [==============================] - 0s 997us/step - loss: 0.1501 - accuracy: 0.9695\n",
            "Epoch 131/200\n",
            "27/27 [==============================] - 0s 968us/step - loss: 0.1261 - accuracy: 0.9811\n",
            "Epoch 132/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1344 - accuracy: 0.9824\n",
            "Epoch 133/200\n",
            "27/27 [==============================] - 0s 991us/step - loss: 0.1431 - accuracy: 0.9704\n",
            "Epoch 134/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1192 - accuracy: 0.9903\n",
            "Epoch 135/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1073 - accuracy: 0.9815\n",
            "Epoch 136/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1256 - accuracy: 0.9773\n",
            "Epoch 137/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1112 - accuracy: 0.9763\n",
            "Epoch 138/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1055 - accuracy: 0.9842\n",
            "Epoch 139/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1049 - accuracy: 0.9824\n",
            "Epoch 140/200\n",
            "27/27 [==============================] - 0s 942us/step - loss: 0.1053 - accuracy: 0.9857\n",
            "Epoch 141/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1252 - accuracy: 0.9552\n",
            "Epoch 142/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1283 - accuracy: 0.9682\n",
            "Epoch 143/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1004 - accuracy: 0.9894\n",
            "Epoch 144/200\n",
            "27/27 [==============================] - 0s 974us/step - loss: 0.1053 - accuracy: 0.9956\n",
            "Epoch 145/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1419 - accuracy: 0.9695\n",
            "Epoch 146/200\n",
            "27/27 [==============================] - 0s 962us/step - loss: 0.1080 - accuracy: 0.9758\n",
            "Epoch 147/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0990 - accuracy: 0.9775\n",
            "Epoch 148/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1034 - accuracy: 0.9903\n",
            "Epoch 149/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0946 - accuracy: 0.9933\n",
            "Epoch 150/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1069 - accuracy: 0.9848\n",
            "Epoch 151/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0784 - accuracy: 0.9883\n",
            "Epoch 152/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0923 - accuracy: 0.9892\n",
            "Epoch 153/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1234 - accuracy: 0.9790\n",
            "Epoch 154/200\n",
            "27/27 [==============================] - 0s 997us/step - loss: 0.0856 - accuracy: 0.9892\n",
            "Epoch 155/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1059 - accuracy: 0.9852\n",
            "Epoch 156/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1194 - accuracy: 0.9689\n",
            "Epoch 157/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0926 - accuracy: 0.9809\n",
            "Epoch 158/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0899 - accuracy: 0.9811\n",
            "Epoch 159/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1019 - accuracy: 0.9704\n",
            "Epoch 160/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.9877\n",
            "Epoch 161/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1293 - accuracy: 0.9778\n",
            "Epoch 162/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0780 - accuracy: 0.9961\n",
            "Epoch 163/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0832 - accuracy: 0.9876\n",
            "Epoch 164/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1224 - accuracy: 0.9610\n",
            "Epoch 165/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1336 - accuracy: 0.9585\n",
            "Epoch 166/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0780 - accuracy: 0.9827\n",
            "Epoch 167/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0949 - accuracy: 0.9871\n",
            "Epoch 168/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0957 - accuracy: 0.9897\n",
            "Epoch 169/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0726 - accuracy: 0.9871\n",
            "Epoch 170/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0851 - accuracy: 0.9873\n",
            "Epoch 171/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0782 - accuracy: 0.9887\n",
            "Epoch 172/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0770 - accuracy: 0.9894\n",
            "Epoch 173/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0990 - accuracy: 0.9946\n",
            "Epoch 174/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0990 - accuracy: 0.9893\n",
            "Epoch 175/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0781 - accuracy: 0.9840\n",
            "Epoch 176/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0946 - accuracy: 0.9609\n",
            "Epoch 177/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0828 - accuracy: 0.9838\n",
            "Epoch 178/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0993 - accuracy: 0.9669\n",
            "Epoch 179/200\n",
            "27/27 [==============================] - 0s 976us/step - loss: 0.1075 - accuracy: 0.9746\n",
            "Epoch 180/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1333 - accuracy: 0.9874\n",
            "Epoch 181/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1096 - accuracy: 0.9626\n",
            "Epoch 182/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0694 - accuracy: 0.9932\n",
            "Epoch 183/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0845 - accuracy: 0.9849\n",
            "Epoch 184/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0920 - accuracy: 0.9834\n",
            "Epoch 185/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1047 - accuracy: 0.9879\n",
            "Epoch 186/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0843 - accuracy: 0.9903\n",
            "Epoch 187/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0982 - accuracy: 0.9770\n",
            "Epoch 188/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1068 - accuracy: 0.9654\n",
            "Epoch 189/200\n",
            "27/27 [==============================] - 0s 994us/step - loss: 0.0978 - accuracy: 0.9852\n",
            "Epoch 190/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0793 - accuracy: 0.9893\n",
            "Epoch 191/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0828 - accuracy: 0.9776\n",
            "Epoch 192/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.9796\n",
            "Epoch 193/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0881 - accuracy: 0.9862\n",
            "Epoch 194/200\n",
            "27/27 [==============================] - 0s 955us/step - loss: 0.0892 - accuracy: 0.9584\n",
            "Epoch 195/200\n",
            "27/27 [==============================] - 0s 919us/step - loss: 0.0961 - accuracy: 0.9900\n",
            "Epoch 196/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0926 - accuracy: 0.9863\n",
            "Epoch 197/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0753 - accuracy: 0.9966\n",
            "Epoch 198/200\n",
            "27/27 [==============================] - 0s 983us/step - loss: 0.1107 - accuracy: 0.9657\n",
            "Epoch 199/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0993 - accuracy: 0.9897\n",
            "Epoch 200/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0570 - accuracy: 0.9936\n",
            "3/3 [==============================] - 0s 3ms/step - loss: 0.0660 - accuracy: 1.0000\n",
            "Epoch 1/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.3868 - accuracy: 0.3288\n",
            "Epoch 2/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.3386 - accuracy: 0.2946\n",
            "Epoch 3/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.1761 - accuracy: 0.3146\n",
            "Epoch 4/200\n",
            "27/27 [==============================] - 0s 938us/step - loss: 1.1140 - accuracy: 0.3333\n",
            "Epoch 5/200\n",
            "27/27 [==============================] - 0s 963us/step - loss: 1.0507 - accuracy: 0.3610\n",
            "Epoch 6/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.0877 - accuracy: 0.3148\n",
            "Epoch 7/200\n",
            "27/27 [==============================] - 0s 988us/step - loss: 1.0015 - accuracy: 0.3820\n",
            "Epoch 8/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.9809 - accuracy: 0.3976\n",
            "Epoch 9/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.9650 - accuracy: 0.4081\n",
            "Epoch 10/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.9619 - accuracy: 0.4625\n",
            "Epoch 11/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.9574 - accuracy: 0.3729\n",
            "Epoch 12/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.9160 - accuracy: 0.4086\n",
            "Epoch 13/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.9248 - accuracy: 0.3684\n",
            "Epoch 14/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.8797 - accuracy: 0.4645\n",
            "Epoch 15/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.8471 - accuracy: 0.4516\n",
            "Epoch 16/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.9022 - accuracy: 0.4740\n",
            "Epoch 17/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.8596 - accuracy: 0.4471\n",
            "Epoch 18/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.8132 - accuracy: 0.4854\n",
            "Epoch 19/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.8426 - accuracy: 0.4495\n",
            "Epoch 20/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.8266 - accuracy: 0.4226\n",
            "Epoch 21/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.7816 - accuracy: 0.5226\n",
            "Epoch 22/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.7774 - accuracy: 0.5157\n",
            "Epoch 23/200\n",
            "27/27 [==============================] - 0s 956us/step - loss: 0.7401 - accuracy: 0.5031\n",
            "Epoch 24/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.7360 - accuracy: 0.6009\n",
            "Epoch 25/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.7272 - accuracy: 0.8114\n",
            "Epoch 26/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.7168 - accuracy: 0.7980\n",
            "Epoch 27/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.7010 - accuracy: 0.8816\n",
            "Epoch 28/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6453 - accuracy: 0.8525\n",
            "Epoch 29/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6631 - accuracy: 0.8631\n",
            "Epoch 30/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6305 - accuracy: 0.8922\n",
            "Epoch 31/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6396 - accuracy: 0.9218\n",
            "Epoch 32/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6305 - accuracy: 0.9418\n",
            "Epoch 33/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5967 - accuracy: 0.8744\n",
            "Epoch 34/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6009 - accuracy: 0.8928\n",
            "Epoch 35/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5915 - accuracy: 0.9476\n",
            "Epoch 36/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5917 - accuracy: 0.9269\n",
            "Epoch 37/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5601 - accuracy: 0.8981\n",
            "Epoch 38/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5516 - accuracy: 0.9472\n",
            "Epoch 39/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5666 - accuracy: 0.8899\n",
            "Epoch 40/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5189 - accuracy: 0.9291\n",
            "Epoch 41/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4937 - accuracy: 0.9313\n",
            "Epoch 42/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5035 - accuracy: 0.9436\n",
            "Epoch 43/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4900 - accuracy: 0.9514\n",
            "Epoch 44/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5097 - accuracy: 0.9214\n",
            "Epoch 45/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4888 - accuracy: 0.9237\n",
            "Epoch 46/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4879 - accuracy: 0.9240\n",
            "Epoch 47/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4598 - accuracy: 0.9632\n",
            "Epoch 48/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4880 - accuracy: 0.9353\n",
            "Epoch 49/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4737 - accuracy: 0.9489\n",
            "Epoch 50/200\n",
            "27/27 [==============================] - 0s 993us/step - loss: 0.4341 - accuracy: 0.9437\n",
            "Epoch 51/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4415 - accuracy: 0.9439\n",
            "Epoch 52/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3999 - accuracy: 0.9596\n",
            "Epoch 53/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3992 - accuracy: 0.9521\n",
            "Epoch 54/200\n",
            "27/27 [==============================] - 0s 959us/step - loss: 0.4265 - accuracy: 0.9742\n",
            "Epoch 55/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4194 - accuracy: 0.9694\n",
            "Epoch 56/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4333 - accuracy: 0.9605\n",
            "Epoch 57/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4213 - accuracy: 0.9692\n",
            "Epoch 58/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4116 - accuracy: 0.9595\n",
            "Epoch 59/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4027 - accuracy: 0.9403\n",
            "Epoch 60/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3912 - accuracy: 0.9576\n",
            "Epoch 61/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3892 - accuracy: 0.9650\n",
            "Epoch 62/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3763 - accuracy: 0.9757\n",
            "Epoch 63/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3455 - accuracy: 0.9685\n",
            "Epoch 64/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3747 - accuracy: 0.9785\n",
            "Epoch 65/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3380 - accuracy: 0.9828\n",
            "Epoch 66/200\n",
            "27/27 [==============================] - 0s 974us/step - loss: 0.3231 - accuracy: 0.9834\n",
            "Epoch 67/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3612 - accuracy: 0.9654\n",
            "Epoch 68/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3763 - accuracy: 0.9628\n",
            "Epoch 69/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3749 - accuracy: 0.9827\n",
            "Epoch 70/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3541 - accuracy: 0.9834\n",
            "Epoch 71/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3219 - accuracy: 0.9816\n",
            "Epoch 72/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3203 - accuracy: 0.9822\n",
            "Epoch 73/200\n",
            "27/27 [==============================] - 0s 972us/step - loss: 0.3266 - accuracy: 0.9793\n",
            "Epoch 74/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.9722\n",
            "Epoch 75/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3204 - accuracy: 0.9573\n",
            "Epoch 76/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3385 - accuracy: 0.9724\n",
            "Epoch 77/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3319 - accuracy: 0.9309\n",
            "Epoch 78/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3326 - accuracy: 0.9217\n",
            "Epoch 79/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2994 - accuracy: 0.9783\n",
            "Epoch 80/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3334 - accuracy: 0.9772\n",
            "Epoch 81/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.9780\n",
            "Epoch 82/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2909 - accuracy: 0.9875\n",
            "Epoch 83/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2725 - accuracy: 0.9822\n",
            "Epoch 84/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2931 - accuracy: 0.9734\n",
            "Epoch 85/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3243 - accuracy: 0.9496\n",
            "Epoch 86/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2698 - accuracy: 0.9894\n",
            "Epoch 87/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3050 - accuracy: 0.9613\n",
            "Epoch 88/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2994 - accuracy: 0.9774\n",
            "Epoch 89/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2871 - accuracy: 0.9613\n",
            "Epoch 90/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2829 - accuracy: 0.9921\n",
            "Epoch 91/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2913 - accuracy: 0.9676\n",
            "Epoch 92/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2434 - accuracy: 0.9806\n",
            "Epoch 93/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2944 - accuracy: 0.9562\n",
            "Epoch 94/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2877 - accuracy: 0.9782\n",
            "Epoch 95/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2993 - accuracy: 0.9548\n",
            "Epoch 96/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2704 - accuracy: 0.9913\n",
            "Epoch 97/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2497 - accuracy: 0.9822\n",
            "Epoch 98/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2622 - accuracy: 0.9771\n",
            "Epoch 99/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2831 - accuracy: 0.9755\n",
            "Epoch 100/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2532 - accuracy: 0.9809\n",
            "Epoch 101/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2322 - accuracy: 0.9803\n",
            "Epoch 102/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2406 - accuracy: 0.9859\n",
            "Epoch 103/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2412 - accuracy: 0.9901\n",
            "Epoch 104/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2271 - accuracy: 0.9901\n",
            "Epoch 105/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2521 - accuracy: 0.9610\n",
            "Epoch 106/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2432 - accuracy: 0.9877\n",
            "Epoch 107/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2678 - accuracy: 0.9727\n",
            "Epoch 108/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2346 - accuracy: 0.9841\n",
            "Epoch 109/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2620 - accuracy: 0.9712\n",
            "Epoch 110/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2483 - accuracy: 0.9650\n",
            "Epoch 111/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2288 - accuracy: 0.9593\n",
            "Epoch 112/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2403 - accuracy: 0.9842\n",
            "Epoch 113/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2475 - accuracy: 0.9823\n",
            "Epoch 114/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2246 - accuracy: 0.9906\n",
            "Epoch 115/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2215 - accuracy: 0.9811\n",
            "Epoch 116/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2147 - accuracy: 0.9877\n",
            "Epoch 117/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2215 - accuracy: 0.9900\n",
            "Epoch 118/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2172 - accuracy: 0.9703\n",
            "Epoch 119/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2284 - accuracy: 0.9788\n",
            "Epoch 120/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2403 - accuracy: 0.9695\n",
            "Epoch 121/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2170 - accuracy: 0.9673\n",
            "Epoch 122/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2257 - accuracy: 0.9658\n",
            "Epoch 123/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1982 - accuracy: 0.9793\n",
            "Epoch 124/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2150 - accuracy: 0.9787\n",
            "Epoch 125/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2307 - accuracy: 0.9588\n",
            "Epoch 126/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2180 - accuracy: 0.9588\n",
            "Epoch 127/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1992 - accuracy: 0.9735\n",
            "Epoch 128/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2233 - accuracy: 0.9863\n",
            "Epoch 129/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2003 - accuracy: 0.9724\n",
            "Epoch 130/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2074 - accuracy: 0.9787\n",
            "Epoch 131/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1881 - accuracy: 0.9965\n",
            "Epoch 132/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1940 - accuracy: 0.9837\n",
            "Epoch 133/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2047 - accuracy: 0.9776\n",
            "Epoch 134/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1939 - accuracy: 0.9773\n",
            "Epoch 135/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2029 - accuracy: 0.9734\n",
            "Epoch 136/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2054 - accuracy: 0.9674\n",
            "Epoch 137/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2345 - accuracy: 0.9733\n",
            "Epoch 138/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1961 - accuracy: 0.9767\n",
            "Epoch 139/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2082 - accuracy: 0.9634\n",
            "Epoch 140/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2125 - accuracy: 0.9797\n",
            "Epoch 141/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1741 - accuracy: 0.9903\n",
            "Epoch 142/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1829 - accuracy: 0.9877\n",
            "Epoch 143/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1945 - accuracy: 0.9947\n",
            "Epoch 144/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1825 - accuracy: 0.9938\n",
            "Epoch 145/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1757 - accuracy: 0.9767\n",
            "Epoch 146/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1953 - accuracy: 0.9660\n",
            "Epoch 147/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1831 - accuracy: 0.9755\n",
            "Epoch 148/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1801 - accuracy: 0.9865\n",
            "Epoch 149/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1574 - accuracy: 0.9870\n",
            "Epoch 150/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1476 - accuracy: 0.9886\n",
            "Epoch 151/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1826 - accuracy: 0.9751\n",
            "Epoch 152/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1694 - accuracy: 0.9765\n",
            "Epoch 153/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1732 - accuracy: 0.9803\n",
            "Epoch 154/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1538 - accuracy: 0.9855\n",
            "Epoch 155/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1587 - accuracy: 0.9766\n",
            "Epoch 156/200\n",
            "27/27 [==============================] - 0s 998us/step - loss: 0.1519 - accuracy: 0.9937\n",
            "Epoch 157/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1639 - accuracy: 0.9846\n",
            "Epoch 158/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1906 - accuracy: 0.9764\n",
            "Epoch 159/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1937 - accuracy: 0.9597\n",
            "Epoch 160/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1528 - accuracy: 0.9893\n",
            "Epoch 161/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1964 - accuracy: 0.9660\n",
            "Epoch 162/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1590 - accuracy: 0.9859\n",
            "Epoch 163/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1383 - accuracy: 0.9923\n",
            "Epoch 164/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1886 - accuracy: 0.9570\n",
            "Epoch 165/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1567 - accuracy: 0.9788\n",
            "Epoch 166/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1743 - accuracy: 0.9910\n",
            "Epoch 167/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1642 - accuracy: 0.9540\n",
            "Epoch 168/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1853 - accuracy: 0.9727\n",
            "Epoch 169/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1635 - accuracy: 0.9695\n",
            "Epoch 170/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1822 - accuracy: 0.9676\n",
            "Epoch 171/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1381 - accuracy: 0.9897\n",
            "Epoch 172/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1323 - accuracy: 0.9927\n",
            "Epoch 173/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1403 - accuracy: 0.9888\n",
            "Epoch 174/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1539 - accuracy: 0.9748\n",
            "Epoch 175/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1386 - accuracy: 0.9783\n",
            "Epoch 176/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1605 - accuracy: 0.9776\n",
            "Epoch 177/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1551 - accuracy: 0.9729\n",
            "Epoch 178/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1733 - accuracy: 0.9582\n",
            "Epoch 179/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1342 - accuracy: 0.9756\n",
            "Epoch 180/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1381 - accuracy: 0.9748\n",
            "Epoch 181/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1391 - accuracy: 0.9806\n",
            "Epoch 182/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1464 - accuracy: 0.9913\n",
            "Epoch 183/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1491 - accuracy: 0.9839\n",
            "Epoch 184/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1361 - accuracy: 0.9859\n",
            "Epoch 185/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1434 - accuracy: 0.9789\n",
            "Epoch 186/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1619 - accuracy: 0.9760\n",
            "Epoch 187/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1253 - accuracy: 0.9817\n",
            "Epoch 188/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1794 - accuracy: 0.9854\n",
            "Epoch 189/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1337 - accuracy: 0.9884\n",
            "Epoch 190/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1103 - accuracy: 0.9892\n",
            "Epoch 191/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1242 - accuracy: 0.9829\n",
            "Epoch 192/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1327 - accuracy: 0.9931\n",
            "Epoch 193/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1237 - accuracy: 0.9958\n",
            "Epoch 194/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1579 - accuracy: 0.9763\n",
            "Epoch 195/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1123 - accuracy: 0.9813\n",
            "Epoch 196/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1166 - accuracy: 0.9839\n",
            "Epoch 197/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1255 - accuracy: 0.9797\n",
            "Epoch 198/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1309 - accuracy: 0.9788\n",
            "Epoch 199/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1391 - accuracy: 0.9717\n",
            "Epoch 200/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1286 - accuracy: 0.9987\n",
            "3/3 [==============================] - 0s 2ms/step - loss: 0.0864 - accuracy: 1.0000\n",
            "Epoch 1/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 2.5725 - accuracy: 0.0000e+00\n",
            "Epoch 2/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 2.0892 - accuracy: 0.0035\n",
            "Epoch 3/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.8213 - accuracy: 0.0451\n",
            "Epoch 4/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.4532 - accuracy: 0.2246\n",
            "Epoch 5/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.2359 - accuracy: 0.3844\n",
            "Epoch 6/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.1521 - accuracy: 0.3188\n",
            "Epoch 7/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.0634 - accuracy: 0.3738\n",
            "Epoch 8/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.9994 - accuracy: 0.5708\n",
            "Epoch 9/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.9340 - accuracy: 0.7513\n",
            "Epoch 10/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.9123 - accuracy: 0.6479\n",
            "Epoch 11/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.8728 - accuracy: 0.6778\n",
            "Epoch 12/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.8364 - accuracy: 0.6923\n",
            "Epoch 13/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.7808 - accuracy: 0.7479\n",
            "Epoch 14/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.7747 - accuracy: 0.6627\n",
            "Epoch 15/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.7452 - accuracy: 0.6364\n",
            "Epoch 16/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.7103 - accuracy: 0.6959\n",
            "Epoch 17/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6776 - accuracy: 0.6820\n",
            "Epoch 18/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6574 - accuracy: 0.7250\n",
            "Epoch 19/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6388 - accuracy: 0.6755\n",
            "Epoch 20/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6160 - accuracy: 0.6951\n",
            "Epoch 21/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5564 - accuracy: 0.7270\n",
            "Epoch 22/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.5692 - accuracy: 0.6686\n",
            "Epoch 23/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5663 - accuracy: 0.6848\n",
            "Epoch 24/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5598 - accuracy: 0.6680\n",
            "Epoch 25/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5286 - accuracy: 0.7056\n",
            "Epoch 26/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5008 - accuracy: 0.7244\n",
            "Epoch 27/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5198 - accuracy: 0.7049\n",
            "Epoch 28/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5414 - accuracy: 0.6640\n",
            "Epoch 29/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4859 - accuracy: 0.7454\n",
            "Epoch 30/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5000 - accuracy: 0.7045\n",
            "Epoch 31/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5202 - accuracy: 0.6987\n",
            "Epoch 32/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4763 - accuracy: 0.8256\n",
            "Epoch 33/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3905 - accuracy: 0.8062\n",
            "Epoch 34/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4494 - accuracy: 0.7848\n",
            "Epoch 35/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4211 - accuracy: 0.8908\n",
            "Epoch 36/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4686 - accuracy: 0.8266\n",
            "Epoch 37/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4182 - accuracy: 0.9154\n",
            "Epoch 38/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4104 - accuracy: 0.9166\n",
            "Epoch 39/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4152 - accuracy: 0.9233\n",
            "Epoch 40/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4558 - accuracy: 0.8731\n",
            "Epoch 41/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4443 - accuracy: 0.8818\n",
            "Epoch 42/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4072 - accuracy: 0.9243\n",
            "Epoch 43/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3542 - accuracy: 0.9202\n",
            "Epoch 44/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4276 - accuracy: 0.8926\n",
            "Epoch 45/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3819 - accuracy: 0.9595\n",
            "Epoch 46/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3964 - accuracy: 0.9140\n",
            "Epoch 47/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3838 - accuracy: 0.9517\n",
            "Epoch 48/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3887 - accuracy: 0.9475\n",
            "Epoch 49/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3269 - accuracy: 0.9807\n",
            "Epoch 50/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3368 - accuracy: 0.9795\n",
            "Epoch 51/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3374 - accuracy: 0.9580\n",
            "Epoch 52/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3275 - accuracy: 0.9513\n",
            "Epoch 53/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3124 - accuracy: 0.9741\n",
            "Epoch 54/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3339 - accuracy: 0.9728\n",
            "Epoch 55/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3425 - accuracy: 0.9500\n",
            "Epoch 56/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3337 - accuracy: 0.9587\n",
            "Epoch 57/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3121 - accuracy: 0.9536\n",
            "Epoch 58/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2894 - accuracy: 0.9645\n",
            "Epoch 59/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3159 - accuracy: 0.9646\n",
            "Epoch 60/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3164 - accuracy: 0.9756\n",
            "Epoch 61/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2908 - accuracy: 0.9864\n",
            "Epoch 62/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3185 - accuracy: 0.9779\n",
            "Epoch 63/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2798 - accuracy: 0.9722\n",
            "Epoch 64/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2946 - accuracy: 0.9767\n",
            "Epoch 65/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2821 - accuracy: 0.9800\n",
            "Epoch 66/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2573 - accuracy: 0.9924\n",
            "Epoch 67/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2646 - accuracy: 0.9836\n",
            "Epoch 68/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2720 - accuracy: 0.9933\n",
            "Epoch 69/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2566 - accuracy: 0.9871\n",
            "Epoch 70/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2600 - accuracy: 0.9820\n",
            "Epoch 71/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2526 - accuracy: 0.9802\n",
            "Epoch 72/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2552 - accuracy: 0.9855\n",
            "Epoch 73/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2072 - accuracy: 0.9826\n",
            "Epoch 74/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2209 - accuracy: 0.9958\n",
            "Epoch 75/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2285 - accuracy: 0.9681\n",
            "Epoch 76/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2451 - accuracy: 0.9865\n",
            "Epoch 77/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2176 - accuracy: 0.9664\n",
            "Epoch 78/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2411 - accuracy: 0.9826\n",
            "Epoch 79/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2424 - accuracy: 0.9918\n",
            "Epoch 80/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2054 - accuracy: 0.9711\n",
            "Epoch 81/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1950 - accuracy: 0.9814\n",
            "Epoch 82/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2366 - accuracy: 0.9500\n",
            "Epoch 83/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2450 - accuracy: 0.9451\n",
            "Epoch 84/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1969 - accuracy: 0.9580\n",
            "Epoch 85/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2287 - accuracy: 0.9682\n",
            "Epoch 86/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1878 - accuracy: 0.9888\n",
            "Epoch 87/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1769 - accuracy: 0.9889\n",
            "Epoch 88/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1967 - accuracy: 0.9826\n",
            "Epoch 89/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2025 - accuracy: 0.9688\n",
            "Epoch 90/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2043 - accuracy: 0.9839\n",
            "Epoch 91/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1825 - accuracy: 0.9799\n",
            "Epoch 92/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1946 - accuracy: 0.9949\n",
            "Epoch 93/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1800 - accuracy: 0.9924\n",
            "Epoch 94/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2044 - accuracy: 0.9881\n",
            "Epoch 95/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1790 - accuracy: 0.9892\n",
            "Epoch 96/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1498 - accuracy: 0.9815\n",
            "Epoch 97/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1774 - accuracy: 0.9881\n",
            "Epoch 98/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1832 - accuracy: 0.9532\n",
            "Epoch 99/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1574 - accuracy: 0.9829\n",
            "Epoch 100/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1742 - accuracy: 0.9704\n",
            "Epoch 101/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1645 - accuracy: 0.9692\n",
            "Epoch 102/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1637 - accuracy: 0.9891\n",
            "Epoch 103/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1828 - accuracy: 0.9755\n",
            "Epoch 104/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1631 - accuracy: 0.9796\n",
            "Epoch 105/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1948 - accuracy: 0.9468\n",
            "Epoch 106/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1535 - accuracy: 0.9802\n",
            "Epoch 107/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1437 - accuracy: 0.9854\n",
            "Epoch 108/200\n",
            "27/27 [==============================] - 0s 990us/step - loss: 0.1746 - accuracy: 0.9523\n",
            "Epoch 109/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1513 - accuracy: 0.9878\n",
            "Epoch 110/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1395 - accuracy: 0.9874\n",
            "Epoch 111/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1313 - accuracy: 0.9850\n",
            "Epoch 112/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1453 - accuracy: 0.9829\n",
            "Epoch 113/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1722 - accuracy: 0.9689\n",
            "Epoch 114/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1263 - accuracy: 0.9901\n",
            "Epoch 115/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1627 - accuracy: 0.9600\n",
            "Epoch 116/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1295 - accuracy: 0.9894\n",
            "Epoch 117/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1143 - accuracy: 0.9905\n",
            "Epoch 118/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1614 - accuracy: 0.9560\n",
            "Epoch 119/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1181 - accuracy: 0.9625\n",
            "Epoch 120/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1254 - accuracy: 0.9834\n",
            "Epoch 121/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1208 - accuracy: 0.9942\n",
            "Epoch 122/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1312 - accuracy: 0.9845\n",
            "Epoch 123/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1202 - accuracy: 0.9941\n",
            "Epoch 124/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0959 - accuracy: 0.9913\n",
            "Epoch 125/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1329 - accuracy: 0.9835\n",
            "Epoch 126/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1123 - accuracy: 0.9827\n",
            "Epoch 127/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1285 - accuracy: 0.9702\n",
            "Epoch 128/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1042 - accuracy: 0.9899\n",
            "Epoch 129/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1231 - accuracy: 0.9818\n",
            "Epoch 130/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1255 - accuracy: 0.9744\n",
            "Epoch 131/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0980 - accuracy: 0.9951\n",
            "Epoch 132/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1070 - accuracy: 0.9968\n",
            "Epoch 133/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0877 - accuracy: 0.9950\n",
            "Epoch 134/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1218 - accuracy: 0.9759\n",
            "Epoch 135/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0965 - accuracy: 0.9854\n",
            "Epoch 136/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0941 - accuracy: 0.9945\n",
            "Epoch 137/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1049 - accuracy: 0.9913\n",
            "Epoch 138/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1026 - accuracy: 0.9755\n",
            "Epoch 139/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1505 - accuracy: 0.9385\n",
            "Epoch 140/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1087 - accuracy: 0.9732\n",
            "Epoch 141/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1006 - accuracy: 0.9838\n",
            "Epoch 142/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0959 - accuracy: 0.9910\n",
            "Epoch 143/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1197 - accuracy: 0.9787\n",
            "Epoch 144/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1123 - accuracy: 0.9480\n",
            "Epoch 145/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1230 - accuracy: 0.9480\n",
            "Epoch 146/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1146 - accuracy: 0.9826\n",
            "Epoch 147/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0939 - accuracy: 0.9742\n",
            "Epoch 148/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1105 - accuracy: 0.9681\n",
            "Epoch 149/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0978 - accuracy: 0.9764\n",
            "Epoch 150/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0879 - accuracy: 0.9758\n",
            "Epoch 151/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0915 - accuracy: 0.9851\n",
            "Epoch 152/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0909 - accuracy: 0.9906\n",
            "Epoch 153/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0959 - accuracy: 0.9910\n",
            "Epoch 154/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1179 - accuracy: 0.9630\n",
            "Epoch 155/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1160 - accuracy: 0.9770\n",
            "Epoch 156/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0909 - accuracy: 0.9831\n",
            "Epoch 157/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0802 - accuracy: 0.9857\n",
            "Epoch 158/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0844 - accuracy: 0.9815\n",
            "Epoch 159/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0873 - accuracy: 0.9888\n",
            "Epoch 160/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1127 - accuracy: 0.9663\n",
            "Epoch 161/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0943 - accuracy: 0.9763\n",
            "Epoch 162/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1281 - accuracy: 0.9499\n",
            "Epoch 163/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0618 - accuracy: 0.9845\n",
            "Epoch 164/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0814 - accuracy: 0.9790\n",
            "Epoch 165/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0846 - accuracy: 0.9729\n",
            "Epoch 166/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0848 - accuracy: 0.9717\n",
            "Epoch 167/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1072 - accuracy: 0.9818\n",
            "Epoch 168/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.9875\n",
            "Epoch 169/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0894 - accuracy: 0.9897\n",
            "Epoch 170/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0716 - accuracy: 0.9897\n",
            "Epoch 171/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1101 - accuracy: 0.9532\n",
            "Epoch 172/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0943 - accuracy: 0.9840\n",
            "Epoch 173/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1040 - accuracy: 0.9487\n",
            "Epoch 174/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1027 - accuracy: 0.9691\n",
            "Epoch 175/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0866 - accuracy: 0.9888\n",
            "Epoch 176/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0836 - accuracy: 0.9739\n",
            "Epoch 177/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0738 - accuracy: 0.9891\n",
            "Epoch 178/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0930 - accuracy: 0.9545\n",
            "Epoch 179/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1100 - accuracy: 0.9411\n",
            "Epoch 180/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0725 - accuracy: 0.9882\n",
            "Epoch 181/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0566 - accuracy: 0.9932\n",
            "Epoch 182/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0751 - accuracy: 0.9803\n",
            "Epoch 183/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1036 - accuracy: 0.9807\n",
            "Epoch 184/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0780 - accuracy: 0.9870\n",
            "Epoch 185/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0679 - accuracy: 0.9807\n",
            "Epoch 186/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0859 - accuracy: 0.9828\n",
            "Epoch 187/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0738 - accuracy: 0.9828\n",
            "Epoch 188/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0820 - accuracy: 0.9833\n",
            "Epoch 189/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0687 - accuracy: 0.9925\n",
            "Epoch 190/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0821 - accuracy: 0.9747\n",
            "Epoch 191/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0901 - accuracy: 0.9756\n",
            "Epoch 192/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0608 - accuracy: 0.9877\n",
            "Epoch 193/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0755 - accuracy: 0.9804\n",
            "Epoch 194/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0906 - accuracy: 0.9811\n",
            "Epoch 195/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0864 - accuracy: 0.9820\n",
            "Epoch 196/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1013 - accuracy: 0.9546\n",
            "Epoch 197/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0679 - accuracy: 0.9844\n",
            "Epoch 198/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1283 - accuracy: 0.9487\n",
            "Epoch 199/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1207 - accuracy: 0.9627\n",
            "Epoch 200/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0778 - accuracy: 0.9930\n",
            "3/3 [==============================] - 0s 3ms/step - loss: 0.1297 - accuracy: 0.9333\n",
            "Epoch 1/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.3428 - accuracy: 0.3573\n",
            "Epoch 2/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.0878 - accuracy: 0.4615\n",
            "Epoch 3/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.0881 - accuracy: 0.4403\n",
            "Epoch 4/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.0573 - accuracy: 0.4519\n",
            "Epoch 5/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.0012 - accuracy: 0.4569\n",
            "Epoch 6/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.9835 - accuracy: 0.7036\n",
            "Epoch 7/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.9813 - accuracy: 0.7211\n",
            "Epoch 8/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.9470 - accuracy: 0.7555\n",
            "Epoch 9/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.8876 - accuracy: 0.8189\n",
            "Epoch 10/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.8804 - accuracy: 0.7676\n",
            "Epoch 11/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.8476 - accuracy: 0.7840\n",
            "Epoch 12/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.8103 - accuracy: 0.8020\n",
            "Epoch 13/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.8322 - accuracy: 0.7489\n",
            "Epoch 14/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.7819 - accuracy: 0.7971\n",
            "Epoch 15/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.7669 - accuracy: 0.7567\n",
            "Epoch 16/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.7464 - accuracy: 0.7993\n",
            "Epoch 17/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.7042 - accuracy: 0.8033\n",
            "Epoch 18/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.7147 - accuracy: 0.7661\n",
            "Epoch 19/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6631 - accuracy: 0.7819\n",
            "Epoch 20/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6169 - accuracy: 0.7924\n",
            "Epoch 21/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5952 - accuracy: 0.8563\n",
            "Epoch 22/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5914 - accuracy: 0.8252\n",
            "Epoch 23/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5628 - accuracy: 0.8363\n",
            "Epoch 24/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5626 - accuracy: 0.7958\n",
            "Epoch 25/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5084 - accuracy: 0.8536\n",
            "Epoch 26/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5180 - accuracy: 0.8302\n",
            "Epoch 27/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5321 - accuracy: 0.8364\n",
            "Epoch 28/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4798 - accuracy: 0.8513\n",
            "Epoch 29/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5102 - accuracy: 0.8015\n",
            "Epoch 30/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4725 - accuracy: 0.8478\n",
            "Epoch 31/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4646 - accuracy: 0.8048\n",
            "Epoch 32/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4250 - accuracy: 0.8953\n",
            "Epoch 33/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4897 - accuracy: 0.8359\n",
            "Epoch 34/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4004 - accuracy: 0.8744\n",
            "Epoch 35/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4132 - accuracy: 0.8477\n",
            "Epoch 36/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4051 - accuracy: 0.8842\n",
            "Epoch 37/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3814 - accuracy: 0.8979\n",
            "Epoch 38/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3581 - accuracy: 0.8912\n",
            "Epoch 39/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4083 - accuracy: 0.8612\n",
            "Epoch 40/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3722 - accuracy: 0.8740\n",
            "Epoch 41/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4035 - accuracy: 0.8575\n",
            "Epoch 42/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3548 - accuracy: 0.9254\n",
            "Epoch 43/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3384 - accuracy: 0.8976\n",
            "Epoch 44/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3915 - accuracy: 0.8870\n",
            "Epoch 45/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3594 - accuracy: 0.8773\n",
            "Epoch 46/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3276 - accuracy: 0.9438\n",
            "Epoch 47/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3262 - accuracy: 0.9221\n",
            "Epoch 48/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3128 - accuracy: 0.9371\n",
            "Epoch 49/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3231 - accuracy: 0.9185\n",
            "Epoch 50/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3342 - accuracy: 0.9168\n",
            "Epoch 51/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2898 - accuracy: 0.9483\n",
            "Epoch 52/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2804 - accuracy: 0.9314\n",
            "Epoch 53/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2595 - accuracy: 0.9545\n",
            "Epoch 54/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2946 - accuracy: 0.9016\n",
            "Epoch 55/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2733 - accuracy: 0.9326\n",
            "Epoch 56/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3273 - accuracy: 0.8927\n",
            "Epoch 57/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2552 - accuracy: 0.9373\n",
            "Epoch 58/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2872 - accuracy: 0.9468\n",
            "Epoch 59/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2812 - accuracy: 0.8884\n",
            "Epoch 60/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2598 - accuracy: 0.9235\n",
            "Epoch 61/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2767 - accuracy: 0.9214\n",
            "Epoch 62/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3016 - accuracy: 0.9100\n",
            "Epoch 63/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2320 - accuracy: 0.9503\n",
            "Epoch 64/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2638 - accuracy: 0.9150\n",
            "Epoch 65/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2428 - accuracy: 0.9446\n",
            "Epoch 66/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2281 - accuracy: 0.9496\n",
            "Epoch 67/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2092 - accuracy: 0.9599\n",
            "Epoch 68/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2318 - accuracy: 0.9498\n",
            "Epoch 69/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2395 - accuracy: 0.9413\n",
            "Epoch 70/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2699 - accuracy: 0.9040\n",
            "Epoch 71/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2200 - accuracy: 0.9507\n",
            "Epoch 72/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2081 - accuracy: 0.9566\n",
            "Epoch 73/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2200 - accuracy: 0.9464\n",
            "Epoch 74/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2025 - accuracy: 0.9664\n",
            "Epoch 75/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2174 - accuracy: 0.9470\n",
            "Epoch 76/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2458 - accuracy: 0.9173\n",
            "Epoch 77/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2278 - accuracy: 0.9558\n",
            "Epoch 78/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1873 - accuracy: 0.9632\n",
            "Epoch 79/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1945 - accuracy: 0.9500\n",
            "Epoch 80/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2204 - accuracy: 0.9541\n",
            "Epoch 81/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1807 - accuracy: 0.9667\n",
            "Epoch 82/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2078 - accuracy: 0.9270\n",
            "Epoch 83/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2106 - accuracy: 0.9387\n",
            "Epoch 84/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2020 - accuracy: 0.9441\n",
            "Epoch 85/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1708 - accuracy: 0.9664\n",
            "Epoch 86/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1753 - accuracy: 0.9537\n",
            "Epoch 87/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1651 - accuracy: 0.9699\n",
            "Epoch 88/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1965 - accuracy: 0.9312\n",
            "Epoch 89/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1614 - accuracy: 0.9695\n",
            "Epoch 90/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2080 - accuracy: 0.9377\n",
            "Epoch 91/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1732 - accuracy: 0.9524\n",
            "Epoch 92/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1714 - accuracy: 0.9602\n",
            "Epoch 93/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1969 - accuracy: 0.9375\n",
            "Epoch 94/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1515 - accuracy: 0.9739\n",
            "Epoch 95/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1459 - accuracy: 0.9683\n",
            "Epoch 96/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1596 - accuracy: 0.9674\n",
            "Epoch 97/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1251 - accuracy: 0.9772\n",
            "Epoch 98/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1555 - accuracy: 0.9498\n",
            "Epoch 99/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1702 - accuracy: 0.9367\n",
            "Epoch 100/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1477 - accuracy: 0.9585\n",
            "Epoch 101/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1066 - accuracy: 0.9836\n",
            "Epoch 102/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1239 - accuracy: 0.9805\n",
            "Epoch 103/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1229 - accuracy: 0.9746\n",
            "Epoch 104/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1528 - accuracy: 0.9718\n",
            "Epoch 105/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1413 - accuracy: 0.9698\n",
            "Epoch 106/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1525 - accuracy: 0.9454\n",
            "Epoch 107/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1636 - accuracy: 0.9346\n",
            "Epoch 108/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1564 - accuracy: 0.9532\n",
            "Epoch 109/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1429 - accuracy: 0.9607\n",
            "Epoch 110/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1883 - accuracy: 0.9286\n",
            "Epoch 111/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1365 - accuracy: 0.9659\n",
            "Epoch 112/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1433 - accuracy: 0.9534\n",
            "Epoch 113/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1339 - accuracy: 0.9599\n",
            "Epoch 114/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1255 - accuracy: 0.9756\n",
            "Epoch 115/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1202 - accuracy: 0.9553\n",
            "Epoch 116/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1506 - accuracy: 0.9268\n",
            "Epoch 117/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1285 - accuracy: 0.9632\n",
            "Epoch 118/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1202 - accuracy: 0.9809\n",
            "Epoch 119/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1316 - accuracy: 0.9697\n",
            "Epoch 120/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0996 - accuracy: 0.9800\n",
            "Epoch 121/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1295 - accuracy: 0.9668\n",
            "Epoch 122/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1219 - accuracy: 0.9585\n",
            "Epoch 123/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1216 - accuracy: 0.9650\n",
            "Epoch 124/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1260 - accuracy: 0.9520\n",
            "Epoch 125/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1265 - accuracy: 0.9601\n",
            "Epoch 126/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1496 - accuracy: 0.9322\n",
            "Epoch 127/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1580 - accuracy: 0.9281\n",
            "Epoch 128/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0872 - accuracy: 0.9822\n",
            "Epoch 129/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0904 - accuracy: 0.9706\n",
            "Epoch 130/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1337 - accuracy: 0.9589\n",
            "Epoch 131/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1668 - accuracy: 0.9374\n",
            "Epoch 132/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1249 - accuracy: 0.9673\n",
            "Epoch 133/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1524 - accuracy: 0.9400\n",
            "Epoch 134/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1056 - accuracy: 0.9887\n",
            "Epoch 135/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0962 - accuracy: 0.9795\n",
            "Epoch 136/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0919 - accuracy: 0.9791\n",
            "Epoch 137/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1412 - accuracy: 0.9321\n",
            "Epoch 138/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1190 - accuracy: 0.9615\n",
            "Epoch 139/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1375 - accuracy: 0.9449\n",
            "Epoch 140/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1051 - accuracy: 0.9609\n",
            "Epoch 141/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1051 - accuracy: 0.9691\n",
            "Epoch 142/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1203 - accuracy: 0.9425\n",
            "Epoch 143/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1219 - accuracy: 0.9621\n",
            "Epoch 144/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1092 - accuracy: 0.9712\n",
            "Epoch 145/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1047 - accuracy: 0.9654\n",
            "Epoch 146/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1189 - accuracy: 0.9564\n",
            "Epoch 147/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0757 - accuracy: 0.9706\n",
            "Epoch 148/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1005 - accuracy: 0.9545\n",
            "Epoch 149/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1308 - accuracy: 0.9353\n",
            "Epoch 150/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1039 - accuracy: 0.9600\n",
            "Epoch 151/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1480 - accuracy: 0.9298\n",
            "Epoch 152/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1069 - accuracy: 0.9790\n",
            "Epoch 153/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1127 - accuracy: 0.9522\n",
            "Epoch 154/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1267 - accuracy: 0.9581\n",
            "Epoch 155/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1157 - accuracy: 0.9497\n",
            "Epoch 156/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0866 - accuracy: 0.9703\n",
            "Epoch 157/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0720 - accuracy: 0.9795\n",
            "Epoch 158/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1262 - accuracy: 0.9374\n",
            "Epoch 159/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1104 - accuracy: 0.9613\n",
            "Epoch 160/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0827 - accuracy: 0.9882\n",
            "Epoch 161/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0880 - accuracy: 0.9632\n",
            "Epoch 162/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0889 - accuracy: 0.9675\n",
            "Epoch 163/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1064 - accuracy: 0.9554\n",
            "Epoch 164/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0787 - accuracy: 0.9781\n",
            "Epoch 165/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0951 - accuracy: 0.9705\n",
            "Epoch 166/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0785 - accuracy: 0.9777\n",
            "Epoch 167/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0891 - accuracy: 0.9809\n",
            "Epoch 168/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0880 - accuracy: 0.9710\n",
            "Epoch 169/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0781 - accuracy: 0.9664\n",
            "Epoch 170/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0725 - accuracy: 0.9868\n",
            "Epoch 171/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1021 - accuracy: 0.9545\n",
            "Epoch 172/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1017 - accuracy: 0.9771\n",
            "Epoch 173/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0642 - accuracy: 0.9882\n",
            "Epoch 174/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0670 - accuracy: 0.9758\n",
            "Epoch 175/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0749 - accuracy: 0.9869\n",
            "Epoch 176/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0881 - accuracy: 0.9720\n",
            "Epoch 177/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1086 - accuracy: 0.9494\n",
            "Epoch 178/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0963 - accuracy: 0.9748\n",
            "Epoch 179/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1344 - accuracy: 0.9577\n",
            "Epoch 180/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0845 - accuracy: 0.9859\n",
            "Epoch 181/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0915 - accuracy: 0.9538\n",
            "Epoch 182/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0770 - accuracy: 0.9837\n",
            "Epoch 183/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0981 - accuracy: 0.9628\n",
            "Epoch 184/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0682 - accuracy: 0.9885\n",
            "Epoch 185/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0862 - accuracy: 0.9750\n",
            "Epoch 186/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0619 - accuracy: 0.9788\n",
            "Epoch 187/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1082 - accuracy: 0.9387\n",
            "Epoch 188/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0881 - accuracy: 0.9776\n",
            "Epoch 189/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0845 - accuracy: 0.9686\n",
            "Epoch 190/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0755 - accuracy: 0.9773\n",
            "Epoch 191/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0675 - accuracy: 0.9813\n",
            "Epoch 192/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0743 - accuracy: 0.9803\n",
            "Epoch 193/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0876 - accuracy: 0.9737\n",
            "Epoch 194/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0902 - accuracy: 0.9631\n",
            "Epoch 195/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0784 - accuracy: 0.9717\n",
            "Epoch 196/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1208 - accuracy: 0.9406\n",
            "Epoch 197/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0927 - accuracy: 0.9623\n",
            "Epoch 198/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1047 - accuracy: 0.9733\n",
            "Epoch 199/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0827 - accuracy: 0.9795\n",
            "Epoch 200/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0714 - accuracy: 0.9904\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.0817 - accuracy: 1.0000\n",
            "Epoch 1/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 4.4545 - accuracy: 0.3472\n",
            "Epoch 2/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 3.2249 - accuracy: 0.3485\n",
            "Epoch 3/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 2.4719 - accuracy: 0.3948\n",
            "Epoch 4/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.7954 - accuracy: 0.6809\n",
            "Epoch 5/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.4804 - accuracy: 0.7029\n",
            "Epoch 6/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.5030 - accuracy: 0.6489\n",
            "Epoch 7/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.3187 - accuracy: 0.6394\n",
            "Epoch 8/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.1576 - accuracy: 0.6407\n",
            "Epoch 9/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.9407 - accuracy: 0.6862\n",
            "Epoch 10/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.8823 - accuracy: 0.6515\n",
            "Epoch 11/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.7602 - accuracy: 0.6961\n",
            "Epoch 12/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.7216 - accuracy: 0.6887\n",
            "Epoch 13/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.7544 - accuracy: 0.6242\n",
            "Epoch 14/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6803 - accuracy: 0.6561\n",
            "Epoch 15/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.6722 - accuracy: 0.6670\n",
            "Epoch 16/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6285 - accuracy: 0.7063\n",
            "Epoch 17/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6730 - accuracy: 0.6615\n",
            "Epoch 18/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6213 - accuracy: 0.7067\n",
            "Epoch 19/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5932 - accuracy: 0.7372\n",
            "Epoch 20/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5691 - accuracy: 0.7735\n",
            "Epoch 21/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5692 - accuracy: 0.8111\n",
            "Epoch 22/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5847 - accuracy: 0.7616\n",
            "Epoch 23/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5153 - accuracy: 0.8425\n",
            "Epoch 24/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5335 - accuracy: 0.8010\n",
            "Epoch 25/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5486 - accuracy: 0.8483\n",
            "Epoch 26/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5415 - accuracy: 0.8589\n",
            "Epoch 27/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.5015 - accuracy: 0.8667\n",
            "Epoch 28/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5059 - accuracy: 0.8261\n",
            "Epoch 29/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4998 - accuracy: 0.8596\n",
            "Epoch 30/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5104 - accuracy: 0.9057\n",
            "Epoch 31/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4497 - accuracy: 0.8775\n",
            "Epoch 32/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4887 - accuracy: 0.8488\n",
            "Epoch 33/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4827 - accuracy: 0.8755\n",
            "Epoch 34/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4283 - accuracy: 0.8995\n",
            "Epoch 35/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4666 - accuracy: 0.9005\n",
            "Epoch 36/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4496 - accuracy: 0.9284\n",
            "Epoch 37/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4562 - accuracy: 0.8916\n",
            "Epoch 38/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4370 - accuracy: 0.9235\n",
            "Epoch 39/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4271 - accuracy: 0.9418\n",
            "Epoch 40/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4289 - accuracy: 0.9381\n",
            "Epoch 41/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4255 - accuracy: 0.9231\n",
            "Epoch 42/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3821 - accuracy: 0.9141\n",
            "Epoch 43/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3675 - accuracy: 0.9529\n",
            "Epoch 44/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4258 - accuracy: 0.9561\n",
            "Epoch 45/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3771 - accuracy: 0.9612\n",
            "Epoch 46/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3890 - accuracy: 0.9692\n",
            "Epoch 47/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3578 - accuracy: 0.9415\n",
            "Epoch 48/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3884 - accuracy: 0.9163\n",
            "Epoch 49/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3692 - accuracy: 0.9521\n",
            "Epoch 50/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3553 - accuracy: 0.9317\n",
            "Epoch 51/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3726 - accuracy: 0.9543\n",
            "Epoch 52/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3402 - accuracy: 0.9708\n",
            "Epoch 53/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3844 - accuracy: 0.9506\n",
            "Epoch 54/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3537 - accuracy: 0.9497\n",
            "Epoch 55/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3538 - accuracy: 0.9594\n",
            "Epoch 56/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3862 - accuracy: 0.9404\n",
            "Epoch 57/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3537 - accuracy: 0.9521\n",
            "Epoch 58/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3588 - accuracy: 0.9512\n",
            "Epoch 59/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3341 - accuracy: 0.9590\n",
            "Epoch 60/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3418 - accuracy: 0.9601\n",
            "Epoch 61/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3330 - accuracy: 0.9470\n",
            "Epoch 62/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3612 - accuracy: 0.9267\n",
            "Epoch 63/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.9684\n",
            "Epoch 64/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3037 - accuracy: 0.9624\n",
            "Epoch 65/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2672 - accuracy: 0.9749\n",
            "Epoch 66/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2864 - accuracy: 0.9814\n",
            "Epoch 67/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3417 - accuracy: 0.9620\n",
            "Epoch 68/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3053 - accuracy: 0.9875\n",
            "Epoch 69/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3100 - accuracy: 0.9586\n",
            "Epoch 70/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3040 - accuracy: 0.9785\n",
            "Epoch 71/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3368 - accuracy: 0.9711\n",
            "Epoch 72/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3082 - accuracy: 0.9542\n",
            "Epoch 73/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2905 - accuracy: 0.9773\n",
            "Epoch 74/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3024 - accuracy: 0.9647\n",
            "Epoch 75/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2997 - accuracy: 0.9735\n",
            "Epoch 76/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2929 - accuracy: 0.9862\n",
            "Epoch 77/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3014 - accuracy: 0.9680\n",
            "Epoch 78/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2987 - accuracy: 0.9772\n",
            "Epoch 79/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2749 - accuracy: 0.9744\n",
            "Epoch 80/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2530 - accuracy: 0.9805\n",
            "Epoch 81/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3083 - accuracy: 0.9674\n",
            "Epoch 82/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2709 - accuracy: 0.9869\n",
            "Epoch 83/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2748 - accuracy: 0.9868\n",
            "Epoch 84/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2929 - accuracy: 0.9668\n",
            "Epoch 85/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2807 - accuracy: 0.9731\n",
            "Epoch 86/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2775 - accuracy: 0.9729\n",
            "Epoch 87/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2626 - accuracy: 0.9755\n",
            "Epoch 88/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2576 - accuracy: 0.9783\n",
            "Epoch 89/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2934 - accuracy: 0.9440\n",
            "Epoch 90/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2663 - accuracy: 0.9660\n",
            "Epoch 91/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2797 - accuracy: 0.9440\n",
            "Epoch 92/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2275 - accuracy: 0.9786\n",
            "Epoch 93/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2453 - accuracy: 0.9921\n",
            "Epoch 94/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2050 - accuracy: 0.9780\n",
            "Epoch 95/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2335 - accuracy: 0.9637\n",
            "Epoch 96/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2293 - accuracy: 0.9823\n",
            "Epoch 97/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2121 - accuracy: 0.9765\n",
            "Epoch 98/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2535 - accuracy: 0.9630\n",
            "Epoch 99/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2237 - accuracy: 0.9788\n",
            "Epoch 100/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2455 - accuracy: 0.9754\n",
            "Epoch 101/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2238 - accuracy: 0.9885\n",
            "Epoch 102/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2149 - accuracy: 0.9801\n",
            "Epoch 103/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2082 - accuracy: 0.9656\n",
            "Epoch 104/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2318 - accuracy: 0.9673\n",
            "Epoch 105/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2405 - accuracy: 0.9570\n",
            "Epoch 106/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2047 - accuracy: 0.9665\n",
            "Epoch 107/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2106 - accuracy: 0.9827\n",
            "Epoch 108/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2145 - accuracy: 0.9844\n",
            "Epoch 109/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1801 - accuracy: 0.9900\n",
            "Epoch 110/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2224 - accuracy: 0.9720\n",
            "Epoch 111/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1973 - accuracy: 0.9668\n",
            "Epoch 112/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1881 - accuracy: 0.9870\n",
            "Epoch 113/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1925 - accuracy: 0.9834\n",
            "Epoch 114/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2082 - accuracy: 0.9709\n",
            "Epoch 115/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1815 - accuracy: 0.9809\n",
            "Epoch 116/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1925 - accuracy: 0.9749\n",
            "Epoch 117/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1888 - accuracy: 0.9885\n",
            "Epoch 118/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1735 - accuracy: 0.9866\n",
            "Epoch 119/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1589 - accuracy: 0.9833\n",
            "Epoch 120/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2223 - accuracy: 0.9546\n",
            "Epoch 121/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1901 - accuracy: 0.9668\n",
            "Epoch 122/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2205 - accuracy: 0.9459\n",
            "Epoch 123/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1831 - accuracy: 0.9890\n",
            "Epoch 124/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1819 - accuracy: 0.9808\n",
            "Epoch 125/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1738 - accuracy: 0.9762\n",
            "Epoch 126/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2125 - accuracy: 0.9433\n",
            "Epoch 127/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1991 - accuracy: 0.9704\n",
            "Epoch 128/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1551 - accuracy: 0.9918\n",
            "Epoch 129/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1871 - accuracy: 0.9667\n",
            "Epoch 130/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1745 - accuracy: 0.9712\n",
            "Epoch 131/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1534 - accuracy: 0.9731\n",
            "Epoch 132/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1902 - accuracy: 0.9799\n",
            "Epoch 133/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1590 - accuracy: 0.9739\n",
            "Epoch 134/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1840 - accuracy: 0.9772\n",
            "Epoch 135/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1593 - accuracy: 0.9956\n",
            "Epoch 136/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1612 - accuracy: 0.9770\n",
            "Epoch 137/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1638 - accuracy: 0.9716\n",
            "Epoch 138/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1945 - accuracy: 0.9649\n",
            "Epoch 139/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1588 - accuracy: 0.9868\n",
            "Epoch 140/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1489 - accuracy: 0.9905\n",
            "Epoch 141/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1916 - accuracy: 0.9779\n",
            "Epoch 142/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1596 - accuracy: 0.9819\n",
            "Epoch 143/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1639 - accuracy: 0.9865\n",
            "Epoch 144/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1457 - accuracy: 0.9822\n",
            "Epoch 145/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1610 - accuracy: 0.9859\n",
            "Epoch 146/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1848 - accuracy: 0.9752\n",
            "Epoch 147/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1438 - accuracy: 0.9858\n",
            "Epoch 148/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1532 - accuracy: 0.9906\n",
            "Epoch 149/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1430 - accuracy: 0.9877\n",
            "Epoch 150/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1573 - accuracy: 0.9654\n",
            "Epoch 151/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1581 - accuracy: 0.9899\n",
            "Epoch 152/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1289 - accuracy: 0.9857\n",
            "Epoch 153/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1427 - accuracy: 0.9906\n",
            "Epoch 154/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1315 - accuracy: 0.9947\n",
            "Epoch 155/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1246 - accuracy: 0.9874\n",
            "Epoch 156/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1507 - accuracy: 0.9827\n",
            "Epoch 157/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1201 - accuracy: 0.9854\n",
            "Epoch 158/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1447 - accuracy: 0.9688\n",
            "Epoch 159/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1445 - accuracy: 0.9714\n",
            "Epoch 160/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1509 - accuracy: 0.9672\n",
            "Epoch 161/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1445 - accuracy: 0.9865\n",
            "Epoch 162/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1396 - accuracy: 0.9787\n",
            "Epoch 163/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1301 - accuracy: 0.9787\n",
            "Epoch 164/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1033 - accuracy: 0.9899\n",
            "Epoch 165/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1565 - accuracy: 0.9765\n",
            "Epoch 166/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1132 - accuracy: 0.9921\n",
            "Epoch 167/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1217 - accuracy: 0.9953\n",
            "Epoch 168/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1257 - accuracy: 0.9851\n",
            "Epoch 169/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1058 - accuracy: 0.9856\n",
            "Epoch 170/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1503 - accuracy: 0.9733\n",
            "Epoch 171/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1154 - accuracy: 0.9668\n",
            "Epoch 172/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1443 - accuracy: 0.9696\n",
            "Epoch 173/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1569 - accuracy: 0.9672\n",
            "Epoch 174/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1224 - accuracy: 0.9908\n",
            "Epoch 175/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1504 - accuracy: 0.9562\n",
            "Epoch 176/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1189 - accuracy: 0.9815\n",
            "Epoch 177/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1194 - accuracy: 0.9951\n",
            "Epoch 178/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1172 - accuracy: 0.9863\n",
            "Epoch 179/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1141 - accuracy: 0.9837\n",
            "Epoch 180/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1106 - accuracy: 0.9908\n",
            "Epoch 181/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0909 - accuracy: 0.9938\n",
            "Epoch 182/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1165 - accuracy: 0.9947\n",
            "Epoch 183/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0944 - accuracy: 0.9936\n",
            "Epoch 184/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1122 - accuracy: 0.9878\n",
            "Epoch 185/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1172 - accuracy: 0.9841\n",
            "Epoch 186/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1236 - accuracy: 0.9810\n",
            "Epoch 187/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1109 - accuracy: 0.9846\n",
            "Epoch 188/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1136 - accuracy: 0.9892\n",
            "Epoch 189/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0994 - accuracy: 0.9888\n",
            "Epoch 190/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0990 - accuracy: 0.9860\n",
            "Epoch 191/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1070 - accuracy: 0.9889\n",
            "Epoch 192/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1145 - accuracy: 0.9833\n",
            "Epoch 193/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0950 - accuracy: 0.9889\n",
            "Epoch 194/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0903 - accuracy: 0.9889\n",
            "Epoch 195/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1011 - accuracy: 0.9687\n",
            "Epoch 196/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1078 - accuracy: 0.9843\n",
            "Epoch 197/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1030 - accuracy: 0.9862\n",
            "Epoch 198/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1124 - accuracy: 0.9915\n",
            "Epoch 199/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1084 - accuracy: 0.9841\n",
            "Epoch 200/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1184 - accuracy: 0.9712\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fd0a1ae3bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.1280 - accuracy: 1.0000\n",
            "Epoch 1/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 2.1690 - accuracy: 0.3219\n",
            "Epoch 2/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.5576 - accuracy: 0.3920\n",
            "Epoch 3/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.4773 - accuracy: 0.3519\n",
            "Epoch 4/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.3124 - accuracy: 0.3184\n",
            "Epoch 5/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.2178 - accuracy: 0.3558\n",
            "Epoch 6/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.0587 - accuracy: 0.3521\n",
            "Epoch 7/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.0107 - accuracy: 0.3169\n",
            "Epoch 8/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.9820 - accuracy: 0.3394\n",
            "Epoch 9/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.9119 - accuracy: 0.3483\n",
            "Epoch 10/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.8592 - accuracy: 0.3606\n",
            "Epoch 11/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.8199 - accuracy: 0.4149\n",
            "Epoch 12/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.8021 - accuracy: 0.3716\n",
            "Epoch 13/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.8171 - accuracy: 0.3526\n",
            "Epoch 14/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.8142 - accuracy: 0.5225\n",
            "Epoch 15/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.7573 - accuracy: 0.6812\n",
            "Epoch 16/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.7415 - accuracy: 0.6765\n",
            "Epoch 17/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.7521 - accuracy: 0.7602\n",
            "Epoch 18/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.7419 - accuracy: 0.7607\n",
            "Epoch 19/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6894 - accuracy: 0.7242\n",
            "Epoch 20/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6914 - accuracy: 0.7774\n",
            "Epoch 21/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6871 - accuracy: 0.8178\n",
            "Epoch 22/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6659 - accuracy: 0.7699\n",
            "Epoch 23/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6938 - accuracy: 0.8682\n",
            "Epoch 24/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6778 - accuracy: 0.8261\n",
            "Epoch 25/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6651 - accuracy: 0.8432\n",
            "Epoch 26/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.6183 - accuracy: 0.8039\n",
            "Epoch 27/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6285 - accuracy: 0.8663\n",
            "Epoch 28/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6548 - accuracy: 0.8697\n",
            "Epoch 29/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6140 - accuracy: 0.8645\n",
            "Epoch 30/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6166 - accuracy: 0.8514\n",
            "Epoch 31/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6083 - accuracy: 0.8970\n",
            "Epoch 32/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.6051 - accuracy: 0.8932\n",
            "Epoch 33/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5774 - accuracy: 0.8740\n",
            "Epoch 34/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5900 - accuracy: 0.9088\n",
            "Epoch 35/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5820 - accuracy: 0.9444\n",
            "Epoch 36/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5680 - accuracy: 0.9134\n",
            "Epoch 37/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5653 - accuracy: 0.9732\n",
            "Epoch 38/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.5733 - accuracy: 0.9158\n",
            "Epoch 39/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5578 - accuracy: 0.9116\n",
            "Epoch 40/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5566 - accuracy: 0.9241\n",
            "Epoch 41/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5325 - accuracy: 0.8900\n",
            "Epoch 42/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5322 - accuracy: 0.9458\n",
            "Epoch 43/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5472 - accuracy: 0.9503\n",
            "Epoch 44/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5279 - accuracy: 0.9171\n",
            "Epoch 45/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5270 - accuracy: 0.8970\n",
            "Epoch 46/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5150 - accuracy: 0.9370\n",
            "Epoch 47/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5073 - accuracy: 0.9614\n",
            "Epoch 48/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.5115 - accuracy: 0.9745\n",
            "Epoch 49/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4981 - accuracy: 0.9616\n",
            "Epoch 50/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5025 - accuracy: 0.9718\n",
            "Epoch 51/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4937 - accuracy: 0.9698\n",
            "Epoch 52/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4948 - accuracy: 0.9131\n",
            "Epoch 53/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4809 - accuracy: 0.9760\n",
            "Epoch 54/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4646 - accuracy: 0.9744\n",
            "Epoch 55/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4906 - accuracy: 0.9403\n",
            "Epoch 56/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4687 - accuracy: 0.9495\n",
            "Epoch 57/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4665 - accuracy: 0.9357\n",
            "Epoch 58/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4718 - accuracy: 0.9512\n",
            "Epoch 59/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4511 - accuracy: 0.9332\n",
            "Epoch 60/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4461 - accuracy: 0.9677\n",
            "Epoch 61/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4394 - accuracy: 0.9733\n",
            "Epoch 62/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4347 - accuracy: 0.9530\n",
            "Epoch 63/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4342 - accuracy: 0.9677\n",
            "Epoch 64/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4249 - accuracy: 0.9709\n",
            "Epoch 65/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4264 - accuracy: 0.9546\n",
            "Epoch 66/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4074 - accuracy: 0.9561\n",
            "Epoch 67/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4110 - accuracy: 0.9548\n",
            "Epoch 68/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4095 - accuracy: 0.9588\n",
            "Epoch 69/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4102 - accuracy: 0.9537\n",
            "Epoch 70/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4102 - accuracy: 0.9569\n",
            "Epoch 71/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3722 - accuracy: 0.9722\n",
            "Epoch 72/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4011 - accuracy: 0.9378\n",
            "Epoch 73/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3535 - accuracy: 0.9858\n",
            "Epoch 74/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3885 - accuracy: 0.9746\n",
            "Epoch 75/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3633 - accuracy: 0.9592\n",
            "Epoch 76/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3536 - accuracy: 0.9741\n",
            "Epoch 77/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3352 - accuracy: 0.9863\n",
            "Epoch 78/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3692 - accuracy: 0.9626\n",
            "Epoch 79/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3328 - accuracy: 0.9790\n",
            "Epoch 80/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3454 - accuracy: 0.9740\n",
            "Epoch 81/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3295 - accuracy: 0.9694\n",
            "Epoch 82/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3404 - accuracy: 0.9547\n",
            "Epoch 83/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3158 - accuracy: 0.9795\n",
            "Epoch 84/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3407 - accuracy: 0.9574\n",
            "Epoch 85/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3196 - accuracy: 0.9733\n",
            "Epoch 86/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2669 - accuracy: 0.9922\n",
            "Epoch 87/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3001 - accuracy: 0.9589\n",
            "Epoch 88/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3010 - accuracy: 0.9417\n",
            "Epoch 89/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2843 - accuracy: 0.9711\n",
            "Epoch 90/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2689 - accuracy: 0.9876\n",
            "Epoch 91/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2876 - accuracy: 0.9764\n",
            "Epoch 92/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2979 - accuracy: 0.9797\n",
            "Epoch 93/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2730 - accuracy: 0.9919\n",
            "Epoch 94/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2814 - accuracy: 0.9778\n",
            "Epoch 95/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2816 - accuracy: 0.9671\n",
            "Epoch 96/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2551 - accuracy: 0.9660\n",
            "Epoch 97/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2608 - accuracy: 0.9720\n",
            "Epoch 98/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2365 - accuracy: 0.9764\n",
            "Epoch 99/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2409 - accuracy: 0.9830\n",
            "Epoch 100/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2558 - accuracy: 0.9651\n",
            "Epoch 101/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2298 - accuracy: 0.9711\n",
            "Epoch 102/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2504 - accuracy: 0.9621\n",
            "Epoch 103/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2320 - accuracy: 0.9783\n",
            "Epoch 104/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2337 - accuracy: 0.9901\n",
            "Epoch 105/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2463 - accuracy: 0.9609\n",
            "Epoch 106/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2672 - accuracy: 0.9332\n",
            "Epoch 107/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2480 - accuracy: 0.9803\n",
            "Epoch 108/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2236 - accuracy: 0.9729\n",
            "Epoch 109/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2346 - accuracy: 0.9893\n",
            "Epoch 110/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2114 - accuracy: 0.9928\n",
            "Epoch 111/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2168 - accuracy: 0.9826\n",
            "Epoch 112/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2106 - accuracy: 0.9826\n",
            "Epoch 113/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2017 - accuracy: 0.9890\n",
            "Epoch 114/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2267 - accuracy: 0.9546\n",
            "Epoch 115/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2065 - accuracy: 0.9415\n",
            "Epoch 116/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1922 - accuracy: 0.9695\n",
            "Epoch 117/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2118 - accuracy: 0.9705\n",
            "Epoch 118/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1942 - accuracy: 0.9846\n",
            "Epoch 119/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2107 - accuracy: 0.9603\n",
            "Epoch 120/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2022 - accuracy: 0.9530\n",
            "Epoch 121/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1838 - accuracy: 0.9920\n",
            "Epoch 122/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2073 - accuracy: 0.9758\n",
            "Epoch 123/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1973 - accuracy: 0.9660\n",
            "Epoch 124/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1920 - accuracy: 0.9748\n",
            "Epoch 125/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1862 - accuracy: 0.9936\n",
            "Epoch 126/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1795 - accuracy: 0.9744\n",
            "Epoch 127/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1768 - accuracy: 0.9877\n",
            "Epoch 128/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2111 - accuracy: 0.9626\n",
            "Epoch 129/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1800 - accuracy: 0.9768\n",
            "Epoch 130/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1923 - accuracy: 0.9743\n",
            "Epoch 131/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1904 - accuracy: 0.9553\n",
            "Epoch 132/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1616 - accuracy: 0.9706\n",
            "Epoch 133/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1778 - accuracy: 0.9762\n",
            "Epoch 134/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1582 - accuracy: 0.9717\n",
            "Epoch 135/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1462 - accuracy: 0.9906\n",
            "Epoch 136/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1620 - accuracy: 0.9633\n",
            "Epoch 137/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1454 - accuracy: 0.9841\n",
            "Epoch 138/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1647 - accuracy: 0.9954\n",
            "Epoch 139/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1835 - accuracy: 0.9408\n",
            "Epoch 140/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1575 - accuracy: 0.9802\n",
            "Epoch 141/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1501 - accuracy: 0.9708\n",
            "Epoch 142/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1381 - accuracy: 0.9815\n",
            "Epoch 143/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1586 - accuracy: 0.9819\n",
            "Epoch 144/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1431 - accuracy: 0.9895\n",
            "Epoch 145/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1480 - accuracy: 0.9729\n",
            "Epoch 146/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1531 - accuracy: 0.9952\n",
            "Epoch 147/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1286 - accuracy: 0.9840\n",
            "Epoch 148/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1497 - accuracy: 0.9877\n",
            "Epoch 149/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1456 - accuracy: 0.9807\n",
            "Epoch 150/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1483 - accuracy: 0.9664\n",
            "Epoch 151/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1388 - accuracy: 0.9744\n",
            "Epoch 152/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1449 - accuracy: 0.9687\n",
            "Epoch 153/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1332 - accuracy: 0.9850\n",
            "Epoch 154/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1422 - accuracy: 0.9889\n",
            "Epoch 155/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1378 - accuracy: 0.9837\n",
            "Epoch 156/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1455 - accuracy: 0.9866\n",
            "Epoch 157/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1467 - accuracy: 0.9689\n",
            "Epoch 158/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1364 - accuracy: 0.9841\n",
            "Epoch 159/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1328 - accuracy: 0.9751\n",
            "Epoch 160/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1334 - accuracy: 0.9829\n",
            "Epoch 161/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1287 - accuracy: 0.9869\n",
            "Epoch 162/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1284 - accuracy: 0.9702\n",
            "Epoch 163/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1541 - accuracy: 0.9637\n",
            "Epoch 164/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1356 - accuracy: 0.9676\n",
            "Epoch 165/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1178 - accuracy: 0.9805\n",
            "Epoch 166/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1085 - accuracy: 0.9928\n",
            "Epoch 167/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1107 - accuracy: 0.9806\n",
            "Epoch 168/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1198 - accuracy: 0.9920\n",
            "Epoch 169/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1184 - accuracy: 0.9878\n",
            "Epoch 170/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1356 - accuracy: 0.9855\n",
            "Epoch 171/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1211 - accuracy: 0.9854\n",
            "Epoch 172/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0977 - accuracy: 0.9877\n",
            "Epoch 173/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1118 - accuracy: 0.9760\n",
            "Epoch 174/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1164 - accuracy: 0.9839\n",
            "Epoch 175/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1212 - accuracy: 0.9706\n",
            "Epoch 176/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1186 - accuracy: 0.9632\n",
            "Epoch 177/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1200 - accuracy: 0.9701\n",
            "Epoch 178/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1180 - accuracy: 0.9850\n",
            "Epoch 179/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1218 - accuracy: 0.9751\n",
            "Epoch 180/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1264 - accuracy: 0.9530\n",
            "Epoch 181/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1125 - accuracy: 0.9845\n",
            "Epoch 182/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1095 - accuracy: 0.9861\n",
            "Epoch 183/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1040 - accuracy: 0.9816\n",
            "Epoch 184/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0868 - accuracy: 0.9938\n",
            "Epoch 185/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1289 - accuracy: 0.9650\n",
            "Epoch 186/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0982 - accuracy: 0.9877\n",
            "Epoch 187/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0929 - accuracy: 0.9922\n",
            "Epoch 188/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0835 - accuracy: 0.9901\n",
            "Epoch 189/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0992 - accuracy: 0.9897\n",
            "Epoch 190/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0962 - accuracy: 0.9737\n",
            "Epoch 191/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1006 - accuracy: 0.9768\n",
            "Epoch 192/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1022 - accuracy: 0.9858\n",
            "Epoch 193/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0965 - accuracy: 0.9878\n",
            "Epoch 194/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1024 - accuracy: 0.9837\n",
            "Epoch 195/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1041 - accuracy: 0.9927\n",
            "Epoch 196/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0862 - accuracy: 0.9863\n",
            "Epoch 197/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0989 - accuracy: 0.9766\n",
            "Epoch 198/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1101 - accuracy: 0.9858\n",
            "Epoch 199/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1053 - accuracy: 0.9952\n",
            "Epoch 200/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0970 - accuracy: 0.9903\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fd0a09959d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.1932 - accuracy: 0.9333\n",
            "Epoch 1/200\n",
            "27/27 [==============================] - 1s 1ms/step - loss: 1.4097 - accuracy: 0.6750\n",
            "Epoch 2/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.1742 - accuracy: 0.6991\n",
            "Epoch 3/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.2169 - accuracy: 0.6244\n",
            "Epoch 4/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.0559 - accuracy: 0.6443\n",
            "Epoch 5/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.0972 - accuracy: 0.5779\n",
            "Epoch 6/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.9230 - accuracy: 0.6503\n",
            "Epoch 7/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.9107 - accuracy: 0.5246\n",
            "Epoch 8/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.8906 - accuracy: 0.3391\n",
            "Epoch 9/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.9264 - accuracy: 0.2792\n",
            "Epoch 10/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.8837 - accuracy: 0.2641\n",
            "Epoch 11/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.8579 - accuracy: 0.3313\n",
            "Epoch 12/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.8527 - accuracy: 0.3306\n",
            "Epoch 13/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.8173 - accuracy: 0.3641\n",
            "Epoch 14/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.8749 - accuracy: 0.3436\n",
            "Epoch 15/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.7922 - accuracy: 0.4285\n",
            "Epoch 16/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.7979 - accuracy: 0.5475\n",
            "Epoch 17/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.7873 - accuracy: 0.6578\n",
            "Epoch 18/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.7475 - accuracy: 0.8310\n",
            "Epoch 19/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.7603 - accuracy: 0.8162\n",
            "Epoch 20/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.7371 - accuracy: 0.8246\n",
            "Epoch 21/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.7388 - accuracy: 0.8077\n",
            "Epoch 22/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6829 - accuracy: 0.8294\n",
            "Epoch 23/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.6884 - accuracy: 0.8405\n",
            "Epoch 24/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6912 - accuracy: 0.8459\n",
            "Epoch 25/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6998 - accuracy: 0.8894\n",
            "Epoch 26/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6590 - accuracy: 0.8859\n",
            "Epoch 27/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6446 - accuracy: 0.8749\n",
            "Epoch 28/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6311 - accuracy: 0.9034\n",
            "Epoch 29/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5969 - accuracy: 0.8849\n",
            "Epoch 30/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6179 - accuracy: 0.9225\n",
            "Epoch 31/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6254 - accuracy: 0.9039\n",
            "Epoch 32/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5938 - accuracy: 0.8956\n",
            "Epoch 33/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5982 - accuracy: 0.8675\n",
            "Epoch 34/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5708 - accuracy: 0.9088\n",
            "Epoch 35/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5512 - accuracy: 0.8790\n",
            "Epoch 36/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5304 - accuracy: 0.9060\n",
            "Epoch 37/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5556 - accuracy: 0.9294\n",
            "Epoch 38/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4866 - accuracy: 0.9438\n",
            "Epoch 39/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5270 - accuracy: 0.8780\n",
            "Epoch 40/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4958 - accuracy: 0.9611\n",
            "Epoch 41/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4925 - accuracy: 0.9225\n",
            "Epoch 42/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4862 - accuracy: 0.9448\n",
            "Epoch 43/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5021 - accuracy: 0.9487\n",
            "Epoch 44/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4458 - accuracy: 0.9815\n",
            "Epoch 45/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4536 - accuracy: 0.9257\n",
            "Epoch 46/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4624 - accuracy: 0.9494\n",
            "Epoch 47/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4508 - accuracy: 0.9588\n",
            "Epoch 48/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4292 - accuracy: 0.9656\n",
            "Epoch 49/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4218 - accuracy: 0.9646\n",
            "Epoch 50/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4067 - accuracy: 0.9837\n",
            "Epoch 51/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4032 - accuracy: 0.9675\n",
            "Epoch 52/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4105 - accuracy: 0.9723\n",
            "Epoch 53/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3895 - accuracy: 0.9472\n",
            "Epoch 54/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4059 - accuracy: 0.9715\n",
            "Epoch 55/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4183 - accuracy: 0.9741\n",
            "Epoch 56/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3970 - accuracy: 0.9566\n",
            "Epoch 57/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3884 - accuracy: 0.9641\n",
            "Epoch 58/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3308 - accuracy: 0.9805\n",
            "Epoch 59/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3509 - accuracy: 0.9605\n",
            "Epoch 60/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3464 - accuracy: 0.9578\n",
            "Epoch 61/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3191 - accuracy: 0.9824\n",
            "Epoch 62/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3152 - accuracy: 0.9846\n",
            "Epoch 63/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3263 - accuracy: 0.9817\n",
            "Epoch 64/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3377 - accuracy: 0.9734\n",
            "Epoch 65/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3380 - accuracy: 0.9597\n",
            "Epoch 66/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3327 - accuracy: 0.9710\n",
            "Epoch 67/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3445 - accuracy: 0.9456\n",
            "Epoch 68/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3357 - accuracy: 0.9794\n",
            "Epoch 69/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3167 - accuracy: 0.9830\n",
            "Epoch 70/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2952 - accuracy: 0.9739\n",
            "Epoch 71/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3268 - accuracy: 0.9571\n",
            "Epoch 72/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3014 - accuracy: 0.9709\n",
            "Epoch 73/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2932 - accuracy: 0.9813\n",
            "Epoch 74/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3132 - accuracy: 0.9646\n",
            "Epoch 75/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2805 - accuracy: 0.9801\n",
            "Epoch 76/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2877 - accuracy: 0.9825\n",
            "Epoch 77/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2843 - accuracy: 0.9788\n",
            "Epoch 78/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2808 - accuracy: 0.9791\n",
            "Epoch 79/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2844 - accuracy: 0.9641\n",
            "Epoch 80/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2807 - accuracy: 0.9689\n",
            "Epoch 81/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2777 - accuracy: 0.9816\n",
            "Epoch 82/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2398 - accuracy: 0.9865\n",
            "Epoch 83/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2499 - accuracy: 0.9900\n",
            "Epoch 84/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2562 - accuracy: 0.9920\n",
            "Epoch 85/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2480 - accuracy: 0.9914\n",
            "Epoch 86/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2355 - accuracy: 0.9905\n",
            "Epoch 87/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2836 - accuracy: 0.9588\n",
            "Epoch 88/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2476 - accuracy: 0.9822\n",
            "Epoch 89/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2305 - accuracy: 0.9815\n",
            "Epoch 90/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2692 - accuracy: 0.9817\n",
            "Epoch 91/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2316 - accuracy: 0.9857\n",
            "Epoch 92/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2479 - accuracy: 0.9614\n",
            "Epoch 93/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2312 - accuracy: 0.9883\n",
            "Epoch 94/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2377 - accuracy: 0.9828\n",
            "Epoch 95/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2441 - accuracy: 0.9745\n",
            "Epoch 96/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2387 - accuracy: 0.9649\n",
            "Epoch 97/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2371 - accuracy: 0.9761\n",
            "Epoch 98/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1998 - accuracy: 0.9659\n",
            "Epoch 99/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2084 - accuracy: 0.9825\n",
            "Epoch 100/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1995 - accuracy: 0.9819\n",
            "Epoch 101/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2199 - accuracy: 0.9864\n",
            "Epoch 102/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2027 - accuracy: 0.9819\n",
            "Epoch 103/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2036 - accuracy: 0.9957\n",
            "Epoch 104/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2090 - accuracy: 0.9804\n",
            "Epoch 105/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1979 - accuracy: 0.9918\n",
            "Epoch 106/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2177 - accuracy: 0.9511\n",
            "Epoch 107/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1969 - accuracy: 0.9842\n",
            "Epoch 108/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1623 - accuracy: 0.9893\n",
            "Epoch 109/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1977 - accuracy: 0.9756\n",
            "Epoch 110/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1973 - accuracy: 0.9854\n",
            "Epoch 111/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1736 - accuracy: 0.9916\n",
            "Epoch 112/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2023 - accuracy: 0.9666\n",
            "Epoch 113/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1944 - accuracy: 0.9717\n",
            "Epoch 114/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2022 - accuracy: 0.9831\n",
            "Epoch 115/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1737 - accuracy: 0.9792\n",
            "Epoch 116/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1783 - accuracy: 0.9840\n",
            "Epoch 117/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1998 - accuracy: 0.9531\n",
            "Epoch 118/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1959 - accuracy: 0.9784\n",
            "Epoch 119/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1523 - accuracy: 0.9947\n",
            "Epoch 120/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1731 - accuracy: 0.9966\n",
            "Epoch 121/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1645 - accuracy: 0.9920\n",
            "Epoch 122/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1589 - accuracy: 0.9873\n",
            "Epoch 123/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1838 - accuracy: 0.9924\n",
            "Epoch 124/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1801 - accuracy: 0.9586\n",
            "Epoch 125/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1624 - accuracy: 0.9612\n",
            "Epoch 126/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1640 - accuracy: 0.9907\n",
            "Epoch 127/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1558 - accuracy: 0.9838\n",
            "Epoch 128/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1797 - accuracy: 0.9738\n",
            "Epoch 129/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1413 - accuracy: 0.9943\n",
            "Epoch 130/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1558 - accuracy: 0.9899\n",
            "Epoch 131/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2002 - accuracy: 0.9502\n",
            "Epoch 132/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1684 - accuracy: 0.9831\n",
            "Epoch 133/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1445 - accuracy: 0.9853\n",
            "Epoch 134/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1861 - accuracy: 0.9870\n",
            "Epoch 135/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1440 - accuracy: 0.9879\n",
            "Epoch 136/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1758 - accuracy: 0.9682\n",
            "Epoch 137/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1380 - accuracy: 0.9787\n",
            "Epoch 138/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1682 - accuracy: 0.9569\n",
            "Epoch 139/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1606 - accuracy: 0.9653\n",
            "Epoch 140/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1684 - accuracy: 0.9440\n",
            "Epoch 141/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1420 - accuracy: 0.9853\n",
            "Epoch 142/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1552 - accuracy: 0.9760\n",
            "Epoch 143/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1729 - accuracy: 0.9608\n",
            "Epoch 144/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1597 - accuracy: 0.9719\n",
            "Epoch 145/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1636 - accuracy: 0.9799\n",
            "Epoch 146/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1346 - accuracy: 0.9857\n",
            "Epoch 147/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1293 - accuracy: 0.9750\n",
            "Epoch 148/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1269 - accuracy: 0.9859\n",
            "Epoch 149/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1463 - accuracy: 0.9624\n",
            "Epoch 150/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1542 - accuracy: 0.9438\n",
            "Epoch 151/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1335 - accuracy: 0.9916\n",
            "Epoch 152/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1105 - accuracy: 0.9937\n",
            "Epoch 153/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1432 - accuracy: 0.9735\n",
            "Epoch 154/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1424 - accuracy: 0.9819\n",
            "Epoch 155/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1084 - accuracy: 0.9865\n",
            "Epoch 156/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1081 - accuracy: 0.9917\n",
            "Epoch 157/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1380 - accuracy: 0.9839\n",
            "Epoch 158/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1243 - accuracy: 0.9728\n",
            "Epoch 159/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1016 - accuracy: 0.9887\n",
            "Epoch 160/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.9857\n",
            "Epoch 161/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1269 - accuracy: 0.9855\n",
            "Epoch 162/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1236 - accuracy: 0.9885\n",
            "Epoch 163/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1289 - accuracy: 0.9789\n",
            "Epoch 164/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1144 - accuracy: 0.9751\n",
            "Epoch 165/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1436 - accuracy: 0.9573\n",
            "Epoch 166/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1058 - accuracy: 0.9877\n",
            "Epoch 167/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1547 - accuracy: 0.9573\n",
            "Epoch 168/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1373 - accuracy: 0.9534\n",
            "Epoch 169/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1256 - accuracy: 0.9789\n",
            "Epoch 170/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1150 - accuracy: 0.9796\n",
            "Epoch 171/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1077 - accuracy: 0.9929\n",
            "Epoch 172/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1252 - accuracy: 0.9664\n",
            "Epoch 173/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1142 - accuracy: 0.9891\n",
            "Epoch 174/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1220 - accuracy: 0.9614\n",
            "Epoch 175/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1201 - accuracy: 0.9840\n",
            "Epoch 176/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1035 - accuracy: 0.9961\n",
            "Epoch 177/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1230 - accuracy: 0.9585\n",
            "Epoch 178/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1042 - accuracy: 0.9787\n",
            "Epoch 179/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1003 - accuracy: 0.9935\n",
            "Epoch 180/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1071 - accuracy: 0.9675\n",
            "Epoch 181/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0946 - accuracy: 0.9846\n",
            "Epoch 182/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1156 - accuracy: 0.9777\n",
            "Epoch 183/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1179 - accuracy: 0.9735\n",
            "Epoch 184/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1036 - accuracy: 0.9682\n",
            "Epoch 185/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0995 - accuracy: 0.9947\n",
            "Epoch 186/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1031 - accuracy: 0.9822\n",
            "Epoch 187/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1283 - accuracy: 0.9741\n",
            "Epoch 188/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0811 - accuracy: 0.9931\n",
            "Epoch 189/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0844 - accuracy: 0.9874\n",
            "Epoch 190/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0936 - accuracy: 0.9972\n",
            "Epoch 191/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1043 - accuracy: 0.9782\n",
            "Epoch 192/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0980 - accuracy: 0.9916\n",
            "Epoch 193/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1025 - accuracy: 0.9778\n",
            "Epoch 194/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0876 - accuracy: 0.9874\n",
            "Epoch 195/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0999 - accuracy: 0.9880\n",
            "Epoch 196/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0967 - accuracy: 0.9813\n",
            "Epoch 197/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1076 - accuracy: 0.9657\n",
            "Epoch 198/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1016 - accuracy: 0.9711\n",
            "Epoch 199/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1034 - accuracy: 0.9843\n",
            "Epoch 200/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1025 - accuracy: 0.9767\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fd0a1b84d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "3/3 [==============================] - 0s 2ms/step - loss: 0.1064 - accuracy: 1.0000\n",
            "Epoch 1/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.0872 - accuracy: 0.4855\n",
            "Epoch 2/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.8792 - accuracy: 0.6882\n",
            "Epoch 3/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.8903 - accuracy: 0.5967\n",
            "Epoch 4/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.8265 - accuracy: 0.6605\n",
            "Epoch 5/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.7565 - accuracy: 0.6965\n",
            "Epoch 6/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.7502 - accuracy: 0.7039\n",
            "Epoch 7/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6916 - accuracy: 0.7464\n",
            "Epoch 8/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6722 - accuracy: 0.8196\n",
            "Epoch 9/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.6436 - accuracy: 0.8263\n",
            "Epoch 10/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6326 - accuracy: 0.7491\n",
            "Epoch 11/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5952 - accuracy: 0.7800\n",
            "Epoch 12/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.6095 - accuracy: 0.6517\n",
            "Epoch 13/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.5698 - accuracy: 0.6909\n",
            "Epoch 14/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5485 - accuracy: 0.7606\n",
            "Epoch 15/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5497 - accuracy: 0.8001\n",
            "Epoch 16/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5666 - accuracy: 0.7414\n",
            "Epoch 17/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5182 - accuracy: 0.8299\n",
            "Epoch 18/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5072 - accuracy: 0.8109\n",
            "Epoch 19/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4897 - accuracy: 0.8790\n",
            "Epoch 20/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4775 - accuracy: 0.8381\n",
            "Epoch 21/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4888 - accuracy: 0.8249\n",
            "Epoch 22/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5000 - accuracy: 0.8661\n",
            "Epoch 23/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4476 - accuracy: 0.8834\n",
            "Epoch 24/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4663 - accuracy: 0.9089\n",
            "Epoch 25/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4008 - accuracy: 0.9343\n",
            "Epoch 26/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3947 - accuracy: 0.8842\n",
            "Epoch 27/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4214 - accuracy: 0.9187\n",
            "Epoch 28/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4555 - accuracy: 0.9170\n",
            "Epoch 29/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3966 - accuracy: 0.9261\n",
            "Epoch 30/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4146 - accuracy: 0.9192\n",
            "Epoch 31/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4142 - accuracy: 0.9281\n",
            "Epoch 32/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4031 - accuracy: 0.9489\n",
            "Epoch 33/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4231 - accuracy: 0.9074\n",
            "Epoch 34/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4112 - accuracy: 0.9444\n",
            "Epoch 35/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3933 - accuracy: 0.9226\n",
            "Epoch 36/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3826 - accuracy: 0.9553\n",
            "Epoch 37/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3570 - accuracy: 0.9500\n",
            "Epoch 38/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3509 - accuracy: 0.9809\n",
            "Epoch 39/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3630 - accuracy: 0.9526\n",
            "Epoch 40/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3586 - accuracy: 0.9439\n",
            "Epoch 41/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3347 - accuracy: 0.9404\n",
            "Epoch 42/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3544 - accuracy: 0.9706\n",
            "Epoch 43/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3222 - accuracy: 0.9647\n",
            "Epoch 44/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3614 - accuracy: 0.9459\n",
            "Epoch 45/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3452 - accuracy: 0.9633\n",
            "Epoch 46/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3249 - accuracy: 0.9710\n",
            "Epoch 47/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3255 - accuracy: 0.9582\n",
            "Epoch 48/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3246 - accuracy: 0.9612\n",
            "Epoch 49/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3189 - accuracy: 0.9712\n",
            "Epoch 50/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3209 - accuracy: 0.9638\n",
            "Epoch 51/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2978 - accuracy: 0.9725\n",
            "Epoch 52/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3211 - accuracy: 0.9505\n",
            "Epoch 53/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3059 - accuracy: 0.9362\n",
            "Epoch 54/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2903 - accuracy: 0.9626\n",
            "Epoch 55/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3014 - accuracy: 0.9270\n",
            "Epoch 56/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2892 - accuracy: 0.9416\n",
            "Epoch 57/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3122 - accuracy: 0.9461\n",
            "Epoch 58/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2701 - accuracy: 0.9880\n",
            "Epoch 59/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2780 - accuracy: 0.9445\n",
            "Epoch 60/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2943 - accuracy: 0.9449\n",
            "Epoch 61/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2473 - accuracy: 0.9574\n",
            "Epoch 62/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2583 - accuracy: 0.9663\n",
            "Epoch 63/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2866 - accuracy: 0.9588\n",
            "Epoch 64/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2651 - accuracy: 0.9816\n",
            "Epoch 65/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2760 - accuracy: 0.9308\n",
            "Epoch 66/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2474 - accuracy: 0.9783\n",
            "Epoch 67/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2300 - accuracy: 0.9841\n",
            "Epoch 68/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2067 - accuracy: 0.9773\n",
            "Epoch 69/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2618 - accuracy: 0.9474\n",
            "Epoch 70/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2227 - accuracy: 0.9853\n",
            "Epoch 71/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2471 - accuracy: 0.9581\n",
            "Epoch 72/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2327 - accuracy: 0.9419\n",
            "Epoch 73/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2358 - accuracy: 0.9730\n",
            "Epoch 74/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2395 - accuracy: 0.9667\n",
            "Epoch 75/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2206 - accuracy: 0.9637\n",
            "Epoch 76/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2157 - accuracy: 0.9820\n",
            "Epoch 77/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2434 - accuracy: 0.9696\n",
            "Epoch 78/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2140 - accuracy: 0.9484\n",
            "Epoch 79/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2382 - accuracy: 0.9670\n",
            "Epoch 80/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2093 - accuracy: 0.9713\n",
            "Epoch 81/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2053 - accuracy: 0.9760\n",
            "Epoch 82/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1854 - accuracy: 0.9793\n",
            "Epoch 83/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2124 - accuracy: 0.9677\n",
            "Epoch 84/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2126 - accuracy: 0.9702\n",
            "Epoch 85/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1855 - accuracy: 0.9814\n",
            "Epoch 86/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1836 - accuracy: 0.9674\n",
            "Epoch 87/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1861 - accuracy: 0.9835\n",
            "Epoch 88/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2155 - accuracy: 0.9498\n",
            "Epoch 89/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1818 - accuracy: 0.9841\n",
            "Epoch 90/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1702 - accuracy: 0.9515\n",
            "Epoch 91/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2232 - accuracy: 0.9505\n",
            "Epoch 92/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2028 - accuracy: 0.9677\n",
            "Epoch 93/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2078 - accuracy: 0.9413\n",
            "Epoch 94/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1808 - accuracy: 0.9804\n",
            "Epoch 95/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1884 - accuracy: 0.9253\n",
            "Epoch 96/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1653 - accuracy: 0.9753\n",
            "Epoch 97/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1765 - accuracy: 0.9594\n",
            "Epoch 98/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1714 - accuracy: 0.9717\n",
            "Epoch 99/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1623 - accuracy: 0.9882\n",
            "Epoch 100/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1619 - accuracy: 0.9843\n",
            "Epoch 101/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1647 - accuracy: 0.9623\n",
            "Epoch 102/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2060 - accuracy: 0.9360\n",
            "Epoch 103/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1631 - accuracy: 0.9624\n",
            "Epoch 104/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1519 - accuracy: 0.9615\n",
            "Epoch 105/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1967 - accuracy: 0.9364\n",
            "Epoch 106/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1991 - accuracy: 0.9576\n",
            "Epoch 107/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1265 - accuracy: 0.9713\n",
            "Epoch 108/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1726 - accuracy: 0.9540\n",
            "Epoch 109/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1668 - accuracy: 0.9572\n",
            "Epoch 110/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1673 - accuracy: 0.9638\n",
            "Epoch 111/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1371 - accuracy: 0.9812\n",
            "Epoch 112/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1543 - accuracy: 0.9647\n",
            "Epoch 113/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1714 - accuracy: 0.9559\n",
            "Epoch 114/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1515 - accuracy: 0.9717\n",
            "Epoch 115/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1730 - accuracy: 0.9454\n",
            "Epoch 116/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1384 - accuracy: 0.9712\n",
            "Epoch 117/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1369 - accuracy: 0.9751\n",
            "Epoch 118/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1186 - accuracy: 0.9933\n",
            "Epoch 119/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1308 - accuracy: 0.9534\n",
            "Epoch 120/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1707 - accuracy: 0.9588\n",
            "Epoch 121/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1442 - accuracy: 0.9660\n",
            "Epoch 122/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1206 - accuracy: 0.9767\n",
            "Epoch 123/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1205 - accuracy: 0.9793\n",
            "Epoch 124/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1253 - accuracy: 0.9757\n",
            "Epoch 125/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1240 - accuracy: 0.9822\n",
            "Epoch 126/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1257 - accuracy: 0.9519\n",
            "Epoch 127/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1333 - accuracy: 0.9760\n",
            "Epoch 128/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1357 - accuracy: 0.9721\n",
            "Epoch 129/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1298 - accuracy: 0.9898\n",
            "Epoch 130/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1474 - accuracy: 0.9644\n",
            "Epoch 131/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1302 - accuracy: 0.9608\n",
            "Epoch 132/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1631 - accuracy: 0.9328\n",
            "Epoch 133/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1427 - accuracy: 0.9560\n",
            "Epoch 134/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1091 - accuracy: 0.9835\n",
            "Epoch 135/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1242 - accuracy: 0.9662\n",
            "Epoch 136/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1307 - accuracy: 0.9670\n",
            "Epoch 137/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0969 - accuracy: 0.9850\n",
            "Epoch 138/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1212 - accuracy: 0.9780\n",
            "Epoch 139/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1180 - accuracy: 0.9682\n",
            "Epoch 140/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0891 - accuracy: 0.9897\n",
            "Epoch 141/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1467 - accuracy: 0.9415\n",
            "Epoch 142/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1117 - accuracy: 0.9851\n",
            "Epoch 143/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1036 - accuracy: 0.9735\n",
            "Epoch 144/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1066 - accuracy: 0.9730\n",
            "Epoch 145/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1315 - accuracy: 0.9663\n",
            "Epoch 146/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1121 - accuracy: 0.9865\n",
            "Epoch 147/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1059 - accuracy: 0.9783\n",
            "Epoch 148/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1308 - accuracy: 0.9720\n",
            "Epoch 149/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.9708\n",
            "Epoch 150/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0938 - accuracy: 0.9940\n",
            "Epoch 151/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.9535\n",
            "Epoch 152/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1220 - accuracy: 0.9640\n",
            "Epoch 153/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0968 - accuracy: 0.9834\n",
            "Epoch 154/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1070 - accuracy: 0.9702\n",
            "Epoch 155/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1106 - accuracy: 0.9645\n",
            "Epoch 156/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1077 - accuracy: 0.9774\n",
            "Epoch 157/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1107 - accuracy: 0.9696\n",
            "Epoch 158/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1238 - accuracy: 0.9659\n",
            "Epoch 159/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1235 - accuracy: 0.9661\n",
            "Epoch 160/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1265 - accuracy: 0.9633\n",
            "Epoch 161/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0935 - accuracy: 0.9798\n",
            "Epoch 162/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1077 - accuracy: 0.9882\n",
            "Epoch 163/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1115 - accuracy: 0.9649\n",
            "Epoch 164/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1196 - accuracy: 0.9359\n",
            "Epoch 165/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0818 - accuracy: 0.9792\n",
            "Epoch 166/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1214 - accuracy: 0.9539\n",
            "Epoch 167/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1133 - accuracy: 0.9824\n",
            "Epoch 168/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0873 - accuracy: 0.9840\n",
            "Epoch 169/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1166 - accuracy: 0.9582\n",
            "Epoch 170/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0835 - accuracy: 0.9752\n",
            "Epoch 171/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0871 - accuracy: 0.9682\n",
            "Epoch 172/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1764 - accuracy: 0.9211\n",
            "Epoch 173/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0857 - accuracy: 0.9851\n",
            "Epoch 174/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1145 - accuracy: 0.9459\n",
            "Epoch 175/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0873 - accuracy: 0.9857\n",
            "Epoch 176/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1119 - accuracy: 0.9578\n",
            "Epoch 177/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0778 - accuracy: 0.9798\n",
            "Epoch 178/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1073 - accuracy: 0.9576\n",
            "Epoch 179/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1327 - accuracy: 0.9503\n",
            "Epoch 180/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0905 - accuracy: 0.9751\n",
            "Epoch 181/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0979 - accuracy: 0.9735\n",
            "Epoch 182/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0802 - accuracy: 0.9764\n",
            "Epoch 183/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1094 - accuracy: 0.9675\n",
            "Epoch 184/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0891 - accuracy: 0.9751\n",
            "Epoch 185/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0959 - accuracy: 0.9464\n",
            "Epoch 186/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0894 - accuracy: 0.9886\n",
            "Epoch 187/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0847 - accuracy: 0.9773\n",
            "Epoch 188/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1036 - accuracy: 0.9696\n",
            "Epoch 189/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1051 - accuracy: 0.9804\n",
            "Epoch 190/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0983 - accuracy: 0.9619\n",
            "Epoch 191/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1175 - accuracy: 0.9439\n",
            "Epoch 192/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0855 - accuracy: 0.9611\n",
            "Epoch 193/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0808 - accuracy: 0.9764\n",
            "Epoch 194/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1037 - accuracy: 0.9513\n",
            "Epoch 195/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0786 - accuracy: 0.9855\n",
            "Epoch 196/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0892 - accuracy: 0.9662\n",
            "Epoch 197/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1089 - accuracy: 0.9703\n",
            "Epoch 198/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0702 - accuracy: 0.9874\n",
            "Epoch 199/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1038 - accuracy: 0.9591\n",
            "Epoch 200/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0965 - accuracy: 0.9796\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fd0a7ec1598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "3/3 [==============================] - 0s 2ms/step - loss: 0.0352 - accuracy: 1.0000\n",
            "Epoch 1/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 2.2176 - accuracy: 0.3173\n",
            "Epoch 2/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.7138 - accuracy: 0.2379\n",
            "Epoch 3/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.3114 - accuracy: 0.1883\n",
            "Epoch 4/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.1977 - accuracy: 0.1631\n",
            "Epoch 5/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.1585 - accuracy: 0.1734\n",
            "Epoch 6/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.0658 - accuracy: 0.2128\n",
            "Epoch 7/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.0048 - accuracy: 0.2289\n",
            "Epoch 8/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.9237 - accuracy: 0.2587\n",
            "Epoch 9/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.8978 - accuracy: 0.2466\n",
            "Epoch 10/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.8786 - accuracy: 0.3090\n",
            "Epoch 11/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.8323 - accuracy: 0.4694\n",
            "Epoch 12/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.8021 - accuracy: 0.6187\n",
            "Epoch 13/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.7992 - accuracy: 0.6217\n",
            "Epoch 14/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.7243 - accuracy: 0.6503\n",
            "Epoch 15/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.7414 - accuracy: 0.6173\n",
            "Epoch 16/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.7400 - accuracy: 0.6057\n",
            "Epoch 17/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.6856 - accuracy: 0.6178\n",
            "Epoch 18/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.6891 - accuracy: 0.6281\n",
            "Epoch 19/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.6477 - accuracy: 0.6829\n",
            "Epoch 20/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6533 - accuracy: 0.6341\n",
            "Epoch 21/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.7011 - accuracy: 0.6147\n",
            "Epoch 22/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.6532 - accuracy: 0.6507\n",
            "Epoch 23/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.5888 - accuracy: 0.7396\n",
            "Epoch 24/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.6187 - accuracy: 0.6792\n",
            "Epoch 25/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.5747 - accuracy: 0.7172\n",
            "Epoch 26/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5983 - accuracy: 0.6807\n",
            "Epoch 27/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5334 - accuracy: 0.7391\n",
            "Epoch 28/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5753 - accuracy: 0.6496\n",
            "Epoch 29/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5389 - accuracy: 0.7262\n",
            "Epoch 30/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5169 - accuracy: 0.7321\n",
            "Epoch 31/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5513 - accuracy: 0.7549\n",
            "Epoch 32/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5514 - accuracy: 0.7272\n",
            "Epoch 33/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5434 - accuracy: 0.7345\n",
            "Epoch 34/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5436 - accuracy: 0.7220\n",
            "Epoch 35/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4848 - accuracy: 0.7798\n",
            "Epoch 36/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4982 - accuracy: 0.7676\n",
            "Epoch 37/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5486 - accuracy: 0.6911\n",
            "Epoch 38/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4867 - accuracy: 0.7985\n",
            "Epoch 39/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.5014 - accuracy: 0.7168\n",
            "Epoch 40/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4702 - accuracy: 0.7080\n",
            "Epoch 41/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4704 - accuracy: 0.7695\n",
            "Epoch 42/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4244 - accuracy: 0.8516\n",
            "Epoch 43/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4461 - accuracy: 0.8487\n",
            "Epoch 44/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4565 - accuracy: 0.8098\n",
            "Epoch 45/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4789 - accuracy: 0.7466\n",
            "Epoch 46/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4322 - accuracy: 0.7918\n",
            "Epoch 47/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4040 - accuracy: 0.8373\n",
            "Epoch 48/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4685 - accuracy: 0.7960\n",
            "Epoch 49/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4167 - accuracy: 0.8450\n",
            "Epoch 50/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4269 - accuracy: 0.8147\n",
            "Epoch 51/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4651 - accuracy: 0.7395\n",
            "Epoch 52/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4500 - accuracy: 0.8561\n",
            "Epoch 53/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4174 - accuracy: 0.8296\n",
            "Epoch 54/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4089 - accuracy: 0.8398\n",
            "Epoch 55/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4019 - accuracy: 0.8349\n",
            "Epoch 56/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3676 - accuracy: 0.8923\n",
            "Epoch 57/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3774 - accuracy: 0.8490\n",
            "Epoch 58/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3826 - accuracy: 0.9238\n",
            "Epoch 59/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3867 - accuracy: 0.9127\n",
            "Epoch 60/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4169 - accuracy: 0.8666\n",
            "Epoch 61/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3889 - accuracy: 0.8772\n",
            "Epoch 62/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3828 - accuracy: 0.9172\n",
            "Epoch 63/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3833 - accuracy: 0.9079\n",
            "Epoch 64/200\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3427 - accuracy: 0.9553\n",
            "Epoch 65/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3908 - accuracy: 0.9165\n",
            "Epoch 66/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3681 - accuracy: 0.9286\n",
            "Epoch 67/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3404 - accuracy: 0.9222\n",
            "Epoch 68/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3370 - accuracy: 0.9569\n",
            "Epoch 69/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3528 - accuracy: 0.9148\n",
            "Epoch 70/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3463 - accuracy: 0.9261\n",
            "Epoch 71/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3519 - accuracy: 0.9501\n",
            "Epoch 72/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3490 - accuracy: 0.9064\n",
            "Epoch 73/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3273 - accuracy: 0.9151\n",
            "Epoch 74/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3437 - accuracy: 0.9710\n",
            "Epoch 75/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3159 - accuracy: 0.9532\n",
            "Epoch 76/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3320 - accuracy: 0.9429\n",
            "Epoch 77/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3190 - accuracy: 0.9363\n",
            "Epoch 78/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3226 - accuracy: 0.9342\n",
            "Epoch 79/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3295 - accuracy: 0.9552\n",
            "Epoch 80/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3026 - accuracy: 0.9625\n",
            "Epoch 81/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2845 - accuracy: 0.9532\n",
            "Epoch 82/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3047 - accuracy: 0.9519\n",
            "Epoch 83/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2951 - accuracy: 0.9638\n",
            "Epoch 84/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2872 - accuracy: 0.9395\n",
            "Epoch 85/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2938 - accuracy: 0.9622\n",
            "Epoch 86/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2793 - accuracy: 0.9661\n",
            "Epoch 87/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2948 - accuracy: 0.9501\n",
            "Epoch 88/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3017 - accuracy: 0.9657\n",
            "Epoch 89/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2693 - accuracy: 0.9493\n",
            "Epoch 90/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2431 - accuracy: 0.9504\n",
            "Epoch 91/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2831 - accuracy: 0.9563\n",
            "Epoch 92/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2885 - accuracy: 0.9346\n",
            "Epoch 93/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2774 - accuracy: 0.9642\n",
            "Epoch 94/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2820 - accuracy: 0.9647\n",
            "Epoch 95/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2460 - accuracy: 0.9480\n",
            "Epoch 96/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2717 - accuracy: 0.9373\n",
            "Epoch 97/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2435 - accuracy: 0.9470\n",
            "Epoch 98/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2306 - accuracy: 0.9800\n",
            "Epoch 99/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2250 - accuracy: 0.9726\n",
            "Epoch 100/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2448 - accuracy: 0.9427\n",
            "Epoch 101/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2430 - accuracy: 0.9652\n",
            "Epoch 102/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2258 - accuracy: 0.9567\n",
            "Epoch 103/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2547 - accuracy: 0.9384\n",
            "Epoch 104/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1983 - accuracy: 0.9768\n",
            "Epoch 105/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2320 - accuracy: 0.9743\n",
            "Epoch 106/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2328 - accuracy: 0.9683\n",
            "Epoch 107/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2194 - accuracy: 0.9661\n",
            "Epoch 108/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2312 - accuracy: 0.9491\n",
            "Epoch 109/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2468 - accuracy: 0.9323\n",
            "Epoch 110/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2301 - accuracy: 0.9651\n",
            "Epoch 111/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1895 - accuracy: 0.9859\n",
            "Epoch 112/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1979 - accuracy: 0.9766\n",
            "Epoch 113/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2138 - accuracy: 0.9437\n",
            "Epoch 114/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2157 - accuracy: 0.9828\n",
            "Epoch 115/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2039 - accuracy: 0.9646\n",
            "Epoch 116/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1935 - accuracy: 0.9801\n",
            "Epoch 117/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2085 - accuracy: 0.9532\n",
            "Epoch 118/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1813 - accuracy: 0.9719\n",
            "Epoch 119/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1840 - accuracy: 0.9843\n",
            "Epoch 120/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1638 - accuracy: 0.9855\n",
            "Epoch 121/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1962 - accuracy: 0.9674\n",
            "Epoch 122/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2173 - accuracy: 0.9441\n",
            "Epoch 123/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2128 - accuracy: 0.9418\n",
            "Epoch 124/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1538 - accuracy: 0.9859\n",
            "Epoch 125/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1761 - accuracy: 0.9596\n",
            "Epoch 126/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1749 - accuracy: 0.9790\n",
            "Epoch 127/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1667 - accuracy: 0.9753\n",
            "Epoch 128/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1948 - accuracy: 0.9339\n",
            "Epoch 129/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1823 - accuracy: 0.9560\n",
            "Epoch 130/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1657 - accuracy: 0.9817\n",
            "Epoch 131/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1853 - accuracy: 0.9539\n",
            "Epoch 132/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1779 - accuracy: 0.9824\n",
            "Epoch 133/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1867 - accuracy: 0.9560\n",
            "Epoch 134/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1371 - accuracy: 0.9863\n",
            "Epoch 135/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1566 - accuracy: 0.9767\n",
            "Epoch 136/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1506 - accuracy: 0.9615\n",
            "Epoch 137/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1438 - accuracy: 0.9931\n",
            "Epoch 138/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1536 - accuracy: 0.9755\n",
            "Epoch 139/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1448 - accuracy: 0.9764\n",
            "Epoch 140/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1660 - accuracy: 0.9578\n",
            "Epoch 141/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2061 - accuracy: 0.9304\n",
            "Epoch 142/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1652 - accuracy: 0.9725\n",
            "Epoch 143/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1613 - accuracy: 0.9644\n",
            "Epoch 144/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1572 - accuracy: 0.9486\n",
            "Epoch 145/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1353 - accuracy: 0.9844\n",
            "Epoch 146/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1741 - accuracy: 0.9593\n",
            "Epoch 147/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1392 - accuracy: 0.9697\n",
            "Epoch 148/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1326 - accuracy: 0.9703\n",
            "Epoch 149/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1492 - accuracy: 0.9776\n",
            "Epoch 150/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1459 - accuracy: 0.9704\n",
            "Epoch 151/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1152 - accuracy: 0.9789\n",
            "Epoch 152/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1172 - accuracy: 0.9820\n",
            "Epoch 153/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1327 - accuracy: 0.9776\n",
            "Epoch 154/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1377 - accuracy: 0.9624\n",
            "Epoch 155/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1278 - accuracy: 0.9955\n",
            "Epoch 156/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1466 - accuracy: 0.9798\n",
            "Epoch 157/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1581 - accuracy: 0.9658\n",
            "Epoch 158/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1254 - accuracy: 0.9804\n",
            "Epoch 159/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1415 - accuracy: 0.9855\n",
            "Epoch 160/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1584 - accuracy: 0.9361\n",
            "Epoch 161/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1285 - accuracy: 0.9852\n",
            "Epoch 162/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1075 - accuracy: 0.9868\n",
            "Epoch 163/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1263 - accuracy: 0.9837\n",
            "Epoch 164/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1155 - accuracy: 0.9829\n",
            "Epoch 165/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1370 - accuracy: 0.9543\n",
            "Epoch 166/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1237 - accuracy: 0.9701\n",
            "Epoch 167/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0986 - accuracy: 0.9828\n",
            "Epoch 168/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1107 - accuracy: 0.9753\n",
            "Epoch 169/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1276 - accuracy: 0.9656\n",
            "Epoch 170/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1156 - accuracy: 0.9823\n",
            "Epoch 171/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1107 - accuracy: 0.9770\n",
            "Epoch 172/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1265 - accuracy: 0.9890\n",
            "Epoch 173/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1228 - accuracy: 0.9614\n",
            "Epoch 174/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1138 - accuracy: 0.9886\n",
            "Epoch 175/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1105 - accuracy: 0.9814\n",
            "Epoch 176/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1509 - accuracy: 0.9603\n",
            "Epoch 177/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1316 - accuracy: 0.9671\n",
            "Epoch 178/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1137 - accuracy: 0.9658\n",
            "Epoch 179/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0938 - accuracy: 0.9947\n",
            "Epoch 180/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0889 - accuracy: 0.9839\n",
            "Epoch 181/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1139 - accuracy: 0.9916\n",
            "Epoch 182/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1296 - accuracy: 0.9653\n",
            "Epoch 183/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1264 - accuracy: 0.9654\n",
            "Epoch 184/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0980 - accuracy: 0.9803\n",
            "Epoch 185/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0928 - accuracy: 0.9849\n",
            "Epoch 186/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0901 - accuracy: 0.9873\n",
            "Epoch 187/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1052 - accuracy: 0.9700\n",
            "Epoch 188/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0964 - accuracy: 0.9778\n",
            "Epoch 189/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1045 - accuracy: 0.9880\n",
            "Epoch 190/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1216 - accuracy: 0.9770\n",
            "Epoch 191/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1049 - accuracy: 0.9760\n",
            "Epoch 192/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0720 - accuracy: 0.9970\n",
            "Epoch 193/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1151 - accuracy: 0.9775\n",
            "Epoch 194/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0946 - accuracy: 0.9863\n",
            "Epoch 195/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0737 - accuracy: 0.9864\n",
            "Epoch 196/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1085 - accuracy: 0.9709\n",
            "Epoch 197/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0828 - accuracy: 0.9835\n",
            "Epoch 198/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0928 - accuracy: 0.9772\n",
            "Epoch 199/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1057 - accuracy: 0.9794\n",
            "Epoch 200/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0798 - accuracy: 0.9857\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fd0a7e45b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "3/3 [==============================] - 0s 3ms/step - loss: 0.1010 - accuracy: 0.9333\n",
            "Epoch 1/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 1.8115 - accuracy: 0.3072\n",
            "Epoch 2/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 1.2432 - accuracy: 0.3012\n",
            "Epoch 3/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.9167 - accuracy: 0.3377\n",
            "Epoch 4/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.8353 - accuracy: 0.6090\n",
            "Epoch 5/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.7876 - accuracy: 0.6392\n",
            "Epoch 6/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6986 - accuracy: 0.7015\n",
            "Epoch 7/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6313 - accuracy: 0.7804\n",
            "Epoch 8/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.6425 - accuracy: 0.7376\n",
            "Epoch 9/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5766 - accuracy: 0.8409\n",
            "Epoch 10/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5523 - accuracy: 0.8341\n",
            "Epoch 11/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5353 - accuracy: 0.8791\n",
            "Epoch 12/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5305 - accuracy: 0.8594\n",
            "Epoch 13/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.5144 - accuracy: 0.8461\n",
            "Epoch 14/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4605 - accuracy: 0.9133\n",
            "Epoch 15/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4814 - accuracy: 0.8543\n",
            "Epoch 16/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4541 - accuracy: 0.8809\n",
            "Epoch 17/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4163 - accuracy: 0.9047\n",
            "Epoch 18/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3922 - accuracy: 0.9227\n",
            "Epoch 19/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4012 - accuracy: 0.9557\n",
            "Epoch 20/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4075 - accuracy: 0.8943\n",
            "Epoch 21/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.4040 - accuracy: 0.9091\n",
            "Epoch 22/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3458 - accuracy: 0.9366\n",
            "Epoch 23/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3831 - accuracy: 0.9426\n",
            "Epoch 24/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3518 - accuracy: 0.9733\n",
            "Epoch 25/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3493 - accuracy: 0.9446\n",
            "Epoch 26/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3319 - accuracy: 0.9804\n",
            "Epoch 27/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3424 - accuracy: 0.9515\n",
            "Epoch 28/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3113 - accuracy: 0.9421\n",
            "Epoch 29/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3075 - accuracy: 0.9780\n",
            "Epoch 30/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3257 - accuracy: 0.9765\n",
            "Epoch 31/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2929 - accuracy: 0.9760\n",
            "Epoch 32/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3046 - accuracy: 0.9495\n",
            "Epoch 33/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.9761\n",
            "Epoch 34/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2766 - accuracy: 0.9899\n",
            "Epoch 35/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2593 - accuracy: 0.9727\n",
            "Epoch 36/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3045 - accuracy: 0.9602\n",
            "Epoch 37/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2748 - accuracy: 0.9868\n",
            "Epoch 38/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2569 - accuracy: 0.9897\n",
            "Epoch 39/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2768 - accuracy: 0.9801\n",
            "Epoch 40/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2476 - accuracy: 0.9818\n",
            "Epoch 41/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2437 - accuracy: 0.9755\n",
            "Epoch 42/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2774 - accuracy: 0.9508\n",
            "Epoch 43/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2870 - accuracy: 0.9743\n",
            "Epoch 44/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2371 - accuracy: 0.9720\n",
            "Epoch 45/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2354 - accuracy: 0.9646\n",
            "Epoch 46/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2373 - accuracy: 0.9701\n",
            "Epoch 47/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2252 - accuracy: 0.9559\n",
            "Epoch 48/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2234 - accuracy: 0.9773\n",
            "Epoch 49/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2119 - accuracy: 0.9854\n",
            "Epoch 50/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2399 - accuracy: 0.9627\n",
            "Epoch 51/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1973 - accuracy: 0.9758\n",
            "Epoch 52/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1922 - accuracy: 0.9716\n",
            "Epoch 53/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2244 - accuracy: 0.9682\n",
            "Epoch 54/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2487 - accuracy: 0.9460\n",
            "Epoch 55/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1933 - accuracy: 0.9513\n",
            "Epoch 56/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2051 - accuracy: 0.9749\n",
            "Epoch 57/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1990 - accuracy: 0.9501\n",
            "Epoch 58/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1765 - accuracy: 0.9717\n",
            "Epoch 59/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1843 - accuracy: 0.9583\n",
            "Epoch 60/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2078 - accuracy: 0.9511\n",
            "Epoch 61/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2069 - accuracy: 0.9588\n",
            "Epoch 62/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1629 - accuracy: 0.9868\n",
            "Epoch 63/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1786 - accuracy: 0.9945\n",
            "Epoch 64/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2038 - accuracy: 0.9614\n",
            "Epoch 65/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1676 - accuracy: 0.9899\n",
            "Epoch 66/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.2236 - accuracy: 0.9612\n",
            "Epoch 67/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1795 - accuracy: 0.9693\n",
            "Epoch 68/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1781 - accuracy: 0.9503\n",
            "Epoch 69/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1797 - accuracy: 0.9668\n",
            "Epoch 70/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1716 - accuracy: 0.9723\n",
            "Epoch 71/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1452 - accuracy: 0.9602\n",
            "Epoch 72/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1907 - accuracy: 0.9548\n",
            "Epoch 73/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1480 - accuracy: 0.9891\n",
            "Epoch 74/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1553 - accuracy: 0.9648\n",
            "Epoch 75/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1593 - accuracy: 0.9881\n",
            "Epoch 76/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1823 - accuracy: 0.9409\n",
            "Epoch 77/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1399 - accuracy: 0.9721\n",
            "Epoch 78/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1398 - accuracy: 0.9743\n",
            "Epoch 79/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1463 - accuracy: 0.9730\n",
            "Epoch 80/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1606 - accuracy: 0.9553\n",
            "Epoch 81/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1418 - accuracy: 0.9601\n",
            "Epoch 82/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1561 - accuracy: 0.9593\n",
            "Epoch 83/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1185 - accuracy: 0.9907\n",
            "Epoch 84/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1431 - accuracy: 0.9720\n",
            "Epoch 85/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1564 - accuracy: 0.9715\n",
            "Epoch 86/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1532 - accuracy: 0.9706\n",
            "Epoch 87/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1467 - accuracy: 0.9571\n",
            "Epoch 88/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1380 - accuracy: 0.9775\n",
            "Epoch 89/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1327 - accuracy: 0.9913\n",
            "Epoch 90/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1392 - accuracy: 0.9789\n",
            "Epoch 91/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1325 - accuracy: 0.9762\n",
            "Epoch 92/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1334 - accuracy: 0.9659\n",
            "Epoch 93/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.9901\n",
            "Epoch 94/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1252 - accuracy: 0.9787\n",
            "Epoch 95/200\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1358 - accuracy: 0.9706\n",
            "Epoch 96/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1329 - accuracy: 0.9709\n",
            "Epoch 97/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1056 - accuracy: 0.9862\n",
            "Epoch 98/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1257 - accuracy: 0.9707\n",
            "Epoch 99/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1299 - accuracy: 0.9707\n",
            "Epoch 100/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1120 - accuracy: 0.9769\n",
            "Epoch 101/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1207 - accuracy: 0.9632\n",
            "Epoch 102/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1378 - accuracy: 0.9635\n",
            "Epoch 103/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1110 - accuracy: 0.9866\n",
            "Epoch 104/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0996 - accuracy: 0.9782\n",
            "Epoch 105/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1213 - accuracy: 0.9560\n",
            "Epoch 106/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1196 - accuracy: 0.9675\n",
            "Epoch 107/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1236 - accuracy: 0.9528\n",
            "Epoch 108/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1105 - accuracy: 0.9772\n",
            "Epoch 109/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0971 - accuracy: 0.9864\n",
            "Epoch 110/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1371 - accuracy: 0.9594\n",
            "Epoch 111/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0968 - accuracy: 0.9945\n",
            "Epoch 112/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1181 - accuracy: 0.9494\n",
            "Epoch 113/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1215 - accuracy: 0.9601\n",
            "Epoch 114/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0990 - accuracy: 0.9766\n",
            "Epoch 115/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1240 - accuracy: 0.9674\n",
            "Epoch 116/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0954 - accuracy: 0.9853\n",
            "Epoch 117/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0929 - accuracy: 0.9830\n",
            "Epoch 118/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1117 - accuracy: 0.9798\n",
            "Epoch 119/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0984 - accuracy: 0.9732\n",
            "Epoch 120/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1118 - accuracy: 0.9624\n",
            "Epoch 121/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1010 - accuracy: 0.9886\n",
            "Epoch 122/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0802 - accuracy: 0.9880\n",
            "Epoch 123/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1092 - accuracy: 0.9681\n",
            "Epoch 124/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1068 - accuracy: 0.9515\n",
            "Epoch 125/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1189 - accuracy: 0.9619\n",
            "Epoch 126/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0935 - accuracy: 0.9799\n",
            "Epoch 127/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1459 - accuracy: 0.9373\n",
            "Epoch 128/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0928 - accuracy: 0.9890\n",
            "Epoch 129/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0994 - accuracy: 0.9749\n",
            "Epoch 130/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1091 - accuracy: 0.9667\n",
            "Epoch 131/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1150 - accuracy: 0.9717\n",
            "Epoch 132/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0836 - accuracy: 0.9833\n",
            "Epoch 133/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1088 - accuracy: 0.9796\n",
            "Epoch 134/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1040 - accuracy: 0.9734\n",
            "Epoch 135/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0850 - accuracy: 0.9807\n",
            "Epoch 136/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0711 - accuracy: 0.9865\n",
            "Epoch 137/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0734 - accuracy: 0.9878\n",
            "Epoch 138/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1066 - accuracy: 0.9634\n",
            "Epoch 139/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0993 - accuracy: 0.9632\n",
            "Epoch 140/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0744 - accuracy: 0.9801\n",
            "Epoch 141/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0941 - accuracy: 0.9657\n",
            "Epoch 142/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1202 - accuracy: 0.9650\n",
            "Epoch 143/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0886 - accuracy: 0.9715\n",
            "Epoch 144/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0944 - accuracy: 0.9659\n",
            "Epoch 145/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0767 - accuracy: 0.9905\n",
            "Epoch 146/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0802 - accuracy: 0.9825\n",
            "Epoch 147/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0862 - accuracy: 0.9795\n",
            "Epoch 148/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0976 - accuracy: 0.9650\n",
            "Epoch 149/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0855 - accuracy: 0.9789\n",
            "Epoch 150/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0833 - accuracy: 0.9815\n",
            "Epoch 151/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0939 - accuracy: 0.9651\n",
            "Epoch 152/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0938 - accuracy: 0.9827\n",
            "Epoch 153/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1188 - accuracy: 0.9576\n",
            "Epoch 154/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0900 - accuracy: 0.9569\n",
            "Epoch 155/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0753 - accuracy: 0.9763\n",
            "Epoch 156/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0895 - accuracy: 0.9556\n",
            "Epoch 157/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1006 - accuracy: 0.9486\n",
            "Epoch 158/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0839 - accuracy: 0.9731\n",
            "Epoch 159/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0718 - accuracy: 0.9874\n",
            "Epoch 160/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0892 - accuracy: 0.9741\n",
            "Epoch 161/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0845 - accuracy: 0.9800\n",
            "Epoch 162/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0959 - accuracy: 0.9669\n",
            "Epoch 163/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0804 - accuracy: 0.9866\n",
            "Epoch 164/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1068 - accuracy: 0.9620\n",
            "Epoch 165/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.9807\n",
            "Epoch 166/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0691 - accuracy: 0.9839\n",
            "Epoch 167/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0870 - accuracy: 0.9684\n",
            "Epoch 168/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0668 - accuracy: 0.9816\n",
            "Epoch 169/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0709 - accuracy: 0.9822\n",
            "Epoch 170/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0967 - accuracy: 0.9448\n",
            "Epoch 171/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0804 - accuracy: 0.9776\n",
            "Epoch 172/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0923 - accuracy: 0.9698\n",
            "Epoch 173/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0663 - accuracy: 0.9770\n",
            "Epoch 174/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0811 - accuracy: 0.9740\n",
            "Epoch 175/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0795 - accuracy: 0.9711\n",
            "Epoch 176/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0702 - accuracy: 0.9799\n",
            "Epoch 177/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0633 - accuracy: 0.9782\n",
            "Epoch 178/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0915 - accuracy: 0.9829\n",
            "Epoch 179/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1033 - accuracy: 0.9612\n",
            "Epoch 180/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1102 - accuracy: 0.9457\n",
            "Epoch 181/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0662 - accuracy: 0.9923\n",
            "Epoch 182/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.1128 - accuracy: 0.9676\n",
            "Epoch 183/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0825 - accuracy: 0.9683\n",
            "Epoch 184/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0849 - accuracy: 0.9695\n",
            "Epoch 185/200\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0692 - accuracy: 0.9732\n",
            "Epoch 186/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0753 - accuracy: 0.9896\n",
            "Epoch 187/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0782 - accuracy: 0.9746\n",
            "Epoch 188/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0978 - accuracy: 0.9652\n",
            "Epoch 189/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0789 - accuracy: 0.9843\n",
            "Epoch 190/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0614 - accuracy: 0.9803\n",
            "Epoch 191/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0879 - accuracy: 0.9596\n",
            "Epoch 192/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0683 - accuracy: 0.9753\n",
            "Epoch 193/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0842 - accuracy: 0.9610\n",
            "Epoch 194/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0674 - accuracy: 0.9830\n",
            "Epoch 195/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0761 - accuracy: 0.9817\n",
            "Epoch 196/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0941 - accuracy: 0.9629\n",
            "Epoch 197/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0782 - accuracy: 0.9799\n",
            "Epoch 198/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0926 - accuracy: 0.9524\n",
            "Epoch 199/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0963 - accuracy: 0.9491\n",
            "Epoch 200/200\n",
            "27/27 [==============================] - 0s 1ms/step - loss: 0.0666 - accuracy: 0.9788\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fd0a3498730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "3/3 [==============================] - 0s 3ms/step - loss: 0.0638 - accuracy: 1.0000\n",
            "Baseline: 98.00% (3.06%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vMXe9phuvfK",
        "outputId": "a483ad82-e48f-4132-9aa5-e19b605072e3"
      },
      "source": [
        "type(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "id": "12CapOv0vYXs",
        "outputId": "ce3e204d-d92a-490f-91a8-b0fd1d81cd1c"
      },
      "source": [
        "from sklearn.datasets import load_iris\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from keras.utils import np_utils\r\n",
        "d=load_iris()\r\n",
        "x=d.data\r\n",
        "y=d.target\r\n",
        "\r\n",
        "L=LabelEncoder()\r\n",
        "L.fit(y)\r\n",
        "y=L.transform(y)\r\n",
        "y=np_utils.to_categorical(y)\r\n",
        "print(y)\r\n",
        "\r\n",
        "def create_model():\r\n",
        "  model=Sequential()\r\n",
        "  model.add(Dense(20,input_dim=len(x[0,:]),activation='relu'))\r\n",
        "  model.add(Dense(10,activation='relu'))\r\n",
        "  model.add(Dense(5,activation='relu'))\r\n",
        "\tmodel.add(Dense(3,activation='relu'))\r\n",
        "\t# Compile model\r\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\treturn model\r\n",
        "\r\n",
        "estimators = KerasClassifier(build_fn=create_model, epochs=10, batch_size=30, verbose=1)\r\n",
        "kfold = KFold(n_splits=10, shuffle=True)\r\n",
        "scores = cross_val_score(estimators,x,y, cv=10)\r\n",
        "print(\"Accuracy : {:0.2f} (+/- {:0.2f})\".format(scores.mean(), scores.std()))\r\n",
        "\r\n",
        "\r\n",
        "#model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01, momentum=0.9))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TabError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-65-e0be55630fe5>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    model.add(Dense(20,input_dim=len(x[0,:]),activation='relu'))\u001b[0m\n\u001b[0m                                                                ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "id": "4aB-j0lHT_99",
        "outputId": "2b308a6c-0c9b-4ce3-f4be-42dc15c359da"
      },
      "source": [
        "\r\n",
        "\r\n",
        "def create_model():\r\n",
        "  model=Sequential()\r\n",
        "  model.add(Dense(20,input_dim=len(x[0,:]),activation='relu'))\r\n",
        "  model.add(Dense(10,activation='relu'))\r\n",
        "  model.add(Dense(5,activation='relu'))\r\n",
        "  model.add(Dense(3,activation='relu'))\r\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "  return model\r\n",
        "\r\n",
        "  \r\n",
        "\r\n",
        "def create_model():\r\n",
        "  model=Sequential()\r\n",
        "  model.add(Dense(20,input_dim=len(x[0,:]),activation='relu'))\r\n",
        "  model.add(Dense(10,activation='relu'))\r\n",
        "  model.add(Dense(3,activation='relu'))\r\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\treturn model\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TabError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-68-384dd7c75a5e>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\u001b[0m\n\u001b[0m                                                                                          ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQJQXxeN6shv"
      },
      "source": [
        "*Training simple neural network for mnist handwritten digits using functional keras API*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k06t2r3I4bHI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da8a3421-7b3a-4231-fd85-b9f6c1ba86b9"
      },
      "source": [
        "import keras\r\n",
        "from keras.datasets import mnist\r\n",
        "(x_train, y_train), (x_test, y_test ) = mnist.load_data()\r\n",
        "\r\n",
        "x_train = x_train.reshape(-1, 784).astype('float32') / 255.0\r\n",
        "x_test = x_test.reshape(-1, 784).astype('float32') / 255.0\r\n",
        "\r\n",
        "inputs = keras.Input(shape=(784))\r\n",
        "x = Dense(512, activation='relu')(inputs)\r\n",
        "x = Dense(256, activation='relu')(x)\r\n",
        "x = Dense(128, activation='relu')(x)\r\n",
        "x = Dense(64, activation='relu')(x)\r\n",
        "outputs = Dense(10)(x)\r\n",
        "\r\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\r\n",
        "model.summary()\r\n",
        "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), \r\n",
        "              optimizer=keras.optimizers.Adam(lr=0.001), metrics=['accuracy'])\r\n",
        "\r\n",
        "hisotry = model.fit(x_train, y_train, epochs=2, batch_size=64, verbose=1)\r\n",
        "\r\n",
        "\r\n",
        "#Calculating training and testing datasets accuracy and loss function\r\n",
        "print('Training Accuracy: ',round((model.evaluate(x_train, y_train, verbose=1)[1]), 2)*100, '%')\r\n",
        "print('Loss Function of Training: ',round((model.evaluate(x_train, y_train, verbose=0)[0]), 2)*100, '%')\r\n",
        "\r\n",
        "print('Test Set Accuracy: ',round((model.evaluate(x_test, y_test, verbose=1)[1]), 2)*100, '%')\r\n",
        "\r\n",
        "\\\\\\print('Loss Function of Test Set: ',round((model.evaluate(x_test, y_test, verbose=0)[0]), 2)*100, '%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 784)]             0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 575,050\n",
            "Trainable params: 575,050\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/2\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.3966 - accuracy: 0.8796\n",
            "Epoch 2/2\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0969 - accuracy: 0.9705\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0755 - accuracy: 0.9761\n",
            "Training Accuracy:  98.0 %\n",
            "Loss Function of Training:  8.0 %\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1131 - accuracy: 0.9668\n",
            "Test Set Accuracy:  97.0 %\n",
            "Loss Function of Test Set:  11.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RwakK2Q4bLB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IgJbmqz4bOc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "01c8fb2c-a51e-4b5d-d30d-2d4599710f4d"
      },
      "source": [
        "# example of loading the cifar10 dataset\r\n",
        "from matplotlib import pyplot\r\n",
        "from keras.datasets import cifar10\r\n",
        "# load dataset\r\n",
        "(trainX, trainy), (testX, testy) = cifar10.load_data()\r\n",
        "# summarize loaded dataset\r\n",
        "print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\r\n",
        "print('Test: X=%s, y=%s' % (testX.shape, testy.shape))\r\n",
        "# plot first few images\r\n",
        "for i in range(9):\r\n",
        "\t# define subplot\r\n",
        "\tpyplot.subplot(330 + 1 + i)\r\n",
        "\t# plot raw pixel data\r\n",
        "\tpyplot.imshow(trainX[i])\r\n",
        "# show the figure\r\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n",
            "Train: X=(50000, 32, 32, 3), y=(50000, 1)\n",
            "Test: X=(10000, 32, 32, 3), y=(10000, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOvsFhEs4bSa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e7a986d-a9e6-4d14-ab10-9f38c5dbbeb4"
      },
      "source": [
        "trainy[8793]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYke32vl4bVr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f1c12f2-5dba-4ca6-d98d-f1152a3722e8"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers\r\n",
        "from tensorflow.keras.datasets import cifar10\r\n",
        "from keras.datasets import mnist\r\n",
        "\r\n",
        "\r\n",
        "(x_trainm, y_trainm), (x_testm, y_testm) = mnist.load_data()\r\n",
        "print(len(x_testm))\r\n",
        "\r\n",
        "x_trainm = x_trainm.reshape(60000,28,28,1).astype(\"float32\") / 255.0\r\n",
        "x_testm = x_testm.reshape(len(x_testm), 28,28,1).astype(\"float32\") / 255.0\r\n",
        "\r\n",
        "print(x_trainm.shape)\r\n",
        "model = keras.Sequential([    \r\n",
        "        layers.Conv2D(32, 3, input_shape=(28,28,1), padding=\"valid\", activation=\"relu\"),\r\n",
        "        layers.MaxPooling2D(),\r\n",
        "        layers.Conv2D(64, 3, activation=\"relu\"),\r\n",
        "        layers.MaxPooling2D(),\r\n",
        "        layers.Conv2D(128, 3, activation=\"relu\"),\r\n",
        "        layers.Flatten(),\r\n",
        "        layers.Dense(64, activation=\"relu\"),\r\n",
        "        layers.Dense(10),])\r\n",
        "model.summary()\r\n",
        "# model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer='Adam', metrics=['accuracy'])\r\n",
        "# model.fit(x_trainm, y_trainm, epochs=1, batch_size=200, verbose=1)\r\n",
        "# model.save('./checkpoints/my_checkpoint')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n",
            "(60000, 28, 28, 1)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 3, 3, 128)         73856     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1152)              0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 64)                73792     \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 167,114\n",
            "Trainable params: 167,114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPvinXGKJ-7X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6212940d-7eb6-49b4-bc9f-8ad9389955bd"
      },
      "source": [
        "from keras.datasets import mnist\r\n",
        "import tensorflow as tf\r\n",
        "import keras\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Conv2D\r\n",
        "from keras.layers import Flatten\r\n",
        "from keras.layers import MaxPooling2D\r\n",
        "from keras import utils\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "print(keras.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "hcxR7TwKqUmG",
        "outputId": "589ba1ef-eaa6-4eaa-90c0-fc352a67620d"
      },
      "source": [
        "(x_train, y_train), (x_test,y_test) = mnist.load_data()\r\n",
        "\r\n",
        "#Preprocessing\r\n",
        "x_train = x_train.reshape(60000, 28, 28, 1).astype('float32') / 255.0\r\n",
        "x_test = x_test.reshape(10000, 28, 28, 1).astype('float32') / 255.0\r\n",
        "\r\n",
        "y_train = utils.to_categorical(y_train)\r\n",
        "y_test = utils.to_categorical(y_test)\r\n",
        "\r\n",
        "for i in range(9):\r\n",
        "  plt.subplot(3,3,i+1)\r\n",
        "  plt.xticks([])\r\n",
        "  plt.yticks([])\r\n",
        "  plt.title(np.argmax(y_train[i]))\r\n",
        "  plt.tight_layout()\r\n",
        "  #argmax(two_examples_one_hot, axis=2)\r\n",
        "  plt.imshow(x_train[i].reshape(28,28))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ire9BYQDo9tQ",
        "outputId": "db2295ef-42d7-495a-8780-4c3885030c99"
      },
      "source": [
        "from keras import layers\r\n",
        "(x_train, y_train), (x_test,y_test) = mnist.load_data()\r\n",
        "\r\n",
        "#Preprocessing\r\n",
        "x_train = x_train.reshape(60000, 28, 28, 1).astype('float32') / 255.0\r\n",
        "x_test = x_test.reshape(10000, 28, 28, 1).astype('float32') / 255.0\r\n",
        "\r\n",
        "#y_train = utils.to_categorical(y_train)\r\n",
        "#y_test = utils.to_categorical(y_test)\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "layers.Conv2D(32, 3, input_shape=(28,28,1), padding=\"same\", activation=\"relu\"),\r\n",
        "layers.MaxPooling2D((2,2), padding=\"same\",),\r\n",
        "layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\r\n",
        "layers.MaxPooling2D((2,2), padding=\"same\",)\r\n",
        "layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\"),\r\n",
        "layers.Flatten(),\r\n",
        "layers.Dense(200, activation=\"relu\"),\r\n",
        "layers.Dense(100, activation=\"relu\"),\r\n",
        "layers.Dense(10)\r\n",
        "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), \r\n",
        "optimizer='adam', metrics=['accuracy'])\r\n",
        "history = model.fit(x_train, y_train, batch_size=500, epochs=1, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-963b897fcf62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n\u001b[1;32m     22\u001b[0m optimizer='adam', metrics=['accuracy'])\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:756 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:203 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:152 __call__\n        losses = call_fn(y_true, y_pred)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:256 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:1569 sparse_categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4941 sparse_categorical_crossentropy\n        labels=target, logits=output)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py:4241 sparse_softmax_cross_entropy_with_logits_v2\n        labels=labels, logits=logits, name=name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py:4156 sparse_softmax_cross_entropy_with_logits\n        logits.get_shape()))\n\n    ValueError: Shape mismatch: The shape of labels (received (500,)) should equal the shape of logits except for the last dimension (received (392000, 1)).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mx2-j0Exqgw",
        "outputId": "b1d0deaa-75e4-45db-d789-c2f0ea7de0f7"
      },
      "source": [
        "model = Sequential()\r\n",
        "model.add(Conv2D(8,3, input_shape=(28,28,1), padding='same', activation='relu', kernel_initializer='he_uniform'))\r\n",
        "model.add(MaxPooling2D(2,2))\r\n",
        "model.add(Conv2D(10,3, activation='relu', kernel_initializer='he_uniform'))\r\n",
        "model.add(MaxPooling2D(2,2))\r\n",
        "model.add(Conv2D(12,3,  activation='relu', kernel_initializer='he_uniform'))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(200, activation='relu'))\r\n",
        "model.add(Dense(100, activation='relu'))\r\n",
        "model.add(Dense(10, activation='softmax'))\r\n",
        "model.summary()\r\n",
        "\r\n",
        "'''\r\n",
        "        keras.Input(shape=(28,28,1)),\r\n",
        "        layers.Conv2D(32, 3, padding=\"valid\", activation=\"relu\"),\r\n",
        "        layers.MaxPooling2D(),\r\n",
        "        layers.Conv2D(64, 3, activation=\"relu\"),\r\n",
        "        layers.MaxPooling2D(),\r\n",
        "        layers.Conv2D(128, 3, activation=\"relu\"),\r\n",
        "        layers.Flatten(),\r\n",
        "        layers.Dense(200, activation=\"relu\"),\r\n",
        "        layers.Dense(100, activation=\"relu\"),\r\n",
        "        layers.Dense(10)])\r\n",
        "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), \r\n",
        "optimizer='adam', metrics=['accuracy'])\r\n",
        "history = model.fit(x_trainm, y_trainm, batch_size=500, epochs=1, verbose=1)\r\n",
        "\r\n",
        "'''\r\n",
        "\r\n",
        "\r\n",
        "model.compile(loss=keras.losses.CategoricalCrossentropy(),\r\n",
        "              optimizer=keras.optimizers.Adam(), \r\n",
        "              metrics = ['accuracy'])\r\n",
        "history = model.fit(x_train, y_train, validation_data=(x_test,y_test), batch_size=200, epochs=1, verbose=1)\r\n",
        "\r\n",
        "print('Training_Accuracy: ', model.evaluate(x_train, y_train, verbose=1)[1])\r\n",
        "print(' Training_Losse: ', model.evaluate(x_train, y_train, verbose=1)[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_44\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_141 (Conv2D)          (None, 28, 28, 8)         80        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_99 (MaxPooling (None, 14, 14, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_142 (Conv2D)          (None, 12, 12, 10)        730       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_100 (MaxPoolin (None, 6, 6, 10)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_143 (Conv2D)          (None, 4, 4, 12)          1092      \n",
            "_________________________________________________________________\n",
            "flatten_28 (Flatten)         (None, 192)               0         \n",
            "_________________________________________________________________\n",
            "dense_66 (Dense)             (None, 200)               38600     \n",
            "_________________________________________________________________\n",
            "dense_67 (Dense)             (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "dense_68 (Dense)             (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 61,612\n",
            "Trainable params: 61,612\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "300/300 [==============================] - 21s 68ms/step - loss: 0.6772 - accuracy: 0.7949 - val_loss: 0.0864 - val_accuracy: 0.9732\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: 0.0909 - accuracy: 0.9718\n",
            "Training_Accuracy:  0.9718499779701233\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0909 - accuracy: 0.9718\n",
            " Training_Losse:  0.09088786691427231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZH0YtvtAH62",
        "outputId": "6e0d8fb0-8170-45a9-ea52-2eef136790d7"
      },
      "source": [
        "model = Sequential()\r\n",
        "model.add(Conv2D(8,3, input_shape=(28,28,1), padding='same', activation='relu', kernel_initializer='he_uniform'))\r\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\r\n",
        "model.add(Conv2D(10,3, activation='relu',  padding='same', kernel_initializer='he_uniform'))\r\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\r\n",
        "model.add(Conv2D(12,3,  activation='relu', padding='same', kernel_initializer='he_uniform'))\r\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\r\n",
        "model.add(Conv2D(16,3,  activation='relu', padding='same', kernel_initializer='he_uniform'))\r\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(200, activation='relu'))\r\n",
        "model.add(Dense(100, activation='relu'))\r\n",
        "model.add(Dense(10, activation='softmax'))\r\n",
        "model.summary()\r\n",
        "\r\n",
        "model.compile(loss=keras.losses.CategoricalCrossentropy(),\r\n",
        "               optimizer=keras.optimizers.Adam(), \r\n",
        "               metrics = ['accuracy'])\r\n",
        "history = model.fit(x_train, y_train, validation_data=(x_test,y_test), batch_size=200, epochs=1, verbose=1)\r\n",
        "\r\n",
        "print('Training_Accuracy: ', model.evaluate(x_train, y_train, verbose=1)[1])\r\n",
        "print(' Training_Losse: ', model.evaluate(x_train, y_train, verbose=1)[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_8 (Conv2D)            (None, 28, 28, 8)         80        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 14, 14, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 14, 14, 10)        730       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 7, 7, 10)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 7, 7, 12)          1092      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 4, 4, 12)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 4, 4, 16)          1744      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 2, 2, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 200)               13000     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 37,756\n",
            "Trainable params: 37,756\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "300/300 [==============================] - 26s 83ms/step - loss: 0.8475 - accuracy: 0.7391 - val_loss: 0.1489 - val_accuracy: 0.9508\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.1649 - accuracy: 0.9478\n",
            "Training_Accuracy:  0.9478166699409485\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.1649 - accuracy: 0.9478\n",
            " Training_Losse:  0.164913609623909\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4znLHNwRyCgm",
        "outputId": "27e5296a-e4c0-46f2-d95d-7e5269a64cc8"
      },
      "source": [
        "model = Sequential()\r\n",
        "model.add(Conv2D(8,3, input_shape=(28,28,1), padding='same', strides=1, activation='relu', kernel_initializer='he_uniform'))\r\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\r\n",
        "model.add(Conv2D(16,3, activation='relu',  padding='same', strides=1, kernel_initializer='he_uniform'))\r\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\r\n",
        "model.add(Conv2D(32,3,  activation='relu', padding='same', strides=1, kernel_initializer='he_uniform'))\r\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\r\n",
        "model.add(Conv2D(64,3,  activation='relu', padding='same', strides=1, kernel_initializer='he_uniform'))\r\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(200, activation='relu'))\r\n",
        "model.add(Dense(100, activation='relu'))\r\n",
        "model.add(Dense(10, activation='softmax'))\r\n",
        "model.summary()\r\n",
        "\r\n",
        "model.compile(loss=keras.losses.CategoricalCrossentropy(),\r\n",
        "               optimizer=keras.optimizers.Adam(), \r\n",
        "               metrics = ['accuracy'])\r\n",
        "history = model.fit(x_train, y_train, validation_data=(x_test,y_test), batch_size=200, epochs=1, verbose=1)\r\n",
        "\r\n",
        "print('Training_Accuracy: ', model.evaluate(x_train, y_train, verbose=1)[1])\r\n",
        "print(' Training_Losse: ', model.evaluate(x_train, y_train, verbose=1)[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_16 (Conv2D)           (None, 28, 28, 8)         80        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling (None, 14, 14, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 14, 14, 16)        1168      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 7, 7, 32)          4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling (None, 4, 4, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 4, 4, 64)          18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_19 (MaxPooling (None, 2, 2, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 200)               51400     \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 96,894\n",
            "Trainable params: 96,894\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "300/300 [==============================] - 30s 99ms/step - loss: 0.7438 - accuracy: 0.7694 - val_loss: 0.0953 - val_accuracy: 0.9690\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 0.1016 - accuracy: 0.9674\n",
            "Training_Accuracy:  0.9674166440963745\n",
            "1875/1875 [==============================] - 16s 9ms/step - loss: 0.1016 - accuracy: 0.9674\n",
            " Training_Losse:  0.10157102346420288\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Enl5sz6YyCou",
        "outputId": "aa8c3ef5-ee85-4740-cfe5-d924df5a0167"
      },
      "source": [
        "# example of calculation 1d convolutions\r\n",
        "from numpy import asarray\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Conv1D\r\n",
        "# define input data\r\n",
        "data = asarray([0, 0, 0, 1, 1, 0, 0, 0])\r\n",
        "data = data.reshape(1, 8, 1)\r\n",
        "# create model\r\n",
        "model = Sequential()\r\n",
        "model.add(Conv1D(1, 3, input_shape=(8, 1)))\r\n",
        "# define a vertical line detector\r\n",
        "weights = [asarray([[[0]],[[1]],[[0]]]), asarray([0.0])]\r\n",
        "# store the weights in the model\r\n",
        "model.set_weights(weights)\r\n",
        "# confirm they were stored\r\n",
        "print(model.get_weights())\r\n",
        "# apply filter to input data\r\n",
        "yhat = model.predict(data)\r\n",
        "print(yhat)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[[0.]],\n",
            "\n",
            "       [[1.]],\n",
            "\n",
            "       [[0.]]], dtype=float32), array([0.], dtype=float32)]\n",
            "[[[0.]\n",
            "  [0.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [0.]\n",
            "  [0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKsXnZv6yCr1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5962f61-c3e6-4bba-8c64-8d6c8066f66b"
      },
      "source": [
        "from keras import utils\r\n",
        "from keras.datasets import mnist\r\n",
        "import numpy as np\r\n",
        "from sklearn.preprocessing import OneHotEncoder\r\n",
        "\r\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n",
        "print('shape of x_train :', x_train.shape)\r\n",
        "print('shape of y_train :', y_train.shape)\r\n",
        "print('shape of x_train :', x_test.shape)\r\n",
        "print('shape of x_test :', x_test.shape)\r\n",
        "\r\n",
        "#Transformation\r\n",
        "y_train = utils.to_categorical(y_train)\r\n",
        "print(y_train.shape)\r\n",
        "\r\n",
        "#Retranformation to original shape\r\n",
        "a=1119\r\n",
        "print('Actual y value after UTIL transformation: ', y_train[a])\r\n",
        "y_train = np.argmax(y_train, axis=1)\r\n",
        "print('Final y_train shape: ',y_train.shape)\r\n",
        "print('Actual y_value after retransformation: ', y_train[a])\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of x_train : (60000, 28, 28)\n",
            "shape of y_train : (60000,)\n",
            "shape of x_train : (10000, 28, 28)\n",
            "shape of x_test : (10000, 28, 28)\n",
            "(60000, 10)\n",
            "Actual y value after UTIL transformation:  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Final y_train shape:  (60000,)\n",
            "Actual y_value after retransformation:  3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2fXCFxKvG23",
        "outputId": "17541aeb-247e-4eae-86f4-b7a603276ffc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TS2ixPyvHAx",
        "outputId": "eec2cee9-a03b-4f73-80f0-cc7873fa90f5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual y value after UTIL transformation:  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "[5 0 4 ... 5 6 8]\n",
            "Actual y_value after retransformation:  3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjYcU2e1vHGl",
        "outputId": "a3614177-45d4-49af-c390-70a5b6b99516"
      },
      "source": [
        "y = np.argmax(y_train, axis=-1)\r\n",
        "print(y[23])\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20b6IJevvHKb",
        "outputId": "8ad19577-6e0a-41ca-ff79-30fc3eef2b5a"
      },
      "source": [
        "yt.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6jm-_xBvHOr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sDwkUoZvHUJ",
        "outputId": "2a76c6ec-6e7a-40d4-9f57-5a33e9a647b8"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import keras\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from keras.datasets import mnist\r\n",
        "from keras import utils\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\r\n",
        "\r\n",
        "\r\n",
        "(x_train,y_train),(x_test, y_test) = mnist.load_data()\r\n",
        "print(keras.__version__)\r\n",
        "\r\n",
        "#Preprocessing\r\n",
        "x_train = x_train.reshape(-1,28,28,1).astype('float32')/255.0\r\n",
        "x_test = x_test.reshape(-1,28,28,1).astype('float32')/255.0\r\n",
        "y_train = utils.to_categorical(y_train)\r\n",
        "y_test = utils.to_categorical(y_test)\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "\r\n",
        "model.add(Conv2D(32,3, padding='same', input_shape=(28,28,1), activation='relu'))\r\n",
        "model.add(MaxPooling2D(2,2))\r\n",
        "model.add(Conv2D(64,3, padding='same', activation='relu'))\r\n",
        "model.add(MaxPooling2D(2,2))\r\n",
        "model.add(Conv2D(128,3, activation='relu'))\r\n",
        "model.add(MaxPooling2D(2,2))\r\n",
        "model.add(Conv2D(256,3,padding='same',  activation='relu'))\r\n",
        "model.add(MaxPooling2D(2,2))\r\n",
        "model.add(Conv2D(512,3,padding='same',  activation='relu'))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(500, activation='relu'))\r\n",
        "model.add(Dense(10, activation='softmax'))\r\n",
        "print(model.summary())\r\n",
        "\r\n",
        "model.compile(loss='categorical_crossentropy',\r\n",
        "              optimizer=keras.optimizers.SGD(lr=0.001), \r\n",
        "              metrics=['accuracy'])\r\n",
        "\r\n",
        "history = model.fit(x_train, y_train, \r\n",
        "                    validation_data=(x_test, y_test),\r\n",
        "                    epochs=1, batch_size=50, verbose=1)\r\n",
        "\r\n",
        "print('Training_Accuracy: ', model.evaluate(x_train, y_train, verbose=1)[1])\r\n",
        "print(' Training_Losse: ', model.evaluate(x_train, y_train, verbose=1)[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.3\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 14, 14, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 5, 5, 128)         73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 2, 2, 256)         295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 1, 1, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 500)               256500    \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 10)                5010      \n",
            "=================================================================\n",
            "Total params: 1,829,510\n",
            "Trainable params: 1,829,510\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "1200/1200 [==============================] - 312s 260ms/step - loss: 2.3016 - accuracy: 0.1111 - val_loss: 2.2992 - val_accuracy: 0.1135\n",
            "1875/1875 [==============================] - 36s 19ms/step - loss: 2.2992 - accuracy: 0.1124\n",
            "Training_Accuracy:  0.11236666887998581\n",
            "1875/1875 [==============================] - 36s 19ms/step - loss: 2.2992 - accuracy: 0.1124\n",
            " Training_Losse:  2.2992284297943115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TElILJgwvHiy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "a1452ce2-46c1-4fc6-99c3-88f105c735f7"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import keras\r\n",
        "from keras.datasets import mnist\r\n",
        "from keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\r\n",
        "from keras.layers import BatchNormalization, Dropout\r\n",
        "from keras import utils\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import seaborn as sns\r\n",
        "from keras.models import Sequential\r\n",
        "\r\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\r\n",
        "\r\n",
        "\r\n",
        "# Visualizing the data\r\n",
        "for x in range(9):\r\n",
        "    plt.subplot(3,3,x+1)\r\n",
        "    plt.tight_layout()\r\n",
        "    plt.xticks([])\r\n",
        "    plt.yticks([])\r\n",
        "    plt.title(y_train[x])\r\n",
        "    plt.imshow(x_train[x])\r\n",
        "\r\n",
        "#Preprocessing\r\n",
        "x_train = x_train.reshape(-1,28,28,1).astype('float32')/255.0\r\n",
        "x_test = x_test.reshape(-1,28,28,1).astype('float32')/255.0\r\n",
        "y_train = utils.to_categorical(y_train)\r\n",
        "y_test = utils.to_categorical(y_test)\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBbatkhAUQz6",
        "outputId": "318d2b54-dd7c-48fe-a7f6-5824509172c6"
      },
      "source": [
        "model=Sequential()\r\n",
        "model.add(Conv2D(30,3,input_shape=(28,28,1), padding='same',\r\n",
        "                 activation='relu'))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(Conv2D(40,3,padding='same', activation='relu'))\r\n",
        "model.add(MaxPooling2D(2,2))\r\n",
        "model.add(Conv2D(50,3,padding='same', activation='relu'))\r\n",
        "model.add(MaxPooling2D(2,2))\r\n",
        "model.add(Conv2D(60,3, padding='same', activation='relu'))\r\n",
        "model.add(MaxPooling2D(2,2))\r\n",
        "model.add(Dropout(0.25))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(100, activation='relu'))\r\n",
        "model.add(Dense(50, activation='relu'))\r\n",
        "model.add(Dense(10, activation='softmax'))\r\n",
        "model.summary()\r\n",
        "\r\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy',\r\n",
        "              metrics=['accuracy'])\r\n",
        "history= model.fit(x_train, y_train, epochs=2,batch_size=200,\r\n",
        "                   verbose=1)\r\n",
        "print('done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 30)        300       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 28, 28, 30)        120       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 40)        10840     \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 40)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 14, 14, 50)        18050     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 50)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 7, 7, 60)          27060     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 60)          0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 3, 3, 60)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 540)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               54100     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 50)                5050      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                510       \n",
            "=================================================================\n",
            "Total params: 116,030\n",
            "Trainable params: 115,970\n",
            "Non-trainable params: 60\n",
            "_________________________________________________________________\n",
            "Epoch 1/2\n",
            "300/300 [==============================] - 201s 668ms/step - loss: 0.6998 - accuracy: 0.7650\n",
            "Epoch 2/2\n",
            "300/300 [==============================] - 201s 671ms/step - loss: 0.0626 - accuracy: 0.9808\n",
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9vj3DnZUe-G",
        "outputId": "09de4373-1c7a-4d5c-fe5a-ce56a0171e21"
      },
      "source": [
        "print('loss Function: ', round(model.evaluate(x_train,y_train, verbose=1)[0],2)*100,'%')\r\n",
        "print('Accuracy Funciton: ',round(model.evaluate(x_train,y_train, verbose=1)[1],2)*100,'%')\r\n",
        "print('Validaiton Loss function', round(model.evaluate(x_test, y_test, verbose=1)[0], 2)*100,'%')\r\n",
        "print('Accuracy of Validation set', round(model.evaluate(x_test, y_test, verbose=1)[1], 2)*100, '%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1875/1875 [==============================] - 59s 31ms/step - loss: 0.0364 - accuracy: 0.9908\n",
            "loss Function:  4.0 %\n",
            "1875/1875 [==============================] - 59s 31ms/step - loss: 0.0364 - accuracy: 0.9908\n",
            "Accuracy Funciton:  99.0 %\n",
            "313/313 [==============================] - 10s 32ms/step - loss: 0.0369 - accuracy: 0.9899\n",
            "Validaiton Loss function 4.0 %\n",
            "313/313 [==============================] - 10s 32ms/step - loss: 0.0369 - accuracy: 0.9899\n",
            "Accuracy of Validation set 99.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ek7LgLfxUA0e"
      },
      "source": [
        "###Classification Report & Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xJMo0XY7Vu-F",
        "outputId": "02f594ae-d17c-42b9-95bb-46afb45c580b"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import keras\r\n",
        "from keras.datasets import mnist\r\n",
        "from keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\r\n",
        "from keras.layers import BatchNormalization, Dropout\r\n",
        "from keras import utils\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import seaborn as sns\r\n",
        "from keras.models import Sequential\r\n",
        "from sklearn.metrics import classification_report,confusion_matrix\r\n",
        "\r\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\r\n",
        "\r\n",
        "\r\n",
        "#Preprocessing\r\n",
        "x_train = x_train.reshape(-1,28,28,1).astype('float32')/255.0\r\n",
        "x_test = x_test.reshape(-1,28,28,1).astype('float32')/255.0\r\n",
        "y_train = utils.to_categorical(y_train)\r\n",
        "y_test = utils.to_categorical(y_test)\r\n",
        "\r\n",
        "model=Sequential()\r\n",
        "model.add(Conv2D(30,3,input_shape=(28,28,1), padding='same',\r\n",
        "                 activation='relu'))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(Conv2D(40,3,padding='same', activation='relu'))\r\n",
        "model.add(MaxPooling2D(2,2))\r\n",
        "model.add(Conv2D(50,3,padding='same', activation='relu'))\r\n",
        "model.add(MaxPooling2D(2,2))\r\n",
        "model.add(Conv2D(60,3, padding='same', activation='relu'))\r\n",
        "model.add(MaxPooling2D(2,2))\r\n",
        "model.add(Dropout(0.25))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(100, activation='relu'))\r\n",
        "model.add(Dense(50, activation='relu'))\r\n",
        "model.add(Dense(10, activation='sigmoid'))\r\n",
        "model.summary()\r\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy' , metrics=['accuracy'])\r\n",
        "history= model.fit(x_train, y_train, epochs=4, batch_size=200, verbose=1)\r\n",
        "\r\n",
        "# Evaluating the Accuracy and Loss\r\n",
        "# print('loss Function: ', round(model.evaluate(x_train,y_train, verbose=1)[0],2)*100,'%')\r\n",
        "# print('Accuracy Funciton: ',round(model.evaluate(x_train,y_train, verbose=1)[1],2)*100,'%')\r\n",
        "# print('Validaiton Loss function', round(model.evaluate(x_test, y_test, verbose=1)[0], 2)*100,'%')\r\n",
        "# print('Accuracy of Validation set', round(model.evaluate(x_test, y_test, verbose=1)[1], 2)*100, '%')\r\n",
        "\r\n",
        "# Instead of probabilities it provides class labels\r\n",
        "y_pred_classes = model.predict_classes(x_test)\r\n",
        "\r\n",
        "# Reverting one-hot encoding on true validation output labels\r\n",
        "y_test_classes = np.argmax(y_test,axis=1)\r\n",
        "print(\"################ CLASSIFICATION REPORT ################\")\r\n",
        "print(classification_report(y_test_classes,y_pred_classes),\"\\n\\n\")\r\n",
        "print(\"################ CONFUSION MATRIX ################\")\r\n",
        "plt.figure(figsize=(8,8))\r\n",
        "sns.heatmap(confusion_matrix(y_test_classes,y_pred_classes),linewidths=.5,cmap=\"YlGnBu\",annot=True,cbar=False,fmt='d')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "\r\n",
        "print('done')\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 30)        300       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 28, 28, 30)        120       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 40)        10840     \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 40)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 14, 14, 50)        18050     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 50)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 7, 7, 60)          27060     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 60)          0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 3, 3, 60)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 540)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               54100     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 50)                5050      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                510       \n",
            "=================================================================\n",
            "Total params: 116,030\n",
            "Trainable params: 115,970\n",
            "Non-trainable params: 60\n",
            "_________________________________________________________________\n",
            "Epoch 1/4\n",
            "300/300 [==============================] - 216s 716ms/step - loss: 0.6758 - accuracy: 0.7776\n",
            "Epoch 2/4\n",
            "300/300 [==============================] - 209s 697ms/step - loss: 0.0698 - accuracy: 0.9787\n",
            "Epoch 3/4\n",
            "300/300 [==============================] - 210s 701ms/step - loss: 0.0455 - accuracy: 0.9848\n",
            "Epoch 4/4\n",
            "300/300 [==============================] - 210s 702ms/step - loss: 0.0344 - accuracy: 0.9892\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "################ CLASSIFICATION REPORT ################\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00       980\n",
            "           1       1.00      0.99      1.00      1135\n",
            "           2       0.99      0.99      0.99      1032\n",
            "           3       1.00      0.98      0.99      1010\n",
            "           4       0.99      1.00      0.99       982\n",
            "           5       0.99      0.99      0.99       892\n",
            "           6       0.99      0.99      0.99       958\n",
            "           7       0.99      1.00      0.99      1028\n",
            "           8       0.98      1.00      0.99       974\n",
            "           9       1.00      0.98      0.99      1009\n",
            "\n",
            "    accuracy                           0.99     10000\n",
            "   macro avg       0.99      0.99      0.99     10000\n",
            "weighted avg       0.99      0.99      0.99     10000\n",
            " \n",
            "\n",
            "\n",
            "################ CONFUSION MATRIX ################\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEIu4ioAZVjy",
        "outputId": "7accbde6-cc4e-43b7-828d-d0911a2a7059"
      },
      "source": [
        "scores = model.evaluate(x_train, y_train, verbose=1)\r\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\r\n",
        "\r\n",
        "# serialize model to JSON\r\n",
        "model_json = model.to_json()\r\n",
        "with open(\"model.json\", \"w\") as json_file:\r\n",
        "    json_file.write(model_json)\r\n",
        "# serialize weights to HDF5\r\n",
        "model.save_weights(\"model.h5\")\r\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1875/1875 [==============================] - 62s 33ms/step - loss: 0.0193 - accuracy: 0.9940\n",
            "accuracy: 99.40%\n",
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeLQzwBHaSzL",
        "outputId": "9bdf4158-6567-4bb1-caf7-f056ab605e4b"
      },
      "source": [
        "from keras.models import model_from_json\r\n",
        "\r\n",
        "# load json and create model\r\n",
        "json_file = open('model.json', 'r')\r\n",
        "loaded_model_json = json_file.read()\r\n",
        "json_file.close()\r\n",
        "loaded_model = model_from_json(loaded_model_json)\r\n",
        "# load weights into new model\r\n",
        "loaded_model.load_weights(\"model.h5\")\r\n",
        "print(\"Loaded model from disk\")\r\n",
        "\r\n",
        "# evaluate loaded model on test data\r\n",
        "loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "#model.compile(optimizer='adam', loss='categorical_crossentropy' , metrics=['accuracy'])\r\n",
        "score = loaded_model.evaluate(x_train, y_train, verbose=1)\r\n",
        "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model from disk\n",
            "1875/1875 [==============================] - 61s 32ms/step - loss: 0.0199 - accuracy: 0.9939\n",
            "accuracy: 99.40%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYqJK6YsxeTp",
        "outputId": "5ce32bde-24a2-428f-b3f2-7984e471f792"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import keras\r\n",
        "from keras.datasets import mnist\r\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, BatchNormalization, Dropout, Dense\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.optimizers import Adam\r\n",
        "from keras.losses import SparseCategoricalCrossentropy\r\n",
        "from keras import utils\r\n",
        "\r\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n",
        "\r\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data() \r\n",
        "print(X_train.shape)\r\n",
        "x_train = x_train.reshape(-1,28,28,1)\r\n",
        "x_test = x_test.reshape(-1,28,28,1)\r\n",
        "y_train = utils.to_categorical(y_train)\r\n",
        "y_test = utils.to_categorical(y_test)\r\n",
        "\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(Conv2D(32, (3,3), input_shape=(28,28,1),activation='relu')) \r\n",
        "model.add(Conv2D(32, (3,3), activation='relu'))\r\n",
        "model.add(Conv2D(32, (3,3), activation='relu'))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(MaxPooling2D())\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(units=128,activation = 'relu'))\r\n",
        "model.add(Dense(units = 64, activation = 'relu'))\r\n",
        "model.add(Dense(units = 32, activation = 'relu'))\r\n",
        "model.add(Dense(units = 10, activation='sigmoid'))\r\n",
        "print(model.summary())\r\n",
        "\r\n",
        "op=keras.optimizers.Adam(lr=0.001)\r\n",
        "\r\n",
        "model.compile(optimizer=op, loss='CategoricalCrossentropy',  metrics=['accuracy'])\r\n",
        "history=model.fit(x_train, y_train, epochs=1, batch_size=200, verbose=1)\r\n",
        "\r\n",
        "print('Loss Funciton: ', model.evaluate(x_train,y_train, verbose=1)[0])\r\n",
        "print('Accuracy', model.evaluate(x_train,y_train, verbose=1)[1])\r\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_38 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_39 (Conv2D)           (None, 24, 24, 32)        9248      \n",
            "_________________________________________________________________\n",
            "conv2d_40 (Conv2D)           (None, 22, 22, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 22, 22, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 11, 11, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3872)              0         \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 128)               495744    \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 10)                330       \n",
            "=================================================================\n",
            "Total params: 525,354\n",
            "Trainable params: 525,290\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n",
            "None\n",
            "300/300 [==============================] - 145s 480ms/step - loss: 0.3154 - accuracy: 0.9020\n",
            "1875/1875 [==============================] - 40s 21ms/step - loss: 0.0429 - accuracy: 0.9864\n",
            "Loss Funciton:  0.04286322370171547\n",
            "1875/1875 [==============================] - 40s 21ms/step - loss: 0.0429 - accuracy: 0.9864\n",
            "Accuracy 0.986383318901062\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlCuh2xI4roQ",
        "outputId": "90d3ebda-4fe2-4d6b-b230-126b751eb519"
      },
      "source": [
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.models import model_from_json\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import pandas as pd\r\n",
        "# fix random seed for reproducibility\r\n",
        "numpy.random.seed(7)\r\n",
        "# load pima indians dataset\r\n",
        "dataset = pd.read_csv('https://raw.githubusercontent.com/azhar2ds/DataSets/master/pima.csv')\r\n",
        "#dataset = np.loadtxt(\"/content/pima-indians-diabetes.data.csv\", delimiter=\",\")\r\n",
        "\r\n",
        "X = dataset.iloc[:,0:8]\r\n",
        "Y = dataset.iloc[:,8]\r\n",
        "Y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrqdpcNwysLm",
        "outputId": "9751214b-db12-4c09-9869-760c3150da9a"
      },
      "source": [
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.models import model_from_json\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import pandas as pd\r\n",
        "# fix random seed for reproducibility\r\n",
        "numpy.random.seed(7)\r\n",
        "# load pima indians dataset\r\n",
        "#dataset = pd.read_csv('https://raw.githubusercontent.com/azhar2ds/DataSets/master/pima.csv')\r\n",
        "dataset = np.loadtxt(\"/content/pima-indians-diabetes.data.csv\", delimiter=\",\")\r\n",
        "\r\n",
        "X = dataset[:,0:8]\r\n",
        "Y = dataset[:,8]\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))\r\n",
        "model.add(Dense(8, activation='relu'))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "# Compile model\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "# Fit the model\r\n",
        "model.fit(X, Y, epochs=150, batch_size=10)\r\n",
        "\r\n",
        "# evaluate the model\r\n",
        "scores = model.evaluate(X, Y, verbose=0)\r\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "77/77 [==============================] - 1s 1ms/step - loss: 2.7036 - accuracy: 0.6450\n",
            "Epoch 2/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.9444 - accuracy: 0.5954\n",
            "Epoch 3/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6802 - accuracy: 0.5919\n",
            "Epoch 4/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6457 - accuracy: 0.6211\n",
            "Epoch 5/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6332 - accuracy: 0.6099\n",
            "Epoch 6/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6338 - accuracy: 0.6562\n",
            "Epoch 7/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6131 - accuracy: 0.6805\n",
            "Epoch 8/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6244 - accuracy: 0.6608\n",
            "Epoch 9/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6157 - accuracy: 0.6723\n",
            "Epoch 10/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5841 - accuracy: 0.6851\n",
            "Epoch 11/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6126 - accuracy: 0.6731\n",
            "Epoch 12/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5997 - accuracy: 0.6660\n",
            "Epoch 13/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6088 - accuracy: 0.6591\n",
            "Epoch 14/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5962 - accuracy: 0.6689\n",
            "Epoch 15/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5712 - accuracy: 0.7031\n",
            "Epoch 16/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5971 - accuracy: 0.6636\n",
            "Epoch 17/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5953 - accuracy: 0.6651\n",
            "Epoch 18/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5749 - accuracy: 0.6784\n",
            "Epoch 19/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5412 - accuracy: 0.7211\n",
            "Epoch 20/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5950 - accuracy: 0.6730\n",
            "Epoch 21/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5937 - accuracy: 0.6785\n",
            "Epoch 22/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5976 - accuracy: 0.6566\n",
            "Epoch 23/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5824 - accuracy: 0.6861\n",
            "Epoch 24/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5894 - accuracy: 0.6754\n",
            "Epoch 25/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5891 - accuracy: 0.6824\n",
            "Epoch 26/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5469 - accuracy: 0.6957\n",
            "Epoch 27/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6117 - accuracy: 0.6479\n",
            "Epoch 28/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5526 - accuracy: 0.7137\n",
            "Epoch 29/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5662 - accuracy: 0.6801\n",
            "Epoch 30/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5791 - accuracy: 0.6650\n",
            "Epoch 31/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5768 - accuracy: 0.6929\n",
            "Epoch 32/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6367 - accuracy: 0.6164\n",
            "Epoch 33/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5896 - accuracy: 0.6774\n",
            "Epoch 34/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5935 - accuracy: 0.6589\n",
            "Epoch 35/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5628 - accuracy: 0.6902\n",
            "Epoch 36/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5780 - accuracy: 0.7115\n",
            "Epoch 37/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5625 - accuracy: 0.7159\n",
            "Epoch 38/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5810 - accuracy: 0.6682\n",
            "Epoch 39/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5785 - accuracy: 0.6927\n",
            "Epoch 40/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5630 - accuracy: 0.7110\n",
            "Epoch 41/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5845 - accuracy: 0.6901\n",
            "Epoch 42/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5589 - accuracy: 0.6920\n",
            "Epoch 43/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5686 - accuracy: 0.6823\n",
            "Epoch 44/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5537 - accuracy: 0.6977\n",
            "Epoch 45/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5385 - accuracy: 0.7027\n",
            "Epoch 46/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5257 - accuracy: 0.7162\n",
            "Epoch 47/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5581 - accuracy: 0.6732\n",
            "Epoch 48/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5899 - accuracy: 0.6619\n",
            "Epoch 49/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5510 - accuracy: 0.6831\n",
            "Epoch 50/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5897 - accuracy: 0.6727\n",
            "Epoch 51/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5452 - accuracy: 0.6890\n",
            "Epoch 52/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5532 - accuracy: 0.6989\n",
            "Epoch 53/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5820 - accuracy: 0.6674\n",
            "Epoch 54/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5701 - accuracy: 0.6864\n",
            "Epoch 55/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5657 - accuracy: 0.6933\n",
            "Epoch 56/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5474 - accuracy: 0.6984\n",
            "Epoch 57/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5540 - accuracy: 0.6998\n",
            "Epoch 58/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5576 - accuracy: 0.6842\n",
            "Epoch 59/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5604 - accuracy: 0.7042\n",
            "Epoch 60/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5634 - accuracy: 0.6825\n",
            "Epoch 61/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5363 - accuracy: 0.7006\n",
            "Epoch 62/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5666 - accuracy: 0.6924\n",
            "Epoch 63/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5553 - accuracy: 0.6846\n",
            "Epoch 64/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5522 - accuracy: 0.7103\n",
            "Epoch 65/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5353 - accuracy: 0.7050\n",
            "Epoch 66/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5765 - accuracy: 0.6705\n",
            "Epoch 67/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5505 - accuracy: 0.6880\n",
            "Epoch 68/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5689 - accuracy: 0.6885\n",
            "Epoch 69/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5592 - accuracy: 0.6886\n",
            "Epoch 70/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5512 - accuracy: 0.6902\n",
            "Epoch 71/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5290 - accuracy: 0.7168\n",
            "Epoch 72/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5554 - accuracy: 0.6843\n",
            "Epoch 73/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5825 - accuracy: 0.6585\n",
            "Epoch 74/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5389 - accuracy: 0.6950\n",
            "Epoch 75/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5433 - accuracy: 0.6951\n",
            "Epoch 76/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5567 - accuracy: 0.6950\n",
            "Epoch 77/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5637 - accuracy: 0.6550\n",
            "Epoch 78/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5552 - accuracy: 0.6938\n",
            "Epoch 79/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5587 - accuracy: 0.6930\n",
            "Epoch 80/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5317 - accuracy: 0.6950\n",
            "Epoch 81/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5357 - accuracy: 0.6870\n",
            "Epoch 82/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5452 - accuracy: 0.6915\n",
            "Epoch 83/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5655 - accuracy: 0.6850\n",
            "Epoch 84/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5419 - accuracy: 0.6956\n",
            "Epoch 85/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5626 - accuracy: 0.6711\n",
            "Epoch 86/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5332 - accuracy: 0.7180\n",
            "Epoch 87/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5152 - accuracy: 0.7110\n",
            "Epoch 88/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5709 - accuracy: 0.6732\n",
            "Epoch 89/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5398 - accuracy: 0.6930\n",
            "Epoch 90/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5458 - accuracy: 0.7035\n",
            "Epoch 91/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5789 - accuracy: 0.6990\n",
            "Epoch 92/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5827 - accuracy: 0.6632\n",
            "Epoch 93/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5395 - accuracy: 0.7200\n",
            "Epoch 94/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5628 - accuracy: 0.6668\n",
            "Epoch 95/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5389 - accuracy: 0.7096\n",
            "Epoch 96/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5217 - accuracy: 0.7049\n",
            "Epoch 97/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5456 - accuracy: 0.6938\n",
            "Epoch 98/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5573 - accuracy: 0.6810\n",
            "Epoch 99/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5634 - accuracy: 0.6821\n",
            "Epoch 100/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5544 - accuracy: 0.6768\n",
            "Epoch 101/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5416 - accuracy: 0.6830\n",
            "Epoch 102/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5456 - accuracy: 0.6843\n",
            "Epoch 103/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5398 - accuracy: 0.6755\n",
            "Epoch 104/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5581 - accuracy: 0.6862\n",
            "Epoch 105/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5548 - accuracy: 0.7000\n",
            "Epoch 106/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5250 - accuracy: 0.7047\n",
            "Epoch 107/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5461 - accuracy: 0.7024\n",
            "Epoch 108/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5281 - accuracy: 0.7306\n",
            "Epoch 109/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5347 - accuracy: 0.7115\n",
            "Epoch 110/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5370 - accuracy: 0.7178\n",
            "Epoch 111/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5294 - accuracy: 0.7188\n",
            "Epoch 112/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5270 - accuracy: 0.7080\n",
            "Epoch 113/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5166 - accuracy: 0.7052\n",
            "Epoch 114/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5642 - accuracy: 0.6795\n",
            "Epoch 115/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5577 - accuracy: 0.6748\n",
            "Epoch 116/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5566 - accuracy: 0.6895\n",
            "Epoch 117/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5320 - accuracy: 0.7117\n",
            "Epoch 118/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5415 - accuracy: 0.7122\n",
            "Epoch 119/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5267 - accuracy: 0.7011\n",
            "Epoch 120/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5449 - accuracy: 0.7053\n",
            "Epoch 121/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5639 - accuracy: 0.6970\n",
            "Epoch 122/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5500 - accuracy: 0.6903\n",
            "Epoch 123/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5445 - accuracy: 0.6954\n",
            "Epoch 124/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5182 - accuracy: 0.6837\n",
            "Epoch 125/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5261 - accuracy: 0.7190\n",
            "Epoch 126/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5220 - accuracy: 0.7175\n",
            "Epoch 127/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5345 - accuracy: 0.6995\n",
            "Epoch 128/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5118 - accuracy: 0.7097\n",
            "Epoch 129/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5275 - accuracy: 0.6806\n",
            "Epoch 130/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5258 - accuracy: 0.6997\n",
            "Epoch 131/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5863 - accuracy: 0.6482\n",
            "Epoch 132/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5406 - accuracy: 0.6834\n",
            "Epoch 133/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5556 - accuracy: 0.6705\n",
            "Epoch 134/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5536 - accuracy: 0.6703\n",
            "Epoch 135/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5529 - accuracy: 0.6794\n",
            "Epoch 136/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5241 - accuracy: 0.6904\n",
            "Epoch 137/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5236 - accuracy: 0.7012\n",
            "Epoch 138/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5492 - accuracy: 0.6862\n",
            "Epoch 139/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5371 - accuracy: 0.6977\n",
            "Epoch 140/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5256 - accuracy: 0.6927\n",
            "Epoch 141/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5234 - accuracy: 0.7233\n",
            "Epoch 142/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5285 - accuracy: 0.7041\n",
            "Epoch 143/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5457 - accuracy: 0.6958\n",
            "Epoch 144/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5362 - accuracy: 0.6730\n",
            "Epoch 145/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5447 - accuracy: 0.6861\n",
            "Epoch 146/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5399 - accuracy: 0.6843\n",
            "Epoch 147/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5335 - accuracy: 0.6733\n",
            "Epoch 148/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5561 - accuracy: 0.6849\n",
            "Epoch 149/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5341 - accuracy: 0.6970\n",
            "Epoch 150/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5057 - accuracy: 0.6870\n",
            "accuracy: 70.44%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXgdZQFJxg1J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "outputId": "e9ca6589-de06-4ecf-f202-44d406584256"
      },
      "source": [
        "\r\n",
        "\r\n",
        "# importing the pandas library \r\n",
        "import pandas as pd \r\n",
        "from sklearn import datasets\r\n",
        "from collections import Counter\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "import keras\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "\r\n",
        "iris=datasets.load_iris()\r\n",
        "X=pd.DataFrame(iris.data)\r\n",
        "Y=list(iris.target)\r\n",
        "print('X Shape: ',X.shape)\r\n",
        "\r\n",
        "\r\n",
        "print(Counter(Y).keys())\r\n",
        "print(Counter(Y).values())\r\n",
        "print('Unique Values:',np.unique(Y, return_counts=True)[0])\r\n",
        "print('TotalValues: ', np.unique(Y, return_counts=True)[1])\r\n",
        "\r\n",
        "\r\n",
        "model=Sequential()\r\n",
        "model.add(Dense(50, input_shape= [4], activation='relu'))\r\n",
        "model.add(Dense(10, activation='relu'))\r\n",
        "model.add(Dense(3, activation='softmax'))\r\n",
        "print(model.summary())\r\n",
        "\r\n",
        "model.compile(optimizer='adam', loss='CategoricalCrossentropy',\r\n",
        "              metrics=['accuracy'])\r\n",
        "\r\n",
        "model.fit(X,Y, epochs=10,  batch_size=10, verbose=1)\r\n",
        "\r\n",
        "Y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X Shape:  (150, 4)\n",
            "dict_keys([0, 1, 2])\n",
            "dict_values([50, 50, 50])\n",
            "Unique Values: [0 1 2]\n",
            "TotalValues:  [50 50 50]\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_29 (Dense)             (None, 50)                250       \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 10)                510       \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 3)                 33        \n",
            "=================================================================\n",
            "Total params: 793\n",
            "Trainable params: 793\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-a0942dffa5db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1097\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m     self._adapter = adapter_cls(\n\u001b[1;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m         \"input: {}, {}\".format(\n\u001b[0;32m--> 964\u001b[0;31m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[1;32m    965\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m     raise RuntimeError(\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, (<class 'list'> containing values of types {\"<class 'numpy.int64'>\"})"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MY7YhWLCykzx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4e24d21-0779-4781-cf5a-5772d1ff44f5"
      },
      "source": [
        "\r\n",
        "import keras\r\n",
        "from keras.layers import BatchNormalization\r\n",
        "# importing the pandas library \r\n",
        "import pandas as pd \r\n",
        "from sklearn import datasets\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "iris=datasets.load_iris()\r\n",
        "X=np.array(iris.data)\r\n",
        "Y=np.array(iris.target)\r\n",
        "x_train, x_test, y_train, y_test= train_test_split(X,Y,test_size=0.3, random_state=1)\r\n",
        "X.shape\r\n",
        "model=Sequential()\r\n",
        "model.add(Dense(500, input_shape= [4], activation='relu'))\r\n",
        "model.add(Dense(200, activation='relu'))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(Dense(100, activation='relu'))\r\n",
        "model.add(Dense(10, activation='relu'))\r\n",
        "model.add(Dense(1, activation='softmax'))\r\n",
        "print(model.summary())\r\n",
        "\r\n",
        "model.compile(optimizer=keras.optimizers.Adam(lr=0.001), loss='CategoricalCrossentropy', metrics=['accuracy'])\r\n",
        "\r\n",
        "history = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test), batch_size=20, verbose=1)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_48 (Dense)             (None, 500)               2500      \n",
            "_________________________________________________________________\n",
            "dense_49 (Dense)             (None, 200)               100200    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 200)               800       \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 10)                1010      \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 124,621\n",
            "Trainable params: 124,221\n",
            "Non-trainable params: 400\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "1/6 [====>.........................] - ETA: 3s - loss: 0.0000e+00 - accuracy: 0.2500WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fd0a70e6400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "6/6 [==============================] - 1s 64ms/step - loss: 0.0000e+00 - accuracy: 0.3031 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 2/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.3052 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 3/100\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 0.2839 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 4/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.2880 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 5/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.3337 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 6/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.3147 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 7/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.2879 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 8/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.3109 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 9/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.3070 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 10/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.3081 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 11/100\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.3435 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 12/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.3576 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 13/100\n",
            "6/6 [==============================] - 0s 53ms/step - loss: 0.0000e+00 - accuracy: 0.2770 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 14/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.2815 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 15/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.2948 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 16/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.2766 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 17/100\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - accuracy: 0.2918 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 18/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.3054 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 19/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.3281 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 20/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.3440 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 21/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.3201 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 22/100\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.3266 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 23/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.2753 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 24/100\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 0.2773 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 25/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.3079 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 26/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.3130 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 27/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.2871 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 28/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.2891 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 29/100\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 0.2754 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 30/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.3108 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 31/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.2811 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 32/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.2761 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 33/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.2936 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 34/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.3143 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 35/100\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.3770 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 36/100\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.2962 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 37/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.2696 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 38/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.2964 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 39/100\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.3718 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 40/100\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.2877 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 41/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.3242 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 42/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.2906 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 43/100\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.3133 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 44/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.3210 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 45/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.3270 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 46/100\n",
            "6/6 [==============================] - 0s 44ms/step - loss: 0.0000e+00 - accuracy: 0.3258 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 47/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.2604 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 48/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.2959 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 49/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.2891 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 50/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.3251 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 51/100\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.2996 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 52/100\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - accuracy: 0.3256 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 53/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.3215 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 54/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.2956 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 55/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.2996 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 56/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.3254 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 57/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.3516 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 58/100\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.2927 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 59/100\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.2508 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 60/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.3602 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 61/100\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.3480 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 62/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.3022 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 63/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.3329 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 64/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.3377 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 65/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.2870 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 66/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.3308 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 67/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.3053 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 68/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.3216 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 69/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.2677 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 70/100\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.3147 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 71/100\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.2883 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 72/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.2998 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 73/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.3105 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 74/100\n",
            "6/6 [==============================] - 0s 42ms/step - loss: 0.0000e+00 - accuracy: 0.3367 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 75/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.3329 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 76/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.2815 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 77/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.2805 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 78/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.3034 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 79/100\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.3297 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 80/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.3227 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 81/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.3365 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 82/100\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.3177 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 83/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.2666 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 84/100\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 0.3168 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 85/100\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.3198 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 86/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.3221 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 87/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.2803 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 88/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.2984 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 89/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.3109 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 90/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.3117 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 91/100\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.2983 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 92/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.2792 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 93/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.3166 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 94/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.2971 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 95/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.2970 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 96/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.2940 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 97/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.3041 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 98/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.3183 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 99/100\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.2960 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n",
            "Epoch 100/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.2746 - val_loss: 0.0000e+00 - val_accuracy: 0.4000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNtoyzoG6tOn"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "from sklearn.datasets import load_iris\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import OneHotEncoder\r\n",
        "\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xweRzQGYJNo",
        "outputId": "5239521f-22b0-42d4-8276-4b4cc24df2c1"
      },
      "source": [
        "iris_data = load_iris() # load the iris dataset\r\n",
        "\r\n",
        "print(type(iris_data.data))\r\n",
        "print('iris_data_target SHAPE: ',iris_data.target.shape)\r\n",
        "x = iris_data.data\r\n",
        "y = iris_data.target.reshape(-1, 1) # Convert data to a single column\r\n",
        "print('Actual Y shape value: ',y.shape)\r\n",
        "print('Values of Y:',y[:4])\r\n",
        "\r\n",
        "print('Y Value after shape:  ',y.shape)\r\n",
        "# # One Hot encode the class labels\r\n",
        "encoder = OneHotEncoder(sparse=False)\r\n",
        "y = encoder.fit_transform(y_)\r\n",
        "print('Values after Encoding: ',y[:4])\r\n",
        "\r\n",
        "# # Split the data for training and testing\r\n",
        "# train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.20)\r\n",
        "\r\n",
        "# # Build the model\r\n",
        "\r\n",
        "# model = Sequential()\r\n",
        "\r\n",
        "# model.add(Dense(10, input_shape=(4,), activation='relu', name='fc1'))\r\n",
        "# model.add(Dense(10, activation='relu', name='fc2'))\r\n",
        "# model.add(Dense(3, activation='softmax', name='output'))\r\n",
        "\r\n",
        "# # Adam optimizer with learning rate of 0.001\r\n",
        "# optimizer = Adam(lr=0.001)\r\n",
        "# model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\r\n",
        "\r\n",
        "# print('Neural Network Model Summary: ')\r\n",
        "# print(model.summary())\r\n",
        "\r\n",
        "# # Train the model\r\n",
        "# model.fit(train_x, train_y, verbose=2, batch_size=5, epochs=200)\r\n",
        "\r\n",
        "# # Test on unseen data\r\n",
        "\r\n",
        "# results = model.evaluate(test_x, test_y)\r\n",
        "\r\n",
        "# print('Final test set loss: {:4f}'.format(results[0]))\r\n",
        "# print('Final test set accuracy: {:4f}'.format(results[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "iris_data_target SHAPE:  (150,)\n",
            "Actual Y shape value:  (150, 1)\n",
            "Values of Y: [[0]\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "Y Value after shape:   (150, 1)\n",
            "Values after Encoding:  [[1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Pvt3BBT6zg5",
        "outputId": "96729add-b0b8-430f-abd3-0774a2a8a15a"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import keras\r\n",
        "from keras import utils\r\n",
        "from sklearn import datasets\r\n",
        "import numpy as np\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense \r\n",
        "\r\n",
        "d=datasets.load_iris()\r\n",
        "x=d.data\r\n",
        "y=d.target\r\n",
        "print(y.shape)\r\n",
        "print(x[:3])\r\n",
        "print(np.unique(y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150,)\n",
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]]\n",
            "[0 1 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7_MKFe5SkXq",
        "outputId": "2d6b7fd8-5704-47c2-b52b-0d02f30e63f7"
      },
      "source": [
        "d=datasets.load_iris()\r\n",
        "x=d.data\r\n",
        "y=d.target\r\n",
        "print(y.shape)\r\n",
        "print(x[:3])\r\n",
        "print(np.unique(y))\r\n",
        "\r\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2, random_state=0)\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(Dense(50, input_shape=[4], activation='relu'))\r\n",
        "model.add(Dense(40, activation='relu'))\r\n",
        "model.add(Dense(20,activation='relu'))\r\n",
        "model.add(Dense(3, activation='sigmoid'))\r\n",
        "\r\n",
        "model.compile(loss=\"SparseCategoricalCrossentropy\", optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\r\n",
        "model.summary()\r\n",
        "history = model.fit(x_train, y_train, epochs=100, batch_size=10, verbose=1)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "a=model.evaluate(x_train,y_train)\r\n",
        "print('Loss Function:',round(a[0],2)*100)\r\n",
        "print('Accuracy of Training:',round(a[1], 2)*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150,)\n",
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]]\n",
            "[0 1 2]\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 50)                250       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 40)                2040      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 20)                820       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 3)                 63        \n",
            "=================================================================\n",
            "Total params: 3,173\n",
            "Trainable params: 3,173\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "12/12 [==============================] - 1s 2ms/step - loss: 2.0111 - accuracy: 0.3334\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 1.0713 - accuracy: 0.4488\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.8796 - accuracy: 0.7688\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.7925 - accuracy: 0.6896\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.7128 - accuracy: 0.6803\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.5953 - accuracy: 0.7497\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.5732 - accuracy: 0.6686\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.5416 - accuracy: 0.8073\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.4902 - accuracy: 0.7899\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.4446 - accuracy: 0.9534\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.4185 - accuracy: 0.9211\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.4122 - accuracy: 0.8975\n",
            "Epoch 13/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.3930 - accuracy: 0.8841\n",
            "Epoch 14/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.3491 - accuracy: 0.9610\n",
            "Epoch 15/100\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.3219 - accuracy: 0.9499\n",
            "Epoch 16/100\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.3258 - accuracy: 0.9626\n",
            "Epoch 17/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.3216 - accuracy: 0.8883\n",
            "Epoch 18/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.2811 - accuracy: 0.9451\n",
            "Epoch 19/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.2457 - accuracy: 0.9834\n",
            "Epoch 20/100\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.2106 - accuracy: 0.9677\n",
            "Epoch 21/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.2332 - accuracy: 0.9420\n",
            "Epoch 22/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.2094 - accuracy: 0.9861\n",
            "Epoch 23/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1966 - accuracy: 0.9446\n",
            "Epoch 24/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1652 - accuracy: 0.9621\n",
            "Epoch 25/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1594 - accuracy: 0.9726\n",
            "Epoch 26/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1781 - accuracy: 0.9545\n",
            "Epoch 27/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1594 - accuracy: 0.9717\n",
            "Epoch 28/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1799 - accuracy: 0.9311\n",
            "Epoch 29/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1639 - accuracy: 0.9512\n",
            "Epoch 30/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1154 - accuracy: 0.9790\n",
            "Epoch 31/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1648 - accuracy: 0.9451\n",
            "Epoch 32/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1209 - accuracy: 0.9608\n",
            "Epoch 33/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1472 - accuracy: 0.9588\n",
            "Epoch 34/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1560 - accuracy: 0.9154\n",
            "Epoch 35/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1552 - accuracy: 0.9436\n",
            "Epoch 36/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1375 - accuracy: 0.9609\n",
            "Epoch 37/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1424 - accuracy: 0.9537\n",
            "Epoch 38/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1707 - accuracy: 0.9470\n",
            "Epoch 39/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0950 - accuracy: 0.9883\n",
            "Epoch 40/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1285 - accuracy: 0.9535\n",
            "Epoch 41/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1194 - accuracy: 0.9650\n",
            "Epoch 42/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1816 - accuracy: 0.8747\n",
            "Epoch 43/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1319 - accuracy: 0.9450\n",
            "Epoch 44/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1423 - accuracy: 0.9307\n",
            "Epoch 45/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1193 - accuracy: 0.9529\n",
            "Epoch 46/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0941 - accuracy: 0.9565\n",
            "Epoch 47/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1439 - accuracy: 0.9043\n",
            "Epoch 48/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1421 - accuracy: 0.9492\n",
            "Epoch 49/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1000 - accuracy: 0.9628\n",
            "Epoch 50/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0626 - accuracy: 0.9881\n",
            "Epoch 51/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0912 - accuracy: 0.9684\n",
            "Epoch 52/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1205 - accuracy: 0.9352\n",
            "Epoch 53/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0910 - accuracy: 0.9406\n",
            "Epoch 54/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1089 - accuracy: 0.9443\n",
            "Epoch 55/100\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0927 - accuracy: 0.9604\n",
            "Epoch 56/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0745 - accuracy: 0.9670\n",
            "Epoch 57/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0831 - accuracy: 0.9625\n",
            "Epoch 58/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0874 - accuracy: 0.9521\n",
            "Epoch 59/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0997 - accuracy: 0.9764\n",
            "Epoch 60/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1195 - accuracy: 0.9350\n",
            "Epoch 61/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1098 - accuracy: 0.9772\n",
            "Epoch 62/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0978 - accuracy: 0.9651\n",
            "Epoch 63/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0978 - accuracy: 0.9646\n",
            "Epoch 64/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1270 - accuracy: 0.9411\n",
            "Epoch 65/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0793 - accuracy: 0.9562\n",
            "Epoch 66/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0864 - accuracy: 0.9638\n",
            "Epoch 67/100\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0999 - accuracy: 0.9666\n",
            "Epoch 68/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0936 - accuracy: 0.9729\n",
            "Epoch 69/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1071 - accuracy: 0.9633\n",
            "Epoch 70/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0664 - accuracy: 0.9818\n",
            "Epoch 71/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0739 - accuracy: 0.9622\n",
            "Epoch 72/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0934 - accuracy: 0.9622\n",
            "Epoch 73/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1031 - accuracy: 0.9537\n",
            "Epoch 74/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0805 - accuracy: 0.9569\n",
            "Epoch 75/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0774 - accuracy: 0.9650\n",
            "Epoch 76/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0654 - accuracy: 0.9860\n",
            "Epoch 77/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0898 - accuracy: 0.9517\n",
            "Epoch 78/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0968 - accuracy: 0.9558\n",
            "Epoch 79/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0849 - accuracy: 0.9532\n",
            "Epoch 80/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0768 - accuracy: 0.9566\n",
            "Epoch 81/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0879 - accuracy: 0.9702\n",
            "Epoch 82/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1317 - accuracy: 0.9535\n",
            "Epoch 83/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1098 - accuracy: 0.9482\n",
            "Epoch 84/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0818 - accuracy: 0.9774\n",
            "Epoch 85/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0996 - accuracy: 0.9542\n",
            "Epoch 86/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1091 - accuracy: 0.9463\n",
            "Epoch 87/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0693 - accuracy: 0.9815\n",
            "Epoch 88/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1065 - accuracy: 0.9565\n",
            "Epoch 89/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0553 - accuracy: 0.9794\n",
            "Epoch 90/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0538 - accuracy: 0.9795\n",
            "Epoch 91/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0895 - accuracy: 0.9583\n",
            "Epoch 92/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0886 - accuracy: 0.9680\n",
            "Epoch 93/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0554 - accuracy: 0.9808\n",
            "Epoch 94/100\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0814 - accuracy: 0.9613\n",
            "Epoch 95/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1039 - accuracy: 0.9596\n",
            "Epoch 96/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.1624 - accuracy: 0.8857\n",
            "Epoch 97/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0551 - accuracy: 0.9687\n",
            "Epoch 98/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0586 - accuracy: 0.9681\n",
            "Epoch 99/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0422 - accuracy: 0.9858\n",
            "Epoch 100/100\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0864 - accuracy: 0.9625\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0909 - accuracy: 0.9667\n",
            "Loss Function: 9.0\n",
            "Accuracy of Training: 97.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq2YsXToVDO6",
        "outputId": "4da99a4a-ac69-40aa-d83f-18955eb772a5"
      },
      "source": [
        "training=model.evaluate(x_train,y_train)\r\n",
        "print('Training Loss Function:',round(training[0],2)*100)\r\n",
        "print('Accuracy of Training:',round(training[1], 2)*100)\r\n",
        "\r\n",
        "testing=model.evaluate(x_test,y_test)\r\n",
        "print('Testing Loss Function:',round(testing[0],2)*100)\r\n",
        "print('Accuracy of Testing:',round(testing[1], 2)*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0684 - accuracy: 0.9750\n",
            "Loss Function: 7.000000000000001\n",
            "Accuracy of Training: 98.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHimvF7tWPbs"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from pylab import rcParams\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import warnings\r\n",
        "from mlxtend.plotting import plot_decision_regions\r\n",
        "from matplotlib.colors import ListedColormap\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.datasets import make_circles, make_moons"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY34X-aklkmW"
      },
      "source": [
        "X, y = make_moons(n_samples=100, noise=0.2, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G0eDFRMlkuc"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SAbQLZYllHL",
        "outputId": "1040a250-66e4-4fe3-9e48-e2a4dbc517e4"
      },
      "source": [
        "model = Sequential()\r\n",
        "model.add(Dense(500, input_dim=2, activation='relu'))\r\n",
        "model.add(Dense(200, activation='relu'))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "model.compile(loss='BinaryCrossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "model.summary()\r\n",
        "history = model.fit(x_train, y_train, epochs=500, batch_size=10, validation_data=(x_test, y_test), verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_52 (Dense)             (None, 500)               1500      \n",
            "_________________________________________________________________\n",
            "dense_53 (Dense)             (None, 200)               100200    \n",
            "_________________________________________________________________\n",
            "dense_54 (Dense)             (None, 1)                 201       \n",
            "=================================================================\n",
            "Total params: 101,901\n",
            "Trainable params: 101,901\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/500\n",
            "8/8 [==============================] - 1s 28ms/step - loss: 0.6288 - accuracy: 0.7247 - val_loss: 0.4682 - val_accuracy: 0.9000\n",
            "Epoch 2/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4446 - accuracy: 0.7961 - val_loss: 0.3253 - val_accuracy: 0.9000\n",
            "Epoch 3/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.3228 - accuracy: 0.8605 - val_loss: 0.2807 - val_accuracy: 0.9500\n",
            "Epoch 4/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.3022 - accuracy: 0.8603 - val_loss: 0.2585 - val_accuracy: 0.9000\n",
            "Epoch 5/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2886 - accuracy: 0.8420 - val_loss: 0.2380 - val_accuracy: 0.9000\n",
            "Epoch 6/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2835 - accuracy: 0.8548 - val_loss: 0.2041 - val_accuracy: 0.9500\n",
            "Epoch 7/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3013 - accuracy: 0.8535 - val_loss: 0.2324 - val_accuracy: 0.9000\n",
            "Epoch 8/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.3184 - accuracy: 0.8283 - val_loss: 0.2472 - val_accuracy: 0.9000\n",
            "Epoch 9/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2953 - accuracy: 0.8579 - val_loss: 0.2191 - val_accuracy: 0.9000\n",
            "Epoch 10/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2465 - accuracy: 0.8703 - val_loss: 0.2042 - val_accuracy: 0.9000\n",
            "Epoch 11/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2686 - accuracy: 0.8588 - val_loss: 0.2517 - val_accuracy: 0.9000\n",
            "Epoch 12/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2845 - accuracy: 0.8486 - val_loss: 0.2008 - val_accuracy: 0.9000\n",
            "Epoch 13/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2856 - accuracy: 0.8169 - val_loss: 0.1856 - val_accuracy: 0.9500\n",
            "Epoch 14/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2371 - accuracy: 0.8873 - val_loss: 0.1974 - val_accuracy: 0.9000\n",
            "Epoch 15/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1877 - accuracy: 0.9045 - val_loss: 0.1732 - val_accuracy: 0.9500\n",
            "Epoch 16/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2135 - accuracy: 0.8864 - val_loss: 0.1960 - val_accuracy: 0.9000\n",
            "Epoch 17/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1770 - accuracy: 0.8817 - val_loss: 0.2393 - val_accuracy: 0.9000\n",
            "Epoch 18/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1677 - accuracy: 0.9334 - val_loss: 0.1436 - val_accuracy: 0.9500\n",
            "Epoch 19/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1534 - accuracy: 0.9303 - val_loss: 0.1538 - val_accuracy: 0.9500\n",
            "Epoch 20/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1924 - accuracy: 0.8946 - val_loss: 0.1597 - val_accuracy: 0.9500\n",
            "Epoch 21/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1782 - accuracy: 0.9261 - val_loss: 0.1652 - val_accuracy: 0.9000\n",
            "Epoch 22/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1435 - accuracy: 0.9226 - val_loss: 0.1230 - val_accuracy: 1.0000\n",
            "Epoch 23/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1208 - accuracy: 0.9379 - val_loss: 0.1250 - val_accuracy: 1.0000\n",
            "Epoch 24/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1331 - accuracy: 0.9308 - val_loss: 0.1191 - val_accuracy: 1.0000\n",
            "Epoch 25/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1013 - accuracy: 0.9593 - val_loss: 0.1171 - val_accuracy: 1.0000\n",
            "Epoch 26/500\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.1071 - accuracy: 0.9444 - val_loss: 0.0986 - val_accuracy: 1.0000\n",
            "Epoch 27/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1169 - accuracy: 0.9702 - val_loss: 0.1103 - val_accuracy: 1.0000\n",
            "Epoch 28/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0659 - accuracy: 0.9754 - val_loss: 0.0752 - val_accuracy: 1.0000\n",
            "Epoch 29/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0842 - accuracy: 0.9752 - val_loss: 0.0898 - val_accuracy: 1.0000\n",
            "Epoch 30/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0722 - accuracy: 0.9860 - val_loss: 0.0652 - val_accuracy: 1.0000\n",
            "Epoch 31/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0484 - accuracy: 0.9795 - val_loss: 0.0815 - val_accuracy: 1.0000\n",
            "Epoch 32/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0439 - accuracy: 0.9888 - val_loss: 0.0690 - val_accuracy: 1.0000\n",
            "Epoch 33/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0570 - accuracy: 0.9888 - val_loss: 0.0570 - val_accuracy: 1.0000\n",
            "Epoch 34/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0708 - accuracy: 0.9807 - val_loss: 0.0732 - val_accuracy: 1.0000\n",
            "Epoch 35/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0696 - accuracy: 0.9684 - val_loss: 0.0574 - val_accuracy: 1.0000\n",
            "Epoch 36/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0334 - accuracy: 0.9972 - val_loss: 0.0548 - val_accuracy: 1.0000\n",
            "Epoch 37/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0299 - accuracy: 0.9956 - val_loss: 0.0583 - val_accuracy: 1.0000\n",
            "Epoch 38/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0577 - accuracy: 0.9684 - val_loss: 0.0526 - val_accuracy: 1.0000\n",
            "Epoch 39/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0532 - accuracy: 1.0000 - val_loss: 0.0592 - val_accuracy: 1.0000\n",
            "Epoch 40/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0414 - accuracy: 0.9956 - val_loss: 0.0405 - val_accuracy: 1.0000\n",
            "Epoch 41/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0342 - accuracy: 0.9916 - val_loss: 0.0505 - val_accuracy: 1.0000\n",
            "Epoch 42/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0339 - accuracy: 1.0000 - val_loss: 0.0539 - val_accuracy: 1.0000\n",
            "Epoch 43/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0269 - accuracy: 1.0000 - val_loss: 0.0409 - val_accuracy: 1.0000\n",
            "Epoch 44/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0228 - accuracy: 0.9888 - val_loss: 0.0409 - val_accuracy: 1.0000\n",
            "Epoch 45/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0160 - accuracy: 0.9972 - val_loss: 0.0366 - val_accuracy: 1.0000\n",
            "Epoch 46/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0401 - accuracy: 1.0000 - val_loss: 0.0399 - val_accuracy: 1.0000\n",
            "Epoch 47/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0325 - accuracy: 1.0000 - val_loss: 0.0405 - val_accuracy: 1.0000\n",
            "Epoch 48/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0221 - accuracy: 0.9851 - val_loss: 0.0369 - val_accuracy: 1.0000\n",
            "Epoch 49/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0239 - accuracy: 1.0000 - val_loss: 0.0294 - val_accuracy: 1.0000\n",
            "Epoch 50/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0241 - accuracy: 1.0000 - val_loss: 0.0389 - val_accuracy: 1.0000\n",
            "Epoch 51/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0215 - accuracy: 1.0000 - val_loss: 0.0388 - val_accuracy: 1.0000\n",
            "Epoch 52/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0171 - accuracy: 1.0000 - val_loss: 0.0251 - val_accuracy: 1.0000\n",
            "Epoch 53/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0196 - accuracy: 1.0000 - val_loss: 0.0275 - val_accuracy: 1.0000\n",
            "Epoch 54/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.0252 - val_accuracy: 1.0000\n",
            "Epoch 55/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0170 - accuracy: 1.0000 - val_loss: 0.0287 - val_accuracy: 1.0000\n",
            "Epoch 56/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.0273 - val_accuracy: 1.0000\n",
            "Epoch 57/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 1.0000\n",
            "Epoch 58/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
            "Epoch 59/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.0215 - val_accuracy: 1.0000\n",
            "Epoch 60/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 1.0000\n",
            "Epoch 61/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.0204 - val_accuracy: 1.0000\n",
            "Epoch 62/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 1.0000\n",
            "Epoch 63/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.0242 - val_accuracy: 1.0000\n",
            "Epoch 64/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.0190 - val_accuracy: 1.0000\n",
            "Epoch 65/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.0211 - val_accuracy: 1.0000\n",
            "Epoch 66/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.0198 - val_accuracy: 1.0000\n",
            "Epoch 67/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.0182 - val_accuracy: 1.0000\n",
            "Epoch 68/500\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.0184 - val_accuracy: 1.0000\n",
            "Epoch 69/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.0191 - val_accuracy: 1.0000\n",
            "Epoch 70/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.0214 - val_accuracy: 1.0000\n",
            "Epoch 71/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 0.0175 - val_accuracy: 1.0000\n",
            "Epoch 72/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.0178 - val_accuracy: 1.0000\n",
            "Epoch 73/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0184 - val_accuracy: 1.0000\n",
            "Epoch 74/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.0157 - val_accuracy: 1.0000\n",
            "Epoch 75/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0168 - val_accuracy: 1.0000\n",
            "Epoch 76/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0153 - val_accuracy: 1.0000\n",
            "Epoch 77/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.0182 - val_accuracy: 1.0000\n",
            "Epoch 78/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.0167 - val_accuracy: 1.0000\n",
            "Epoch 79/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.0143 - val_accuracy: 1.0000\n",
            "Epoch 80/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.0138 - val_accuracy: 1.0000\n",
            "Epoch 81/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.0157 - val_accuracy: 1.0000\n",
            "Epoch 82/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0149 - val_accuracy: 1.0000\n",
            "Epoch 83/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0152 - val_accuracy: 1.0000\n",
            "Epoch 84/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.0147 - val_accuracy: 1.0000\n",
            "Epoch 85/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 1.0000\n",
            "Epoch 86/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0134 - val_accuracy: 1.0000\n",
            "Epoch 87/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0138 - val_accuracy: 1.0000\n",
            "Epoch 88/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.0128 - val_accuracy: 1.0000\n",
            "Epoch 89/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0155 - val_accuracy: 1.0000\n",
            "Epoch 90/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0143 - val_accuracy: 1.0000\n",
            "Epoch 91/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.0123 - val_accuracy: 1.0000\n",
            "Epoch 92/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.0141 - val_accuracy: 1.0000\n",
            "Epoch 93/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.0138 - val_accuracy: 1.0000\n",
            "Epoch 94/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0121 - val_accuracy: 1.0000\n",
            "Epoch 95/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0136 - val_accuracy: 1.0000\n",
            "Epoch 96/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0124 - val_accuracy: 1.0000\n",
            "Epoch 97/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0126 - val_accuracy: 1.0000\n",
            "Epoch 98/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0126 - val_accuracy: 1.0000\n",
            "Epoch 99/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.0128 - val_accuracy: 1.0000\n",
            "Epoch 100/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0127 - val_accuracy: 1.0000\n",
            "Epoch 101/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.0131 - val_accuracy: 1.0000\n",
            "Epoch 102/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0109 - val_accuracy: 1.0000\n",
            "Epoch 103/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
            "Epoch 104/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0132 - val_accuracy: 1.0000\n",
            "Epoch 105/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0125 - val_accuracy: 1.0000\n",
            "Epoch 106/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0103 - val_accuracy: 1.0000\n",
            "Epoch 107/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
            "Epoch 108/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0124 - val_accuracy: 1.0000\n",
            "Epoch 109/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 1.0000\n",
            "Epoch 110/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.0124 - val_accuracy: 1.0000\n",
            "Epoch 111/500\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0102 - val_accuracy: 1.0000\n",
            "Epoch 112/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0098 - val_accuracy: 1.0000\n",
            "Epoch 113/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0105 - val_accuracy: 1.0000\n",
            "Epoch 114/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0118 - val_accuracy: 1.0000\n",
            "Epoch 115/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0110 - val_accuracy: 1.0000\n",
            "Epoch 116/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0116 - val_accuracy: 1.0000\n",
            "Epoch 117/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0102 - val_accuracy: 1.0000\n",
            "Epoch 118/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0102 - val_accuracy: 1.0000\n",
            "Epoch 119/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0093 - val_accuracy: 1.0000\n",
            "Epoch 120/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0098 - val_accuracy: 1.0000\n",
            "Epoch 121/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0111 - val_accuracy: 1.0000\n",
            "Epoch 122/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
            "Epoch 123/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0098 - val_accuracy: 1.0000\n",
            "Epoch 124/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0113 - val_accuracy: 1.0000\n",
            "Epoch 125/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
            "Epoch 126/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0092 - val_accuracy: 1.0000\n",
            "Epoch 127/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0094 - val_accuracy: 1.0000\n",
            "Epoch 128/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0090 - val_accuracy: 1.0000\n",
            "Epoch 129/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0093 - val_accuracy: 1.0000\n",
            "Epoch 130/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
            "Epoch 131/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0098 - val_accuracy: 1.0000\n",
            "Epoch 132/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0093 - val_accuracy: 1.0000\n",
            "Epoch 133/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0096 - val_accuracy: 1.0000\n",
            "Epoch 134/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0103 - val_accuracy: 1.0000\n",
            "Epoch 135/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
            "Epoch 136/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
            "Epoch 137/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0091 - val_accuracy: 1.0000\n",
            "Epoch 138/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0099 - val_accuracy: 1.0000\n",
            "Epoch 139/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0096 - val_accuracy: 1.0000\n",
            "Epoch 140/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0093 - val_accuracy: 1.0000\n",
            "Epoch 141/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0091 - val_accuracy: 1.0000\n",
            "Epoch 142/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0081 - val_accuracy: 1.0000\n",
            "Epoch 143/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0084 - val_accuracy: 1.0000\n",
            "Epoch 144/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0085 - val_accuracy: 1.0000\n",
            "Epoch 145/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0092 - val_accuracy: 1.0000\n",
            "Epoch 146/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 8.7307e-04 - accuracy: 1.0000 - val_loss: 0.0090 - val_accuracy: 1.0000\n",
            "Epoch 147/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 9.1610e-04 - accuracy: 1.0000 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
            "Epoch 148/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0084 - val_accuracy: 1.0000\n",
            "Epoch 149/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
            "Epoch 150/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 7.2927e-04 - accuracy: 1.0000 - val_loss: 0.0081 - val_accuracy: 1.0000\n",
            "Epoch 151/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0092 - val_accuracy: 1.0000\n",
            "Epoch 152/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 8.0794e-04 - accuracy: 1.0000 - val_loss: 0.0084 - val_accuracy: 1.0000\n",
            "Epoch 153/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 8.5603e-04 - accuracy: 1.0000 - val_loss: 0.0078 - val_accuracy: 1.0000\n",
            "Epoch 154/500\n",
            "8/8 [==============================] - 0s 28ms/step - loss: 8.1517e-04 - accuracy: 1.0000 - val_loss: 0.0080 - val_accuracy: 1.0000\n",
            "Epoch 155/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 7.2360e-04 - accuracy: 1.0000 - val_loss: 0.0076 - val_accuracy: 1.0000\n",
            "Epoch 156/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0079 - val_accuracy: 1.0000\n",
            "Epoch 157/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
            "Epoch 158/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 8.9494e-04 - accuracy: 1.0000 - val_loss: 0.0075 - val_accuracy: 1.0000\n",
            "Epoch 159/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 9.4363e-04 - accuracy: 1.0000 - val_loss: 0.0073 - val_accuracy: 1.0000\n",
            "Epoch 160/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 8.4430e-04 - accuracy: 1.0000 - val_loss: 0.0081 - val_accuracy: 1.0000\n",
            "Epoch 161/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.7223e-04 - accuracy: 1.0000 - val_loss: 0.0080 - val_accuracy: 1.0000\n",
            "Epoch 162/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0078 - val_accuracy: 1.0000\n",
            "Epoch 163/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.4523e-04 - accuracy: 1.0000 - val_loss: 0.0075 - val_accuracy: 1.0000\n",
            "Epoch 164/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 9.7660e-04 - accuracy: 1.0000 - val_loss: 0.0078 - val_accuracy: 1.0000\n",
            "Epoch 165/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 6.8058e-04 - accuracy: 1.0000 - val_loss: 0.0075 - val_accuracy: 1.0000\n",
            "Epoch 166/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 8.6411e-04 - accuracy: 1.0000 - val_loss: 0.0071 - val_accuracy: 1.0000\n",
            "Epoch 167/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 8.9058e-04 - accuracy: 1.0000 - val_loss: 0.0078 - val_accuracy: 1.0000\n",
            "Epoch 168/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 7.7276e-04 - accuracy: 1.0000 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
            "Epoch 169/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 9.0049e-04 - accuracy: 1.0000 - val_loss: 0.0074 - val_accuracy: 1.0000\n",
            "Epoch 170/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 7.3841e-04 - accuracy: 1.0000 - val_loss: 0.0073 - val_accuracy: 1.0000\n",
            "Epoch 171/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 6.0436e-04 - accuracy: 1.0000 - val_loss: 0.0071 - val_accuracy: 1.0000\n",
            "Epoch 172/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 6.8009e-04 - accuracy: 1.0000 - val_loss: 0.0072 - val_accuracy: 1.0000\n",
            "Epoch 173/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 9.2859e-04 - accuracy: 1.0000 - val_loss: 0.0077 - val_accuracy: 1.0000\n",
            "Epoch 174/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0075 - val_accuracy: 1.0000\n",
            "Epoch 175/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 6.5779e-04 - accuracy: 1.0000 - val_loss: 0.0074 - val_accuracy: 1.0000\n",
            "Epoch 176/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 6.9221e-04 - accuracy: 1.0000 - val_loss: 0.0070 - val_accuracy: 1.0000\n",
            "Epoch 177/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.4665e-04 - accuracy: 1.0000 - val_loss: 0.0066 - val_accuracy: 1.0000\n",
            "Epoch 178/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 8.3864e-04 - accuracy: 1.0000 - val_loss: 0.0066 - val_accuracy: 1.0000\n",
            "Epoch 179/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0075 - val_accuracy: 1.0000\n",
            "Epoch 180/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.8597e-04 - accuracy: 1.0000 - val_loss: 0.0078 - val_accuracy: 1.0000\n",
            "Epoch 181/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 8.2657e-04 - accuracy: 1.0000 - val_loss: 0.0070 - val_accuracy: 1.0000\n",
            "Epoch 182/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.8969e-04 - accuracy: 1.0000 - val_loss: 0.0067 - val_accuracy: 1.0000\n",
            "Epoch 183/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 6.7394e-04 - accuracy: 1.0000 - val_loss: 0.0072 - val_accuracy: 1.0000\n",
            "Epoch 184/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 6.4509e-04 - accuracy: 1.0000 - val_loss: 0.0068 - val_accuracy: 1.0000\n",
            "Epoch 185/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 6.3693e-04 - accuracy: 1.0000 - val_loss: 0.0066 - val_accuracy: 1.0000\n",
            "Epoch 186/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 8.1677e-04 - accuracy: 1.0000 - val_loss: 0.0069 - val_accuracy: 1.0000\n",
            "Epoch 187/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 9.4445e-04 - accuracy: 1.0000 - val_loss: 0.0068 - val_accuracy: 1.0000\n",
            "Epoch 188/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 6.7745e-04 - accuracy: 1.0000 - val_loss: 0.0063 - val_accuracy: 1.0000\n",
            "Epoch 189/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 7.2657e-04 - accuracy: 1.0000 - val_loss: 0.0071 - val_accuracy: 1.0000\n",
            "Epoch 190/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.2364e-04 - accuracy: 1.0000 - val_loss: 0.0069 - val_accuracy: 1.0000\n",
            "Epoch 191/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 8.6306e-04 - accuracy: 1.0000 - val_loss: 0.0066 - val_accuracy: 1.0000\n",
            "Epoch 192/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 4.7753e-04 - accuracy: 1.0000 - val_loss: 0.0066 - val_accuracy: 1.0000\n",
            "Epoch 193/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 4.2921e-04 - accuracy: 1.0000 - val_loss: 0.0062 - val_accuracy: 1.0000\n",
            "Epoch 194/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 8.9349e-04 - accuracy: 1.0000 - val_loss: 0.0065 - val_accuracy: 1.0000\n",
            "Epoch 195/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 6.4660e-04 - accuracy: 1.0000 - val_loss: 0.0063 - val_accuracy: 1.0000\n",
            "Epoch 196/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 6.2779e-04 - accuracy: 1.0000 - val_loss: 0.0065 - val_accuracy: 1.0000\n",
            "Epoch 197/500\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 5.3226e-04 - accuracy: 1.0000 - val_loss: 0.0066 - val_accuracy: 1.0000\n",
            "Epoch 198/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.8618e-04 - accuracy: 1.0000 - val_loss: 0.0066 - val_accuracy: 1.0000\n",
            "Epoch 199/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 6.9679e-04 - accuracy: 1.0000 - val_loss: 0.0063 - val_accuracy: 1.0000\n",
            "Epoch 200/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 8.5124e-04 - accuracy: 1.0000 - val_loss: 0.0060 - val_accuracy: 1.0000\n",
            "Epoch 201/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.7313e-04 - accuracy: 1.0000 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
            "Epoch 202/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 6.3177e-04 - accuracy: 1.0000 - val_loss: 0.0063 - val_accuracy: 1.0000\n",
            "Epoch 203/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.4304e-04 - accuracy: 1.0000 - val_loss: 0.0064 - val_accuracy: 1.0000\n",
            "Epoch 204/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 3.7876e-04 - accuracy: 1.0000 - val_loss: 0.0062 - val_accuracy: 1.0000\n",
            "Epoch 205/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 3.7293e-04 - accuracy: 1.0000 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
            "Epoch 206/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.6263e-04 - accuracy: 1.0000 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
            "Epoch 207/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 6.5703e-04 - accuracy: 1.0000 - val_loss: 0.0062 - val_accuracy: 1.0000\n",
            "Epoch 208/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 4.9334e-04 - accuracy: 1.0000 - val_loss: 0.0063 - val_accuracy: 1.0000\n",
            "Epoch 209/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 4.2440e-04 - accuracy: 1.0000 - val_loss: 0.0060 - val_accuracy: 1.0000\n",
            "Epoch 210/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 4.4066e-04 - accuracy: 1.0000 - val_loss: 0.0060 - val_accuracy: 1.0000\n",
            "Epoch 211/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 4.4040e-04 - accuracy: 1.0000 - val_loss: 0.0063 - val_accuracy: 1.0000\n",
            "Epoch 212/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.7539e-04 - accuracy: 1.0000 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
            "Epoch 213/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 3.2956e-04 - accuracy: 1.0000 - val_loss: 0.0060 - val_accuracy: 1.0000\n",
            "Epoch 214/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.5230e-04 - accuracy: 1.0000 - val_loss: 0.0059 - val_accuracy: 1.0000\n",
            "Epoch 215/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.0983e-04 - accuracy: 1.0000 - val_loss: 0.0058 - val_accuracy: 1.0000\n",
            "Epoch 216/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 3.9847e-04 - accuracy: 1.0000 - val_loss: 0.0060 - val_accuracy: 1.0000\n",
            "Epoch 217/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2.8886e-04 - accuracy: 1.0000 - val_loss: 0.0059 - val_accuracy: 1.0000\n",
            "Epoch 218/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 4.2481e-04 - accuracy: 1.0000 - val_loss: 0.0062 - val_accuracy: 1.0000\n",
            "Epoch 219/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 4.7630e-04 - accuracy: 1.0000 - val_loss: 0.0057 - val_accuracy: 1.0000\n",
            "Epoch 220/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 3.3074e-04 - accuracy: 1.0000 - val_loss: 0.0056 - val_accuracy: 1.0000\n",
            "Epoch 221/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 4.1702e-04 - accuracy: 1.0000 - val_loss: 0.0055 - val_accuracy: 1.0000\n",
            "Epoch 222/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 2.7097e-04 - accuracy: 1.0000 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
            "Epoch 223/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 3.1976e-04 - accuracy: 1.0000 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
            "Epoch 224/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 3.3590e-04 - accuracy: 1.0000 - val_loss: 0.0058 - val_accuracy: 1.0000\n",
            "Epoch 225/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 3.9487e-04 - accuracy: 1.0000 - val_loss: 0.0056 - val_accuracy: 1.0000\n",
            "Epoch 226/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.0977e-04 - accuracy: 1.0000 - val_loss: 0.0053 - val_accuracy: 1.0000\n",
            "Epoch 227/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.4483e-04 - accuracy: 1.0000 - val_loss: 0.0056 - val_accuracy: 1.0000\n",
            "Epoch 228/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 2.5492e-04 - accuracy: 1.0000 - val_loss: 0.0059 - val_accuracy: 1.0000\n",
            "Epoch 229/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.0797e-04 - accuracy: 1.0000 - val_loss: 0.0054 - val_accuracy: 1.0000\n",
            "Epoch 230/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.7990e-04 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 1.0000\n",
            "Epoch 231/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 3.2556e-04 - accuracy: 1.0000 - val_loss: 0.0055 - val_accuracy: 1.0000\n",
            "Epoch 232/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2.6928e-04 - accuracy: 1.0000 - val_loss: 0.0057 - val_accuracy: 1.0000\n",
            "Epoch 233/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 3.6221e-04 - accuracy: 1.0000 - val_loss: 0.0055 - val_accuracy: 1.0000\n",
            "Epoch 234/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 4.1819e-04 - accuracy: 1.0000 - val_loss: 0.0054 - val_accuracy: 1.0000\n",
            "Epoch 235/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 2.6328e-04 - accuracy: 1.0000 - val_loss: 0.0056 - val_accuracy: 1.0000\n",
            "Epoch 236/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 4.3713e-04 - accuracy: 1.0000 - val_loss: 0.0055 - val_accuracy: 1.0000\n",
            "Epoch 237/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 3.9450e-04 - accuracy: 1.0000 - val_loss: 0.0053 - val_accuracy: 1.0000\n",
            "Epoch 238/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 3.0049e-04 - accuracy: 1.0000 - val_loss: 0.0054 - val_accuracy: 1.0000\n",
            "Epoch 239/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2.2614e-04 - accuracy: 1.0000 - val_loss: 0.0054 - val_accuracy: 1.0000\n",
            "Epoch 240/500\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 2.3545e-04 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
            "Epoch 241/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 3.8307e-04 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 1.0000\n",
            "Epoch 242/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.8803e-04 - accuracy: 1.0000 - val_loss: 0.0055 - val_accuracy: 1.0000\n",
            "Epoch 243/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 3.5827e-04 - accuracy: 1.0000 - val_loss: 0.0054 - val_accuracy: 1.0000\n",
            "Epoch 244/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 3.4072e-04 - accuracy: 1.0000 - val_loss: 0.0053 - val_accuracy: 1.0000\n",
            "Epoch 245/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.1915e-04 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
            "Epoch 246/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 3.6795e-04 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 1.0000\n",
            "Epoch 247/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 2.0286e-04 - accuracy: 1.0000 - val_loss: 0.0053 - val_accuracy: 1.0000\n",
            "Epoch 248/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2.9979e-04 - accuracy: 1.0000 - val_loss: 0.0053 - val_accuracy: 1.0000\n",
            "Epoch 249/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 3.7622e-04 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
            "Epoch 250/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 2.9104e-04 - accuracy: 1.0000 - val_loss: 0.0055 - val_accuracy: 1.0000\n",
            "Epoch 251/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.8722e-04 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 1.0000\n",
            "Epoch 252/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 4.2164e-04 - accuracy: 1.0000 - val_loss: 0.0048 - val_accuracy: 1.0000\n",
            "Epoch 253/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 4.0930e-04 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
            "Epoch 254/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 2.5824e-04 - accuracy: 1.0000 - val_loss: 0.0053 - val_accuracy: 1.0000\n",
            "Epoch 255/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 4.0761e-04 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 1.0000\n",
            "Epoch 256/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 2.5157e-04 - accuracy: 1.0000 - val_loss: 0.0049 - val_accuracy: 1.0000\n",
            "Epoch 257/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 3.0652e-04 - accuracy: 1.0000 - val_loss: 0.0049 - val_accuracy: 1.0000\n",
            "Epoch 258/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.6797e-04 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
            "Epoch 259/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 2.8387e-04 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
            "Epoch 260/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.6678e-04 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
            "Epoch 261/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 2.0265e-04 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
            "Epoch 262/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 2.6698e-04 - accuracy: 1.0000 - val_loss: 0.0049 - val_accuracy: 1.0000\n",
            "Epoch 263/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 4.3451e-04 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
            "Epoch 264/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 2.7121e-04 - accuracy: 1.0000 - val_loss: 0.0048 - val_accuracy: 1.0000\n",
            "Epoch 265/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2.0941e-04 - accuracy: 1.0000 - val_loss: 0.0049 - val_accuracy: 1.0000\n",
            "Epoch 266/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.6172e-04 - accuracy: 1.0000 - val_loss: 0.0050 - val_accuracy: 1.0000\n",
            "Epoch 267/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.3916e-04 - accuracy: 1.0000 - val_loss: 0.0049 - val_accuracy: 1.0000\n",
            "Epoch 268/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.4903e-04 - accuracy: 1.0000 - val_loss: 0.0048 - val_accuracy: 1.0000\n",
            "Epoch 269/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 2.4000e-04 - accuracy: 1.0000 - val_loss: 0.0050 - val_accuracy: 1.0000\n",
            "Epoch 270/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.6214e-04 - accuracy: 1.0000 - val_loss: 0.0050 - val_accuracy: 1.0000\n",
            "Epoch 271/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.9722e-04 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
            "Epoch 272/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.9043e-04 - accuracy: 1.0000 - val_loss: 0.0048 - val_accuracy: 1.0000\n",
            "Epoch 273/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 2.1587e-04 - accuracy: 1.0000 - val_loss: 0.0048 - val_accuracy: 1.0000\n",
            "Epoch 274/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2.6988e-04 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
            "Epoch 275/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 3.7733e-04 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
            "Epoch 276/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 2.2088e-04 - accuracy: 1.0000 - val_loss: 0.0048 - val_accuracy: 1.0000\n",
            "Epoch 277/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2.8171e-04 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
            "Epoch 278/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 2.6193e-04 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
            "Epoch 279/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.9208e-04 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
            "Epoch 280/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 4.0440e-04 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
            "Epoch 281/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 2.1056e-04 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
            "Epoch 282/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.8003e-04 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
            "Epoch 283/500\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 2.0500e-04 - accuracy: 1.0000 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
            "Epoch 284/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 2.0024e-04 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
            "Epoch 285/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 3.4807e-04 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
            "Epoch 286/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 2.3835e-04 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
            "Epoch 287/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2.0492e-04 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
            "Epoch 288/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.2631e-04 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
            "Epoch 289/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.4087e-04 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
            "Epoch 290/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1.9461e-04 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 1.0000\n",
            "Epoch 291/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 2.2601e-04 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
            "Epoch 292/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.7622e-04 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 1.0000\n",
            "Epoch 293/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2.0454e-04 - accuracy: 1.0000 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
            "Epoch 294/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.4718e-04 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
            "Epoch 295/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.5528e-04 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 1.0000\n",
            "Epoch 296/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.6753e-04 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 1.0000\n",
            "Epoch 297/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2.0632e-04 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
            "Epoch 298/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.7848e-04 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 1.0000\n",
            "Epoch 299/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.7556e-04 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 1.0000\n",
            "Epoch 300/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1.9915e-04 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 1.0000\n",
            "Epoch 301/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1.8070e-04 - accuracy: 1.0000 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
            "Epoch 302/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 2.3003e-04 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 1.0000\n",
            "Epoch 303/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.9399e-04 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
            "Epoch 304/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 2.0861e-04 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
            "Epoch 305/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 2.4653e-04 - accuracy: 1.0000 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
            "Epoch 306/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.8958e-04 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 1.0000\n",
            "Epoch 307/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.5076e-04 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
            "Epoch 308/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.8232e-04 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 1.0000\n",
            "Epoch 309/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.7177e-04 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 1.0000\n",
            "Epoch 310/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.6514e-04 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
            "Epoch 311/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.7116e-04 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
            "Epoch 312/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.2732e-04 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 1.0000\n",
            "Epoch 313/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 2.0168e-04 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
            "Epoch 314/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.4722e-04 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
            "Epoch 315/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 2.1071e-04 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
            "Epoch 316/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 2.0114e-04 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
            "Epoch 317/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.3892e-04 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
            "Epoch 318/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 1.3876e-04 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
            "Epoch 319/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.1827e-04 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
            "Epoch 320/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 2.1708e-04 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
            "Epoch 321/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.4931e-04 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
            "Epoch 322/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 9.2768e-05 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
            "Epoch 323/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.3541e-04 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
            "Epoch 324/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.6738e-04 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
            "Epoch 325/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 2.2948e-04 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
            "Epoch 326/500\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 9.5975e-05 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
            "Epoch 327/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.5372e-04 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
            "Epoch 328/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.5375e-04 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
            "Epoch 329/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.6046e-04 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
            "Epoch 330/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.4603e-04 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
            "Epoch 331/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1.0787e-04 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
            "Epoch 332/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 9.0836e-05 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
            "Epoch 333/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 8.7205e-05 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
            "Epoch 334/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.3637e-04 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
            "Epoch 335/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.1712e-04 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
            "Epoch 336/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.2921e-04 - accuracy: 1.0000 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
            "Epoch 337/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.4685e-04 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
            "Epoch 338/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 9.4910e-05 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
            "Epoch 339/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.0869e-04 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
            "Epoch 340/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.5270e-04 - accuracy: 1.0000 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
            "Epoch 341/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.2408e-04 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
            "Epoch 342/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 1.7131e-04 - accuracy: 1.0000 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
            "Epoch 343/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1.2135e-04 - accuracy: 1.0000 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
            "Epoch 344/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 9.3013e-05 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "Epoch 345/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.1070e-04 - accuracy: 1.0000 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
            "Epoch 346/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1.4340e-04 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
            "Epoch 347/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.2373e-04 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
            "Epoch 348/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 7.1870e-05 - accuracy: 1.0000 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
            "Epoch 349/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1.0942e-04 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "Epoch 350/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.1496e-04 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
            "Epoch 351/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 6.8893e-05 - accuracy: 1.0000 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
            "Epoch 352/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 1.2549e-04 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
            "Epoch 353/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1.6563e-04 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "Epoch 354/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 7.8332e-05 - accuracy: 1.0000 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
            "Epoch 355/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.6421e-04 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
            "Epoch 356/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1.1278e-04 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
            "Epoch 357/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.4512e-04 - accuracy: 1.0000 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
            "Epoch 358/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 1.1325e-04 - accuracy: 1.0000 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
            "Epoch 359/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.0828e-04 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "Epoch 360/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 9.9828e-05 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
            "Epoch 361/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.6242e-04 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
            "Epoch 362/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.0918e-04 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "Epoch 363/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.6606e-04 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "Epoch 364/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1.1976e-04 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
            "Epoch 365/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.4323e-04 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "Epoch 366/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1.0621e-04 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
            "Epoch 367/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1.7202e-04 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
            "Epoch 368/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 9.7576e-05 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
            "Epoch 369/500\n",
            "8/8 [==============================] - 0s 28ms/step - loss: 1.6384e-04 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
            "Epoch 370/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 7.3186e-05 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
            "Epoch 371/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 6.7634e-05 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "Epoch 372/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 1.0597e-04 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "Epoch 373/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1.0802e-04 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
            "Epoch 374/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1.3366e-04 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
            "Epoch 375/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.1102e-04 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
            "Epoch 376/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 8.0584e-05 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
            "Epoch 377/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 8.7623e-05 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
            "Epoch 378/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.1645e-04 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "Epoch 379/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.0110e-04 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
            "Epoch 380/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 9.2108e-05 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
            "Epoch 381/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.1905e-04 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
            "Epoch 382/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.1555e-04 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
            "Epoch 383/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.3529e-04 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
            "Epoch 384/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1.2962e-04 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
            "Epoch 385/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.9540e-05 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
            "Epoch 386/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 9.1632e-05 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
            "Epoch 387/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.5113e-04 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
            "Epoch 388/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1.1523e-04 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
            "Epoch 389/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 8.1200e-05 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
            "Epoch 390/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 8.6180e-05 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
            "Epoch 391/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.1708e-04 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
            "Epoch 392/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 8.6492e-05 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
            "Epoch 393/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 9.5062e-05 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
            "Epoch 394/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 7.8458e-05 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
            "Epoch 395/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.0257e-04 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
            "Epoch 396/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 7.0254e-05 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
            "Epoch 397/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.5780e-05 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
            "Epoch 398/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.7176e-05 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
            "Epoch 399/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 7.9351e-05 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
            "Epoch 400/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.1706e-04 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
            "Epoch 401/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 7.9325e-05 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
            "Epoch 402/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 8.2452e-05 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
            "Epoch 403/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.1663e-05 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
            "Epoch 404/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 7.3420e-05 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
            "Epoch 405/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 9.4102e-05 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
            "Epoch 406/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 7.5649e-05 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
            "Epoch 407/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 1.0036e-04 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
            "Epoch 408/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 9.8172e-05 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
            "Epoch 409/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 7.7509e-05 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
            "Epoch 410/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.5736e-05 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
            "Epoch 411/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 8.2977e-05 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
            "Epoch 412/500\n",
            "8/8 [==============================] - 0s 28ms/step - loss: 1.1686e-04 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
            "Epoch 413/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.3291e-05 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
            "Epoch 414/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 9.6569e-05 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
            "Epoch 415/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.0316e-04 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
            "Epoch 416/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 5.1490e-05 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
            "Epoch 417/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 9.2365e-05 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
            "Epoch 418/500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 8.0161e-05 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
            "Epoch 419/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.1605e-04 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
            "Epoch 420/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 7.1192e-05 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
            "Epoch 421/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 7.3173e-05 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
            "Epoch 422/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 7.3128e-05 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
            "Epoch 423/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 8.4091e-05 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
            "Epoch 424/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 6.7205e-05 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
            "Epoch 425/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 6.2988e-05 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
            "Epoch 426/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 9.5715e-05 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
            "Epoch 427/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 6.0231e-05 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
            "Epoch 428/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 8.0754e-05 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
            "Epoch 429/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.9753e-05 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
            "Epoch 430/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 6.0775e-05 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
            "Epoch 431/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 7.9522e-05 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
            "Epoch 432/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 6.8679e-05 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
            "Epoch 433/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 9.2585e-05 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
            "Epoch 434/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 7.8218e-05 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
            "Epoch 435/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 8.7766e-05 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
            "Epoch 436/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 4.8445e-05 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
            "Epoch 437/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 7.5616e-05 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
            "Epoch 438/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.0943e-04 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
            "Epoch 439/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 6.8879e-05 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
            "Epoch 440/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.9249e-05 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
            "Epoch 441/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.6545e-05 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
            "Epoch 442/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.5722e-05 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
            "Epoch 443/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 3.9264e-05 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
            "Epoch 444/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.7128e-05 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
            "Epoch 445/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 6.9831e-05 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
            "Epoch 446/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 7.4445e-05 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
            "Epoch 447/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 7.3761e-05 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
            "Epoch 448/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 8.9322e-05 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
            "Epoch 449/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 6.4614e-05 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
            "Epoch 450/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 3.2261e-05 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
            "Epoch 451/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 4.7990e-05 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
            "Epoch 452/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 4.7506e-05 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
            "Epoch 453/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 6.2914e-05 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
            "Epoch 454/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.2358e-05 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
            "Epoch 455/500\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 8.5851e-05 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
            "Epoch 456/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 8.0353e-05 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
            "Epoch 457/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.5354e-05 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
            "Epoch 458/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 6.5463e-05 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
            "Epoch 459/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 7.5962e-05 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
            "Epoch 460/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 6.8887e-05 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
            "Epoch 461/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.3314e-05 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
            "Epoch 462/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 3.8957e-05 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
            "Epoch 463/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.9530e-05 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
            "Epoch 464/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 4.0581e-05 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
            "Epoch 465/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 4.5060e-05 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
            "Epoch 466/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 4.0609e-05 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
            "Epoch 467/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.7549e-05 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
            "Epoch 468/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.6832e-05 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
            "Epoch 469/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 3.3873e-05 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
            "Epoch 470/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.5822e-05 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
            "Epoch 471/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.8304e-05 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
            "Epoch 472/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 3.3026e-05 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
            "Epoch 473/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 4.1971e-05 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
            "Epoch 474/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 8.2906e-05 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
            "Epoch 475/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 4.6842e-05 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
            "Epoch 476/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 2.9997e-05 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
            "Epoch 477/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 6.6951e-05 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
            "Epoch 478/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 4.2811e-05 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
            "Epoch 479/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 6.5896e-05 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
            "Epoch 480/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 4.3576e-05 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
            "Epoch 481/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 4.1716e-05 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
            "Epoch 482/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 4.7051e-05 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
            "Epoch 483/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.0846e-05 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
            "Epoch 484/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 4.3796e-05 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
            "Epoch 485/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 3.8083e-05 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
            "Epoch 486/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.9191e-05 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
            "Epoch 487/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.2436e-05 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
            "Epoch 488/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 3.4255e-05 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
            "Epoch 489/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 3.6741e-05 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
            "Epoch 490/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 2.9954e-05 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
            "Epoch 491/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 3.6447e-05 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
            "Epoch 492/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 7.5365e-05 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
            "Epoch 493/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 4.1049e-05 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
            "Epoch 494/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.0675e-05 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
            "Epoch 495/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 3.6602e-05 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
            "Epoch 496/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 4.2642e-05 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
            "Epoch 497/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 2.2996e-05 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
            "Epoch 498/500\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 3.0630e-05 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
            "Epoch 499/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 4.5695e-05 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
            "Epoch 500/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 4.3410e-05 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98fDl51allTX",
        "outputId": "22c250d0-bc1c-4d58-fdc3-8840493dbd4b"
      },
      "source": [
        "print('Loss Funciton: ', model.evaluate(x_train,y_train, verbose=1)[0])\r\n",
        "print('Accuracy', model.evaluate(x_train,y_train, verbose=1)[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3/3 [==============================] - 0s 6ms/step - loss: 4.2868e-05 - accuracy: 1.0000\n",
            "Loss Funciton:  4.2867766751442105e-05\n",
            "3/3 [==============================] - 0s 3ms/step - loss: 4.2868e-05 - accuracy: 1.0000\n",
            "Accuracy 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "SUl5Q9FCllXn",
        "outputId": "3e80b575-6e99-4e44-8ea0-3d06ad4a2298"
      },
      "source": [
        "plt.plot(history.history['loss'], label='train')\r\n",
        "plt.plot(history.history['val_loss'], label='test')\r\n",
        "plt.legend()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "zRM1U5OpllcN",
        "outputId": "800e7854-6473-469b-d709-f3ae26c23bbf"
      },
      "source": [
        "print(y_test.shape)\r\n",
        "print(y_test)\r\n",
        "print(y_test.ravel())\r\n",
        "plot_decision_regions(x_test, y_test.ravel(), clf = model, legend=2)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20,)\n",
            "[1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1]\n",
            "[1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/mlxtend/plotting/decision_regions.py:244: MatplotlibDeprecationWarning: Passing unsupported keyword arguments to axis() will raise a TypeError in 3.3.\n",
            "  ax.axis(xmin=xx.min(), xmax=xx.max(), y_min=yy.min(), y_max=yy.max())\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6COnMyJvllgY",
        "outputId": "33147ca0-543b-4cb7-da91-3a7802166579"
      },
      "source": [
        "from sklearn import datasets\r\n",
        "import keras \r\n",
        "import tensorflow as tf\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "d=datasets.load_iris()\r\n",
        "x = d.data\r\n",
        "y = d.target\r\n",
        "\r\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.15, random_state=0)\r\n",
        "\r\n",
        "model =  Sequential()\r\n",
        "model.add(Dense(100, input_shape=[4], activation='relu'))\r\n",
        "model.add(Dense(50, activation='relu'))\r\n",
        "model.add(Dense(3, activation='softmax'))\r\n",
        "print(model.summary())\r\n",
        "model.compile(loss='SparseCategoricalCrossentropy', optimizer=keras.optimizers.Adam(lr=0.001), metrics=['accuracy'])\r\n",
        "\r\n",
        "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=100, batch_size=30)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_58 (Dense)             (None, 100)               500       \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 50)                5050      \n",
            "_________________________________________________________________\n",
            "dense_60 (Dense)             (None, 3)                 153       \n",
            "=================================================================\n",
            "Total params: 5,703\n",
            "Trainable params: 5,703\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "5/5 [==============================] - 1s 51ms/step - loss: 1.1759 - accuracy: 0.3697 - val_loss: 0.9744 - val_accuracy: 0.5217\n",
            "Epoch 2/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.9286 - accuracy: 0.6530 - val_loss: 0.9336 - val_accuracy: 0.5217\n",
            "Epoch 3/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.8634 - accuracy: 0.6759 - val_loss: 0.8559 - val_accuracy: 0.5217\n",
            "Epoch 4/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.7618 - accuracy: 0.7101 - val_loss: 0.8135 - val_accuracy: 0.5217\n",
            "Epoch 5/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.7241 - accuracy: 0.6662 - val_loss: 0.6957 - val_accuracy: 0.5217\n",
            "Epoch 6/100\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.6654 - accuracy: 0.7152 - val_loss: 0.6478 - val_accuracy: 0.5217\n",
            "Epoch 7/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.5925 - accuracy: 0.7285 - val_loss: 0.6587 - val_accuracy: 0.5217\n",
            "Epoch 8/100\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5675 - accuracy: 0.6842 - val_loss: 0.5897 - val_accuracy: 0.5217\n",
            "Epoch 9/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.5130 - accuracy: 0.7796 - val_loss: 0.5197 - val_accuracy: 0.9565\n",
            "Epoch 10/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.5242 - accuracy: 0.9373 - val_loss: 0.5002 - val_accuracy: 0.9565\n",
            "Epoch 11/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.4688 - accuracy: 0.9136 - val_loss: 0.5230 - val_accuracy: 0.5652\n",
            "Epoch 12/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.4399 - accuracy: 0.7620 - val_loss: 0.5052 - val_accuracy: 0.5652\n",
            "Epoch 13/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.4427 - accuracy: 0.8574 - val_loss: 0.4374 - val_accuracy: 1.0000\n",
            "Epoch 14/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.4253 - accuracy: 0.9639 - val_loss: 0.4707 - val_accuracy: 0.6522\n",
            "Epoch 15/100\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.4073 - accuracy: 0.8180 - val_loss: 0.4954 - val_accuracy: 0.5652\n",
            "Epoch 16/100\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.3667 - accuracy: 0.8086 - val_loss: 0.4264 - val_accuracy: 0.8261\n",
            "Epoch 17/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.3957 - accuracy: 0.9315 - val_loss: 0.3919 - val_accuracy: 1.0000\n",
            "Epoch 18/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.3628 - accuracy: 0.9347 - val_loss: 0.4368 - val_accuracy: 0.6957\n",
            "Epoch 19/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.3480 - accuracy: 0.8665 - val_loss: 0.3960 - val_accuracy: 0.8261\n",
            "Epoch 20/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.3488 - accuracy: 0.9512 - val_loss: 0.3428 - val_accuracy: 1.0000\n",
            "Epoch 21/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.3481 - accuracy: 0.9614 - val_loss: 0.3845 - val_accuracy: 0.8261\n",
            "Epoch 22/100\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.3298 - accuracy: 0.9069 - val_loss: 0.4089 - val_accuracy: 0.6957\n",
            "Epoch 23/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.3294 - accuracy: 0.8870 - val_loss: 0.3207 - val_accuracy: 1.0000\n",
            "Epoch 24/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.3118 - accuracy: 0.9543 - val_loss: 0.2991 - val_accuracy: 1.0000\n",
            "Epoch 25/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.2851 - accuracy: 0.9620 - val_loss: 0.3301 - val_accuracy: 0.9565\n",
            "Epoch 26/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.2858 - accuracy: 0.9574 - val_loss: 0.3246 - val_accuracy: 0.9565\n",
            "Epoch 27/100\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.2827 - accuracy: 0.9469 - val_loss: 0.3294 - val_accuracy: 0.9130\n",
            "Epoch 28/100\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.2543 - accuracy: 0.9630 - val_loss: 0.2775 - val_accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.2439 - accuracy: 0.9719 - val_loss: 0.2483 - val_accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.2459 - accuracy: 0.9762 - val_loss: 0.2737 - val_accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.2343 - accuracy: 0.9551 - val_loss: 0.2621 - val_accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.2390 - accuracy: 0.9398 - val_loss: 0.2495 - val_accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.2468 - accuracy: 0.9515 - val_loss: 0.2262 - val_accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "5/5 [==============================] - 0s 47ms/step - loss: 0.2101 - accuracy: 0.9816 - val_loss: 0.2178 - val_accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.2062 - accuracy: 0.9738 - val_loss: 0.1951 - val_accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.1989 - accuracy: 0.9645 - val_loss: 0.2241 - val_accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.1906 - accuracy: 0.9537 - val_loss: 0.2220 - val_accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.1873 - accuracy: 0.9673 - val_loss: 0.1779 - val_accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.1943 - accuracy: 0.9714 - val_loss: 0.1771 - val_accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.1820 - accuracy: 0.9701 - val_loss: 0.1813 - val_accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.1867 - accuracy: 0.9617 - val_loss: 0.1672 - val_accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.1541 - accuracy: 0.9673 - val_loss: 0.1836 - val_accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.1865 - accuracy: 0.9548 - val_loss: 0.1622 - val_accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.1777 - accuracy: 0.9477 - val_loss: 0.1248 - val_accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.1466 - accuracy: 0.9751 - val_loss: 0.1781 - val_accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.1503 - accuracy: 0.9586 - val_loss: 0.1286 - val_accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.1385 - accuracy: 0.9673 - val_loss: 0.1665 - val_accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.1674 - accuracy: 0.9512 - val_loss: 0.1817 - val_accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.1590 - accuracy: 0.9426 - val_loss: 0.1040 - val_accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.1433 - accuracy: 0.9518 - val_loss: 0.1099 - val_accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.1306 - accuracy: 0.9758 - val_loss: 0.1513 - val_accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.1203 - accuracy: 0.9756 - val_loss: 0.0965 - val_accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.1422 - accuracy: 0.9572 - val_loss: 0.0843 - val_accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.1399 - accuracy: 0.9701 - val_loss: 0.0992 - val_accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.1256 - accuracy: 0.9673 - val_loss: 0.1213 - val_accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.1320 - accuracy: 0.9714 - val_loss: 0.0851 - val_accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.1373 - accuracy: 0.9630 - val_loss: 0.0882 - val_accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.1171 - accuracy: 0.9719 - val_loss: 0.1085 - val_accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.1361 - accuracy: 0.9589 - val_loss: 0.0873 - val_accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.1329 - accuracy: 0.9494 - val_loss: 0.0812 - val_accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.1018 - accuracy: 0.9719 - val_loss: 0.1151 - val_accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.1148 - accuracy: 0.9701 - val_loss: 0.0889 - val_accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.1074 - accuracy: 0.9701 - val_loss: 0.0831 - val_accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.1230 - accuracy: 0.9687 - val_loss: 0.0775 - val_accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.1028 - accuracy: 0.9719 - val_loss: 0.0753 - val_accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.1143 - accuracy: 0.9747 - val_loss: 0.0824 - val_accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.1084 - accuracy: 0.9714 - val_loss: 0.0547 - val_accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.1175 - accuracy: 0.9775 - val_loss: 0.0580 - val_accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.1109 - accuracy: 0.9673 - val_loss: 0.1017 - val_accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.1213 - accuracy: 0.9691 - val_loss: 0.0711 - val_accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.1047 - accuracy: 0.9645 - val_loss: 0.0607 - val_accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.0991 - accuracy: 0.9688 - val_loss: 0.0586 - val_accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.1144 - accuracy: 0.9543 - val_loss: 0.0600 - val_accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.1050 - accuracy: 0.9494 - val_loss: 0.0604 - val_accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.1055 - accuracy: 0.9673 - val_loss: 0.0710 - val_accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.1066 - accuracy: 0.9589 - val_loss: 0.0679 - val_accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "5/5 [==============================] - 0s 47ms/step - loss: 0.1109 - accuracy: 0.9589 - val_loss: 0.0783 - val_accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.0849 - accuracy: 0.9710 - val_loss: 0.0533 - val_accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.1123 - accuracy: 0.9571 - val_loss: 0.0465 - val_accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.1053 - accuracy: 0.9633 - val_loss: 0.0662 - val_accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0901 - accuracy: 0.9701 - val_loss: 0.0448 - val_accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0847 - accuracy: 0.9773 - val_loss: 0.0414 - val_accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.1040 - accuracy: 0.9827 - val_loss: 0.0400 - val_accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0992 - accuracy: 0.9719 - val_loss: 0.0571 - val_accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.1018 - accuracy: 0.9589 - val_loss: 0.0532 - val_accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0850 - accuracy: 0.9747 - val_loss: 0.0582 - val_accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0759 - accuracy: 0.9751 - val_loss: 0.0403 - val_accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0853 - accuracy: 0.9761 - val_loss: 0.0338 - val_accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.0756 - accuracy: 0.9775 - val_loss: 0.0720 - val_accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.1041 - accuracy: 0.9654 - val_loss: 0.0459 - val_accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0814 - accuracy: 0.9645 - val_loss: 0.0368 - val_accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.0926 - accuracy: 0.9568 - val_loss: 0.0400 - val_accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0864 - accuracy: 0.9626 - val_loss: 0.0462 - val_accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0909 - accuracy: 0.9673 - val_loss: 0.0456 - val_accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.0863 - accuracy: 0.9787 - val_loss: 0.0285 - val_accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.0844 - accuracy: 0.9829 - val_loss: 0.0375 - val_accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0840 - accuracy: 0.9688 - val_loss: 0.0337 - val_accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0759 - accuracy: 0.9787 - val_loss: 0.0375 - val_accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.0769 - accuracy: 0.9802 - val_loss: 0.0486 - val_accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.1049 - accuracy: 0.9617 - val_loss: 0.0361 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "k7tuySzullkt",
        "outputId": "84a439f2-fb33-41fd-8734-2eaa45f43a04"
      },
      "source": [
        "plt.plot(history.history['loss'], label='train')\r\n",
        "plt.plot(history.history['val_loss'], label='test')\r\n",
        "plt.legend()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_ZvuVJMllt2"
      },
      "source": [
        "weights =  model.get_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOlDhqpxzsxQ",
        "outputId": "4bd0b75b-3a55-4e90-de63-02750a4219cf"
      },
      "source": [
        "n=len(weights)\r\n",
        "for x in range(n):\r\n",
        "  i=np.max(weights[x])\r\n",
        "  print(i)\r\n",
        "\r\n",
        "n=len(weights)\r\n",
        "for x in range(n):\r\n",
        "  i=np.min(weights[x])\r\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.36511213\n",
            "0.14812356\n",
            "0.43386036\n",
            "0.119158156\n",
            "0.4762637\n",
            "0.05001568\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6EXlX2M0syd"
      },
      "source": [
        "###Applying L2 Regularization Technique"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lX1DVBhzt3Z",
        "outputId": "54ddb7af-f6b0-4774-f3ff-3b036490f593"
      },
      "source": [
        "# weights begore the L2 Regularization\r\n",
        "n=len(weights)\r\n",
        "for x in range(n):\r\n",
        "  i=np.min(weights[x])\r\n",
        "  print(i)\r\n",
        "\r\n",
        "print('0.36511213\r\n",
        "0.14812356\r\n",
        "0.43386036\r\n",
        "0.119158156\r\n",
        "0.4762637\r\n",
        "0.05001568\\n\r\n",
        "-0.3736164\r\n",
        "-0.12738284\r\n",
        "-0.4192349\r\n",
        "-0.077513225\r\n",
        "-0.5944754\r\n",
        "-0.07054395\r\n",
        "')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-0.3736164\n",
            "-0.12738284\n",
            "-0.4192349\n",
            "-0.077513225\n",
            "-0.5944754\n",
            "-0.07054395\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpCHd3Vd1Gz4",
        "outputId": "d573ade8-6edb-4ef1-cf50-ee80c9b398d4"
      },
      "source": [
        "from sklearn import datasets\r\n",
        "import keras \r\n",
        "import tensorflow as tf\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "d=datasets.load_iris()\r\n",
        "x = d.data\r\n",
        "y = d.target\r\n",
        "\r\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.15, random_state=0)\r\n",
        "\r\n",
        "model =  Sequential()\r\n",
        "model.add(Dense(100, input_shape=[4], activation='relu', kernel_regularizer='l2'))\r\n",
        "model.add(Dense(50, activation='relu', kernel_regularizer='l2'))\r\n",
        "model.add(Dense(3, activation='softmax', kernel_regularizer='l2'))\r\n",
        "print(model.summary())\r\n",
        "model.compile(loss='SparseCategoricalCrossentropy', optimizer=keras.optimizers.Adam(lr=0.001), metrics=['accuracy'])\r\n",
        "\r\n",
        "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=100, batch_size=30)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 100)               500       \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 50)                5050      \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 3)                 153       \n",
            "=================================================================\n",
            "Total params: 5,703\n",
            "Trainable params: 5,703\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "5/5 [==============================] - 1s 42ms/step - loss: 1.8993 - accuracy: 0.3090 - val_loss: 1.7474 - val_accuracy: 0.5217\n",
            "Epoch 2/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 1.6911 - accuracy: 0.6717 - val_loss: 1.6959 - val_accuracy: 0.5217\n",
            "Epoch 3/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 1.5820 - accuracy: 0.6930 - val_loss: 1.5885 - val_accuracy: 0.5217\n",
            "Epoch 4/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 1.4907 - accuracy: 0.6847 - val_loss: 1.4618 - val_accuracy: 0.5652\n",
            "Epoch 5/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 1.3993 - accuracy: 0.6851 - val_loss: 1.3893 - val_accuracy: 0.5217\n",
            "Epoch 6/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 1.3034 - accuracy: 0.6952 - val_loss: 1.2991 - val_accuracy: 0.5652\n",
            "Epoch 7/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 1.2508 - accuracy: 0.7108 - val_loss: 1.2007 - val_accuracy: 0.8261\n",
            "Epoch 8/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 1.1462 - accuracy: 0.8717 - val_loss: 1.1471 - val_accuracy: 0.6522\n",
            "Epoch 9/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 1.0832 - accuracy: 0.8629 - val_loss: 1.0986 - val_accuracy: 0.6087\n",
            "Epoch 10/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 1.0357 - accuracy: 0.8662 - val_loss: 1.0324 - val_accuracy: 0.8261\n",
            "Epoch 11/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.9848 - accuracy: 0.9475 - val_loss: 0.9774 - val_accuracy: 0.8696\n",
            "Epoch 12/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.9293 - accuracy: 0.9381 - val_loss: 0.9219 - val_accuracy: 0.9565\n",
            "Epoch 13/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.8952 - accuracy: 0.9628 - val_loss: 0.8874 - val_accuracy: 0.9565\n",
            "Epoch 14/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.8639 - accuracy: 0.9728 - val_loss: 0.8922 - val_accuracy: 0.8261\n",
            "Epoch 15/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.8409 - accuracy: 0.8703 - val_loss: 0.8893 - val_accuracy: 0.7391\n",
            "Epoch 16/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.8301 - accuracy: 0.8421 - val_loss: 0.8088 - val_accuracy: 0.9565\n",
            "Epoch 17/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.7811 - accuracy: 0.9728 - val_loss: 0.7887 - val_accuracy: 0.9565\n",
            "Epoch 18/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.7448 - accuracy: 0.9390 - val_loss: 0.8057 - val_accuracy: 0.8261\n",
            "Epoch 19/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.7271 - accuracy: 0.9120 - val_loss: 0.7479 - val_accuracy: 0.9565\n",
            "Epoch 20/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.7060 - accuracy: 0.9775 - val_loss: 0.7028 - val_accuracy: 1.0000\n",
            "Epoch 21/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.6852 - accuracy: 0.9633 - val_loss: 0.7310 - val_accuracy: 0.8261\n",
            "Epoch 22/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.6874 - accuracy: 0.9287 - val_loss: 0.7170 - val_accuracy: 0.8261\n",
            "Epoch 23/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.6627 - accuracy: 0.9244 - val_loss: 0.6780 - val_accuracy: 0.9565\n",
            "Epoch 24/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.6404 - accuracy: 0.9447 - val_loss: 0.6451 - val_accuracy: 0.9565\n",
            "Epoch 25/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.6213 - accuracy: 0.9756 - val_loss: 0.6281 - val_accuracy: 0.9565\n",
            "Epoch 26/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.6036 - accuracy: 0.9457 - val_loss: 0.6341 - val_accuracy: 0.9565\n",
            "Epoch 27/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.5960 - accuracy: 0.9611 - val_loss: 0.6129 - val_accuracy: 0.9565\n",
            "Epoch 28/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.5643 - accuracy: 0.9579 - val_loss: 0.5809 - val_accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.5700 - accuracy: 0.9799 - val_loss: 0.5450 - val_accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.5595 - accuracy: 0.9784 - val_loss: 0.5970 - val_accuracy: 0.9565\n",
            "Epoch 31/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.5273 - accuracy: 0.9722 - val_loss: 0.5671 - val_accuracy: 0.9565\n",
            "Epoch 32/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.5483 - accuracy: 0.9571 - val_loss: 0.5057 - val_accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.5209 - accuracy: 0.9691 - val_loss: 0.5437 - val_accuracy: 0.9565\n",
            "Epoch 34/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.5112 - accuracy: 0.9491 - val_loss: 0.5476 - val_accuracy: 0.9565\n",
            "Epoch 35/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.5106 - accuracy: 0.9565 - val_loss: 0.5105 - val_accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.4860 - accuracy: 0.9560 - val_loss: 0.5051 - val_accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "5/5 [==============================] - 0s 37ms/step - loss: 0.4761 - accuracy: 0.9815 - val_loss: 0.4669 - val_accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.4790 - accuracy: 0.9633 - val_loss: 0.4888 - val_accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.4652 - accuracy: 0.9654 - val_loss: 0.4645 - val_accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.4579 - accuracy: 0.9628 - val_loss: 0.4740 - val_accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.4459 - accuracy: 0.9647 - val_loss: 0.4512 - val_accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.4473 - accuracy: 0.9796 - val_loss: 0.4275 - val_accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.4488 - accuracy: 0.9753 - val_loss: 0.4617 - val_accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.4436 - accuracy: 0.9710 - val_loss: 0.4290 - val_accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.4221 - accuracy: 0.9721 - val_loss: 0.4305 - val_accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.4125 - accuracy: 0.9843 - val_loss: 0.3997 - val_accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.4249 - accuracy: 0.9654 - val_loss: 0.4039 - val_accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.4173 - accuracy: 0.9685 - val_loss: 0.4105 - val_accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.4194 - accuracy: 0.9596 - val_loss: 0.4030 - val_accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.4073 - accuracy: 0.9733 - val_loss: 0.3980 - val_accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3990 - accuracy: 0.9694 - val_loss: 0.4794 - val_accuracy: 0.9565\n",
            "Epoch 52/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.4008 - accuracy: 0.9670 - val_loss: 0.3668 - val_accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.4013 - accuracy: 0.9741 - val_loss: 0.3733 - val_accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.4002 - accuracy: 0.9756 - val_loss: 0.4091 - val_accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3850 - accuracy: 0.9776 - val_loss: 0.3875 - val_accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3708 - accuracy: 0.9880 - val_loss: 0.3550 - val_accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3767 - accuracy: 0.9801 - val_loss: 0.3606 - val_accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3954 - accuracy: 0.9745 - val_loss: 0.3618 - val_accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3877 - accuracy: 0.9636 - val_loss: 0.3905 - val_accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3928 - accuracy: 0.9426 - val_loss: 0.3707 - val_accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3701 - accuracy: 0.9861 - val_loss: 0.3447 - val_accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3661 - accuracy: 0.9751 - val_loss: 0.3413 - val_accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3587 - accuracy: 0.9847 - val_loss: 0.3620 - val_accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3644 - accuracy: 0.9815 - val_loss: 0.3436 - val_accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3718 - accuracy: 0.9759 - val_loss: 0.3541 - val_accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3851 - accuracy: 0.9605 - val_loss: 0.3516 - val_accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3628 - accuracy: 0.9676 - val_loss: 0.3313 - val_accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3587 - accuracy: 0.9875 - val_loss: 0.3369 - val_accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3564 - accuracy: 0.9818 - val_loss: 0.3320 - val_accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.3522 - accuracy: 0.9639 - val_loss: 0.3933 - val_accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3494 - accuracy: 0.9674 - val_loss: 0.3299 - val_accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3602 - accuracy: 0.9775 - val_loss: 0.3034 - val_accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3674 - accuracy: 0.9614 - val_loss: 0.3364 - val_accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.3565 - accuracy: 0.9633 - val_loss: 0.3654 - val_accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3423 - accuracy: 0.9651 - val_loss: 0.3127 - val_accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3577 - accuracy: 0.9676 - val_loss: 0.3160 - val_accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3599 - accuracy: 0.9710 - val_loss: 0.3496 - val_accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3672 - accuracy: 0.9596 - val_loss: 0.3281 - val_accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3511 - accuracy: 0.9631 - val_loss: 0.3171 - val_accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3393 - accuracy: 0.9762 - val_loss: 0.3578 - val_accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3552 - accuracy: 0.9489 - val_loss: 0.2939 - val_accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3494 - accuracy: 0.9654 - val_loss: 0.3093 - val_accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3365 - accuracy: 0.9701 - val_loss: 0.3730 - val_accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3414 - accuracy: 0.9579 - val_loss: 0.3122 - val_accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3305 - accuracy: 0.9688 - val_loss: 0.2852 - val_accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3393 - accuracy: 0.9610 - val_loss: 0.3294 - val_accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3515 - accuracy: 0.9574 - val_loss: 0.2980 - val_accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3268 - accuracy: 0.9875 - val_loss: 0.3028 - val_accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3443 - accuracy: 0.9799 - val_loss: 0.2909 - val_accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3385 - accuracy: 0.9676 - val_loss: 0.3183 - val_accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3218 - accuracy: 0.9707 - val_loss: 0.3248 - val_accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.3223 - accuracy: 0.9747 - val_loss: 0.2950 - val_accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "5/5 [==============================] - 0s 39ms/step - loss: 0.3117 - accuracy: 0.9787 - val_loss: 0.2973 - val_accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3136 - accuracy: 0.9778 - val_loss: 0.3212 - val_accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3165 - accuracy: 0.9735 - val_loss: 0.3106 - val_accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3266 - accuracy: 0.9523 - val_loss: 0.2767 - val_accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3244 - accuracy: 0.9787 - val_loss: 0.3190 - val_accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3424 - accuracy: 0.9599 - val_loss: 0.2916 - val_accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3032 - accuracy: 0.9835 - val_loss: 0.3006 - val_accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3208 - accuracy: 0.9772 - val_loss: 0.2743 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "tl0gfjtm1gYw",
        "outputId": "ffed3506-0098-4a8e-944d-90644f73e5f6"
      },
      "source": [
        "plt.plot(history.history['loss'], label='train')\r\n",
        "plt.plot(history.history['val_loss'], label='test')\r\n",
        "plt.legend()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bK4IoEJXSKnZ"
      },
      "source": [
        "without L2 Regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kFuWehxISN-i",
        "outputId": "033fa115-c075-47cb-8926-3e420eee4872"
      },
      "source": [
        "from sklearn import datasets\r\n",
        "import keras \r\n",
        "import tensorflow as tf\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "d=datasets.load_iris()\r\n",
        "x = d.data\r\n",
        "y = d.target\r\n",
        "\r\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.15, random_state=0)\r\n",
        "\r\n",
        "model2 =  Sequential()\r\n",
        "model2.add(Dense(100, input_shape=[4], activation='relu', kernel_regularizer='l2'))\r\n",
        "model2.add(Dense(50, activation='relu', kernel_regularizer='l2'))\r\n",
        "model2.add(Dense(3, activation='softmax', kernel_regularizer='l2'))\r\n",
        "print(model2.summary())\r\n",
        "model2.compile(loss='SparseCategoricalCrossentropy', optimizer=keras.optimizers.Adam(lr=0.001), metrics=['accuracy'])\r\n",
        "\r\n",
        "history2 = model2.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=100, batch_size=30)\r\n",
        "\r\n",
        "plt.plot(history2.history['loss'], label='train')\r\n",
        "plt.plot(history2.history['val_loss'], label='test')\r\n",
        "plt.legend()\r\n",
        "plt.show()\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 100)               500       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 50)                5050      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 153       \n",
            "=================================================================\n",
            "Total params: 5,703\n",
            "Trainable params: 5,703\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "5/5 [==============================] - 1s 44ms/step - loss: 1.9765 - accuracy: 0.3002 - val_loss: 1.7565 - val_accuracy: 0.2174\n",
            "Epoch 2/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 1.7079 - accuracy: 0.3519 - val_loss: 1.6358 - val_accuracy: 0.4783\n",
            "Epoch 3/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 1.5830 - accuracy: 0.6986 - val_loss: 1.5493 - val_accuracy: 0.5217\n",
            "Epoch 4/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 1.4754 - accuracy: 0.7106 - val_loss: 1.4755 - val_accuracy: 0.5217\n",
            "Epoch 5/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 1.3871 - accuracy: 0.6962 - val_loss: 1.4012 - val_accuracy: 0.5217\n",
            "Epoch 6/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 1.3013 - accuracy: 0.7166 - val_loss: 1.3247 - val_accuracy: 0.5217\n",
            "Epoch 7/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 1.2466 - accuracy: 0.6847 - val_loss: 1.2294 - val_accuracy: 0.6087\n",
            "Epoch 8/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 1.1652 - accuracy: 0.8160 - val_loss: 1.1564 - val_accuracy: 0.8261\n",
            "Epoch 9/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 1.1354 - accuracy: 0.9423 - val_loss: 1.0953 - val_accuracy: 0.9565\n",
            "Epoch 10/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 1.0688 - accuracy: 0.9398 - val_loss: 1.0893 - val_accuracy: 0.6087\n",
            "Epoch 11/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 1.0273 - accuracy: 0.7709 - val_loss: 1.0645 - val_accuracy: 0.5217\n",
            "Epoch 12/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.9785 - accuracy: 0.8177 - val_loss: 0.9774 - val_accuracy: 0.9565\n",
            "Epoch 13/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.9584 - accuracy: 0.9486 - val_loss: 0.9518 - val_accuracy: 0.8696\n",
            "Epoch 14/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.9128 - accuracy: 0.8838 - val_loss: 0.9957 - val_accuracy: 0.5217\n",
            "Epoch 15/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.8758 - accuracy: 0.7358 - val_loss: 0.9551 - val_accuracy: 0.6087\n",
            "Epoch 16/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.8480 - accuracy: 0.8273 - val_loss: 0.8580 - val_accuracy: 0.9565\n",
            "Epoch 17/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.8474 - accuracy: 0.9503 - val_loss: 0.8195 - val_accuracy: 1.0000\n",
            "Epoch 18/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.8266 - accuracy: 0.9492 - val_loss: 0.8616 - val_accuracy: 0.6957\n",
            "Epoch 19/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.7920 - accuracy: 0.8477 - val_loss: 0.8494 - val_accuracy: 0.6957\n",
            "Epoch 20/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.7674 - accuracy: 0.8741 - val_loss: 0.7858 - val_accuracy: 0.9565\n",
            "Epoch 21/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.7490 - accuracy: 0.9444 - val_loss: 0.7519 - val_accuracy: 0.9565\n",
            "Epoch 22/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.7301 - accuracy: 0.9370 - val_loss: 0.7902 - val_accuracy: 0.7391\n",
            "Epoch 23/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.7109 - accuracy: 0.8984 - val_loss: 0.8071 - val_accuracy: 0.6522\n",
            "Epoch 24/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.6986 - accuracy: 0.8509 - val_loss: 0.7329 - val_accuracy: 0.8261\n",
            "Epoch 25/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.6785 - accuracy: 0.9626 - val_loss: 0.6627 - val_accuracy: 1.0000\n",
            "Epoch 26/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.6644 - accuracy: 0.9685 - val_loss: 0.6739 - val_accuracy: 0.9565\n",
            "Epoch 27/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.6471 - accuracy: 0.9608 - val_loss: 0.6822 - val_accuracy: 0.9130\n",
            "Epoch 28/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.6331 - accuracy: 0.9688 - val_loss: 0.6094 - val_accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.6374 - accuracy: 0.9531 - val_loss: 0.6060 - val_accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.5960 - accuracy: 0.9765 - val_loss: 0.6406 - val_accuracy: 0.9565\n",
            "Epoch 31/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.6036 - accuracy: 0.9602 - val_loss: 0.6159 - val_accuracy: 0.9565\n",
            "Epoch 32/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.5714 - accuracy: 0.9630 - val_loss: 0.5944 - val_accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.5642 - accuracy: 0.9708 - val_loss: 0.5992 - val_accuracy: 0.9565\n",
            "Epoch 34/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.5584 - accuracy: 0.9407 - val_loss: 0.5627 - val_accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.5477 - accuracy: 0.9589 - val_loss: 0.5332 - val_accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.5438 - accuracy: 0.9796 - val_loss: 0.5452 - val_accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.5379 - accuracy: 0.9642 - val_loss: 0.5349 - val_accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.5351 - accuracy: 0.9488 - val_loss: 0.5145 - val_accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.5236 - accuracy: 0.9713 - val_loss: 0.5251 - val_accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.4944 - accuracy: 0.9642 - val_loss: 0.5132 - val_accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.4822 - accuracy: 0.9682 - val_loss: 0.4836 - val_accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.4892 - accuracy: 0.9758 - val_loss: 0.5147 - val_accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.4804 - accuracy: 0.9616 - val_loss: 0.4578 - val_accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.4765 - accuracy: 0.9787 - val_loss: 0.4859 - val_accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.4776 - accuracy: 0.9614 - val_loss: 0.5336 - val_accuracy: 0.9565\n",
            "Epoch 46/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.4577 - accuracy: 0.9606 - val_loss: 0.4518 - val_accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.4521 - accuracy: 0.9778 - val_loss: 0.4369 - val_accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.4633 - accuracy: 0.9631 - val_loss: 0.4694 - val_accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.4566 - accuracy: 0.9599 - val_loss: 0.4242 - val_accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.4485 - accuracy: 0.9833 - val_loss: 0.4358 - val_accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.4469 - accuracy: 0.9531 - val_loss: 0.4496 - val_accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.4237 - accuracy: 0.9731 - val_loss: 0.4101 - val_accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.4269 - accuracy: 0.9867 - val_loss: 0.4132 - val_accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.4305 - accuracy: 0.9789 - val_loss: 0.3835 - val_accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.4195 - accuracy: 0.9710 - val_loss: 0.4697 - val_accuracy: 0.9565\n",
            "Epoch 56/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.4227 - accuracy: 0.9588 - val_loss: 0.4080 - val_accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.4328 - accuracy: 0.9741 - val_loss: 0.3651 - val_accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.4355 - accuracy: 0.9447 - val_loss: 0.4332 - val_accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.4190 - accuracy: 0.9574 - val_loss: 0.4546 - val_accuracy: 0.9565\n",
            "Epoch 60/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.4119 - accuracy: 0.9532 - val_loss: 0.3735 - val_accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.3863 - accuracy: 0.9815 - val_loss: 0.3955 - val_accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.4011 - accuracy: 0.9691 - val_loss: 0.3766 - val_accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.4075 - accuracy: 0.9571 - val_loss: 0.3606 - val_accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.4089 - accuracy: 0.9636 - val_loss: 0.4131 - val_accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.4200 - accuracy: 0.9287 - val_loss: 0.3712 - val_accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3836 - accuracy: 0.9735 - val_loss: 0.3736 - val_accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3814 - accuracy: 0.9705 - val_loss: 0.3521 - val_accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3907 - accuracy: 0.9559 - val_loss: 0.3903 - val_accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.3826 - accuracy: 0.9619 - val_loss: 0.3535 - val_accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.4033 - accuracy: 0.9523 - val_loss: 0.3341 - val_accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3798 - accuracy: 0.9847 - val_loss: 0.4039 - val_accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.4075 - accuracy: 0.9305 - val_loss: 0.3561 - val_accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3620 - accuracy: 0.9861 - val_loss: 0.3388 - val_accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3646 - accuracy: 0.9778 - val_loss: 0.3395 - val_accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3772 - accuracy: 0.9657 - val_loss: 0.3438 - val_accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3637 - accuracy: 0.9815 - val_loss: 0.3441 - val_accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3692 - accuracy: 0.9690 - val_loss: 0.3467 - val_accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3721 - accuracy: 0.9657 - val_loss: 0.3350 - val_accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.3608 - accuracy: 0.9713 - val_loss: 0.3573 - val_accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.3610 - accuracy: 0.9673 - val_loss: 0.3486 - val_accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.3737 - accuracy: 0.9685 - val_loss: 0.3208 - val_accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3573 - accuracy: 0.9799 - val_loss: 0.3320 - val_accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3517 - accuracy: 0.9716 - val_loss: 0.3716 - val_accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3523 - accuracy: 0.9648 - val_loss: 0.3367 - val_accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3568 - accuracy: 0.9654 - val_loss: 0.3168 - val_accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3589 - accuracy: 0.9705 - val_loss: 0.3227 - val_accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3502 - accuracy: 0.9673 - val_loss: 0.3423 - val_accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3574 - accuracy: 0.9589 - val_loss: 0.3091 - val_accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.3431 - accuracy: 0.9589 - val_loss: 0.3269 - val_accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3445 - accuracy: 0.9747 - val_loss: 0.3301 - val_accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3317 - accuracy: 0.9821 - val_loss: 0.3215 - val_accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3336 - accuracy: 0.9807 - val_loss: 0.3095 - val_accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3341 - accuracy: 0.9775 - val_loss: 0.2879 - val_accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.3653 - accuracy: 0.9515 - val_loss: 0.3388 - val_accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3252 - accuracy: 0.9671 - val_loss: 0.3273 - val_accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3426 - accuracy: 0.9605 - val_loss: 0.2828 - val_accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3488 - accuracy: 0.9565 - val_loss: 0.3217 - val_accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3489 - accuracy: 0.9614 - val_loss: 0.3410 - val_accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3356 - accuracy: 0.9571 - val_loss: 0.2983 - val_accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3284 - accuracy: 0.9843 - val_loss: 0.2993 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2utq8q91_Yt"
      },
      "source": [
        "weights=model.get_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSB7Ujct2E6k",
        "outputId": "c0bf2c2a-8e27-45c6-e593-116d3541b1f4"
      },
      "source": [
        "n=len(weights)\r\n",
        "for x in range(n):\r\n",
        "  i=np.max(weights[x])\r\n",
        "  print(i)\r\n",
        "\r\n",
        "n=len(weights)\r\n",
        "for x in range(n):\r\n",
        "  i=np.min(weights[x])\r\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.30409816\n",
            "0.19127303\n",
            "0.23772149\n",
            "0.13761252\n",
            "0.41880363\n",
            "0.058261484\n",
            "-0.31776637\n",
            "-0.1041899\n",
            "-0.23814902\n",
            "-0.082195334\n",
            "-0.48008966\n",
            "-0.055038534\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR1IAlRiz4XB"
      },
      "source": [
        "After Applying L2 Regularization technique the weights has dropped"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5NWzVq61G5K",
        "outputId": "7cde0927-2f30-43a4-eec1-45f125d45e49"
      },
      "source": [
        "n=len(weights)\r\n",
        "for x in range(n):\r\n",
        "  i=np.max(weights[x])\r\n",
        "  print(i)\r\n",
        "\r\n",
        "n=len(weights)\r\n",
        "for x in range(n):\r\n",
        "  i=np.min(weights[x])\r\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.36511213\n",
            "0.14812356\n",
            "0.43386036\n",
            "0.119158156\n",
            "0.4762637\n",
            "0.05001568\n",
            "-0.3736164\n",
            "-0.12738284\n",
            "-0.4192349\n",
            "-0.077513225\n",
            "-0.5944754\n",
            "-0.07054395\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueW8DsKe2WZ5"
      },
      "source": [
        "Using Regularization Technique L2 for optimization of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xIblPAXQ1HNb",
        "outputId": "01d8c67d-9e1b-47be-e95a-1ff51613e6a2"
      },
      "source": [
        "from keras.datasets import mnist\r\n",
        "import keras\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Conv2D, Dense, MaxPooling2D, Flatten, Dropout, BatchNormalization\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n",
        "\r\n",
        "\r\n",
        "x_train = x_train.reshape(len(x_train), 28,28,1).astype('float32') / 255.0\r\n",
        "x_test = x_test.reshape(len(x_test), 28,28,1). astype('float32') / 255.0\r\n",
        "\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(Conv2D(16,3, input_shape=(28,28,1), activation='relu', ))\r\n",
        "model.add(MaxPooling2D(2,2))\r\n",
        "model.add(Conv2D(32,3, activation='relu', ))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(Dropout(0.25))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(100, activation='relu'))\r\n",
        "model.add(Dense(10, activation='softmax'))\r\n",
        "print(model.summary())\r\n",
        "\r\n",
        "model.compile(loss='SparseCategoricalCrossentropy',\r\n",
        "              optimizer=keras.optimizers.Adam(lr=0.001),\r\n",
        "              metrics=['accuracy'])\r\n",
        "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=500, verbose=1)\r\n",
        "\r\n",
        "plt.plot(history.history['loss'], label='train')\r\n",
        "plt.plot(history.history['val_loss'], label='test')\r\n",
        "plt.legend()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_8 (Conv2D)            (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 13, 13, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 11, 11, 32)        4640      \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 11, 11, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 11, 11, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 3872)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 100)               387300    \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 393,238\n",
            "Trainable params: 393,174\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "120/120 [==============================] - 24s 198ms/step - loss: 0.4389 - accuracy: 0.8629 - val_loss: 1.4296 - val_accuracy: 0.9752\n",
            "Epoch 2/10\n",
            "120/120 [==============================] - 23s 195ms/step - loss: 0.0498 - accuracy: 0.9856 - val_loss: 0.7692 - val_accuracy: 0.9856\n",
            "Epoch 3/10\n",
            "120/120 [==============================] - 24s 197ms/step - loss: 0.0280 - accuracy: 0.9919 - val_loss: 0.2560 - val_accuracy: 0.9873\n",
            "Epoch 4/10\n",
            "120/120 [==============================] - 24s 198ms/step - loss: 0.0197 - accuracy: 0.9934 - val_loss: 0.0625 - val_accuracy: 0.9875\n",
            "Epoch 5/10\n",
            "120/120 [==============================] - 24s 197ms/step - loss: 0.0131 - accuracy: 0.9960 - val_loss: 0.0325 - val_accuracy: 0.9892\n",
            "Epoch 6/10\n",
            "120/120 [==============================] - 24s 197ms/step - loss: 0.0112 - accuracy: 0.9965 - val_loss: 0.0292 - val_accuracy: 0.9905\n",
            "Epoch 7/10\n",
            "120/120 [==============================] - 24s 197ms/step - loss: 0.0082 - accuracy: 0.9974 - val_loss: 0.0320 - val_accuracy: 0.9900\n",
            "Epoch 8/10\n",
            "120/120 [==============================] - 24s 197ms/step - loss: 0.0065 - accuracy: 0.9981 - val_loss: 0.0331 - val_accuracy: 0.9901\n",
            "Epoch 9/10\n",
            "120/120 [==============================] - 24s 197ms/step - loss: 0.0056 - accuracy: 0.9981 - val_loss: 0.0343 - val_accuracy: 0.9898\n",
            "Epoch 10/10\n",
            "120/120 [==============================] - 24s 197ms/step - loss: 0.0057 - accuracy: 0.9980 - val_loss: 0.0354 - val_accuracy: 0.9908\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a9bc221f7613>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXlKBIMnfqFz",
        "outputId": "61d4d9e2-c0c1-4b39-9752-980a5f39133c"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "from keras.datasets import mnist\r\n",
        "import keras\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Conv2D, Dense, MaxPooling2D, Flatten, Dropout, BatchNormalization\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n",
        "\r\n",
        "x_train = x_train.reshape(len(x_train), 28,28,1).astype('float32') / 255.0\r\n",
        "x_test = x_test.reshape(len(x_test), 28,28,1). astype('float32') / 255.0\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(Conv2D(9,3, input_shape=(28,28, 1), activation='relu'))\r\n",
        "model.add(MaxPooling2D(2,2))\r\n",
        "model.add(Conv2D(21,3, activation='relu'))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(Dropout(0.25))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(50, activation='relu'))\r\n",
        "model.add(Dense(10, activation='softmax'))\r\n",
        "print(model.summary())\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_14 (Conv2D)           (None, 26, 26, 9)         90        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 13, 13, 9)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 11, 11, 21)        1722      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 11, 11, 21)        84        \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 11, 11, 21)        0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 2541)              0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 50)                127100    \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 10)                510       \n",
            "=================================================================\n",
            "Total params: 129,506\n",
            "Trainable params: 129,464\n",
            "Non-trainable params: 42\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQXzwQQEgC6i",
        "outputId": "33b9ea37-9c8b-4b5c-d6cd-472b7f4ccb5c"
      },
      "source": [
        "%%time\r\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\r\n",
        "history = model.fit(x_train,y_train, validation_data=(x_test, y_test), epochs=10, batch_size=800, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "75/75 [==============================] - 20s 260ms/step - loss: 27.1841 - accuracy: 0.1048 - val_loss: 27.2503 - val_accuracy: 0.0980\n",
            "Epoch 2/10\n",
            "75/75 [==============================] - 20s 261ms/step - loss: 27.2768 - accuracy: 0.1029 - val_loss: 27.2503 - val_accuracy: 0.0980\n",
            "Epoch 3/10\n",
            "75/75 [==============================] - 20s 261ms/step - loss: 27.2145 - accuracy: 0.1028 - val_loss: 27.2503 - val_accuracy: 0.0977\n",
            "Epoch 4/10\n",
            "75/75 [==============================] - 20s 261ms/step - loss: 27.3890 - accuracy: 0.1057 - val_loss: 27.2503 - val_accuracy: 0.1010\n",
            "Epoch 5/10\n",
            "75/75 [==============================] - 20s 261ms/step - loss: 27.1323 - accuracy: 0.1050 - val_loss: 27.2503 - val_accuracy: 0.1010\n",
            "Epoch 6/10\n",
            "75/75 [==============================] - 20s 264ms/step - loss: 27.2491 - accuracy: 0.1029 - val_loss: 27.2503 - val_accuracy: 0.1010\n",
            "Epoch 7/10\n",
            "75/75 [==============================] - 20s 263ms/step - loss: 27.3609 - accuracy: 0.1037 - val_loss: 27.2503 - val_accuracy: 0.1009\n",
            "Epoch 8/10\n",
            "75/75 [==============================] - 20s 261ms/step - loss: 27.3071 - accuracy: 0.1015 - val_loss: 27.2503 - val_accuracy: 0.0957\n",
            "Epoch 9/10\n",
            "75/75 [==============================] - 20s 262ms/step - loss: 27.1868 - accuracy: 0.1015 - val_loss: 27.2503 - val_accuracy: 0.0958\n",
            "Epoch 10/10\n",
            "75/75 [==============================] - 20s 260ms/step - loss: 27.3580 - accuracy: 0.0989 - val_loss: 27.2503 - val_accuracy: 0.1009\n",
            "CPU times: user 6min 4s, sys: 5.14 s, total: 6min 10s\n",
            "Wall time: 3min 16s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biLrwEsDeyMz",
        "outputId": "0c47dfef-0bed-42fa-b8ea-86d7191a439f"
      },
      "source": [
        "results = model.evaluate(x_train, y_train)\r\n",
        "print('Final test set loss: {:4f}'.format(results[0]))\r\n",
        "print('Final test set accuracy: {:4f}'.format(results[1]))\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1875/1875 [==============================] - 12s 6ms/step - loss: 27.3046 - accuracy: 0.1022\n",
            "Final test set loss: 27.304579\n",
            "Final test set accuracy: 0.102200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_DVhHmZfeG4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBb8dNdU1HUz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 754
        },
        "outputId": "a6be3515-76eb-4f58-8ca9-7f0ad324d81d"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import keras\r\n",
        "from keras.datasets import mnist\r\n",
        "from keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\r\n",
        "from keras.layers import BatchNormalization, Dropout\r\n",
        "from keras import utils\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import seaborn as sns\r\n",
        "from keras.models import Sequential\r\n",
        "\r\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\r\n",
        "\r\n",
        "\r\n",
        "# Visualizing the data\r\n",
        "for x in range(9):\r\n",
        "    plt.subplot(3,3,x+1)\r\n",
        "    plt.tight_layout()\r\n",
        "    plt.xticks([])\r\n",
        "    plt.yticks([])\r\n",
        "    plt.title(y_train[x])\r\n",
        "    plt.imshow(x_train[x])\r\n",
        "\r\n",
        "#Preprocessing\r\n",
        "x_train = x_train.reshape(-1,28,28,1).astype('float32')/255.0\r\n",
        "x_test = x_test.reshape(-1,28,28,1).astype('float32')/255.0\r\n",
        "y_train = utils.to_categorical(y_train)\r\n",
        "y_test = utils.to_categorical(y_test)\r\n",
        "\r\n",
        "model=Sequential()\r\n",
        "model.add(Conv2D(30,3,input_shape=(28,28,1), padding='same', activation='relu'))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(Conv2D(40,3,padding='same', activation='relu'))\r\n",
        "model.add(MaxPooling2D(2,2))\r\n",
        "model.add(Conv2D(50,3,padding='same', activation='relu'))\r\n",
        "model.add(MaxPooling2D(2,2))\r\n",
        "model.add(Dropout(0.25))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(50, activation='relu'))\r\n",
        "model.add(Dense(10, activation='softmax'))\r\n",
        "model.summary()\r\n",
        "\r\n",
        "# set up learning rate decay\r\n",
        "lr_decay = tf.keras.callbacks.LearningRateScheduler(\r\n",
        "    lambda epoch: LEARNING_RATE * LEARNING_RATE_EXP_DECAY**epoch,\r\n",
        "    verbose=True)\r\n",
        "\r\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy',\r\n",
        "              metrics=['accuracy'])\r\n",
        "history= model.fit(x_train, y_train, epochs=3,batch_size=200,\r\n",
        "                   callbacks=[lr_decay], verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 28, 28, 30)        300       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 28, 28, 30)        120       \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 28, 28, 40)        10840     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 40)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 14, 14, 50)        18050     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 50)          0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 7, 7, 50)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2450)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 50)                122550    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                510       \n",
            "=================================================================\n",
            "Total params: 152,370\n",
            "Trainable params: 152,310\n",
            "Non-trainable params: 60\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "i2wJVHOYiCUJ",
        "outputId": "12db9bfd-de81-4d5e-c797-f2497b9795cd"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy',\r\n",
        "              metrics=['accuracy'])\r\n",
        "history= model.fit(x_train, y_train, epochs=3,batch_size=200,\r\n",
        "                   callbacks=[lr_decay], verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_begin\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1904\u001b[0m       \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1905\u001b[0;31m       \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1906\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Support for old API for backward compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: <lambda>() takes 1 positional argument but 2 were given",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-cf9daa9bdd79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m      3\u001b[0m history= model.fit(x_train, y_train, epochs=3,batch_size=200,\n\u001b[0;32m----> 4\u001b[0;31m                    callbacks=[lr_decay], verbose=1)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1088\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1091\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_begin\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m           \u001b[0mnumpy_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_begin\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1905\u001b[0m       \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1906\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Support for old API for backward compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1907\u001b[0;31m       \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1908\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m       raise ValueError('The output of the \"schedule\" function '\n",
            "\u001b[0;32m<ipython-input-3-c9477284a158>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# set up learning rate decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m lr_decay = tf.keras.callbacks.LearningRateScheduler(\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mlambda\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mLEARNING_RATE_EXP_DECAY\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     verbose=True)\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'LEARNING_RATE' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf1Cmjca1Hh-"
      },
      "source": [
        "a=model.evaluate(x_train,y_train)\r\n",
        "print('Loss Function:',round(a[0],2)*100)\r\n",
        "print('Accuracy of Training:',round(a[1], 2)*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBeoe8Al1Hb3"
      },
      "source": [
        "plt.plot(history.history['loss'], label='train')\r\n",
        "plt.plot(history.history['val_loss'], label='test')\r\n",
        "plt.legend()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2bDF99SIQ3dE",
        "outputId": "a4395e29-d7e0-4081-b56e-7812288584b1"
      },
      "source": [
        "# importing the pandas library \r\n",
        "import pandas as pd \r\n",
        "from sklearn import datasets\r\n",
        "from collections import Counter\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import tensorflow as tf\r\n",
        "import keras\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras import utils\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "iris=datasets.load_iris()\r\n",
        "X=np.array(iris.data)\r\n",
        "Y=np.array(iris.target)\r\n",
        "\r\n",
        "\r\n",
        "x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2)\r\n",
        "\r\n",
        "model=Sequential()\r\n",
        "model.add(Dense(50, input_shape= [4], activation='relu', kernel_regularizer='l2'))\r\n",
        "model.add(Dense(10, activation='relu'))\r\n",
        "model.add(Dense(3, activation='sigmoid'))\r\n",
        "print(model.summary())\r\n",
        "\r\n",
        "model.compile(optimizer='adam', loss='SparseCategoricalCrossentropy',\r\n",
        "              metrics=['accuracy'])\r\n",
        "\r\n",
        "history = model.fit(x_train, y_train, epochs=30, validation_data=(x_train, y_train), batch_size=4, verbose=1)\r\n",
        "\r\n",
        "results = model.evaluate(x_test, y_test)\r\n",
        "\r\n",
        "print('Loss Function:',round(results[0],2)*100,'%')\r\n",
        "print('Accuracy of Training:',round(results[1], 2)*100,'%')\r\n",
        "\r\n",
        "\r\n",
        "# plot loss during training\r\n",
        "plt.subplot(211)\r\n",
        "plt.title('Loss')\r\n",
        "plt.plot(history.history['loss'], label='train')\r\n",
        "plt.plot(history.history['val_loss'], label='test')\r\n",
        "plt.legend()\r\n",
        "# plot accuracy during training\r\n",
        "plt.subplot(212)\r\n",
        "plt.title('Accuracy')\r\n",
        "plt.plot(history.history['accuracy'], label='train')\r\n",
        "plt.plot(history.history['val_accuracy'], label='test')\r\n",
        "plt.legend()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 50)                250       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                510       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 33        \n",
            "=================================================================\n",
            "Total params: 793\n",
            "Trainable params: 793\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/30\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 1.2008 - accuracy: 0.3703 - val_loss: 1.1362 - val_accuracy: 0.3333\n",
            "Epoch 2/30\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.0992 - accuracy: 0.4867 - val_loss: 1.0781 - val_accuracy: 0.3583\n",
            "Epoch 3/30\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.0774 - accuracy: 0.3242 - val_loss: 1.0176 - val_accuracy: 0.5833\n",
            "Epoch 4/30\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.0179 - accuracy: 0.4945 - val_loss: 0.9676 - val_accuracy: 0.8167\n",
            "Epoch 5/30\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.9623 - accuracy: 0.8195 - val_loss: 0.9144 - val_accuracy: 0.8500\n",
            "Epoch 6/30\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.8998 - accuracy: 0.8114 - val_loss: 0.8586 - val_accuracy: 0.9000\n",
            "Epoch 7/30\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.8470 - accuracy: 0.8173 - val_loss: 0.7914 - val_accuracy: 0.9000\n",
            "Epoch 8/30\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7836 - accuracy: 0.7569 - val_loss: 0.7354 - val_accuracy: 0.8083\n",
            "Epoch 9/30\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7273 - accuracy: 0.8098 - val_loss: 0.6503 - val_accuracy: 0.9333\n",
            "Epoch 10/30\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6479 - accuracy: 0.8947 - val_loss: 0.5896 - val_accuracy: 0.9417\n",
            "Epoch 11/30\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.5784 - accuracy: 0.8728 - val_loss: 0.5479 - val_accuracy: 0.8667\n",
            "Epoch 12/30\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5289 - accuracy: 0.9132 - val_loss: 0.4928 - val_accuracy: 0.9417\n",
            "Epoch 13/30\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5117 - accuracy: 0.9011 - val_loss: 0.4517 - val_accuracy: 0.9583\n",
            "Epoch 14/30\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4546 - accuracy: 0.9571 - val_loss: 0.4202 - val_accuracy: 0.9583\n",
            "Epoch 15/30\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3836 - accuracy: 0.9683 - val_loss: 0.3924 - val_accuracy: 0.9667\n",
            "Epoch 16/30\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.4062 - accuracy: 0.9641 - val_loss: 0.3686 - val_accuracy: 0.9667\n",
            "Epoch 17/30\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3679 - accuracy: 0.9859 - val_loss: 0.3485 - val_accuracy: 0.9583\n",
            "Epoch 18/30\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3308 - accuracy: 0.9695 - val_loss: 0.3326 - val_accuracy: 0.9583\n",
            "Epoch 19/30\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3145 - accuracy: 0.9746 - val_loss: 0.3185 - val_accuracy: 0.9583\n",
            "Epoch 20/30\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3126 - accuracy: 0.9382 - val_loss: 0.3041 - val_accuracy: 0.9583\n",
            "Epoch 21/30\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3155 - accuracy: 0.9623 - val_loss: 0.2961 - val_accuracy: 0.9583\n",
            "Epoch 22/30\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2977 - accuracy: 0.9595 - val_loss: 0.2761 - val_accuracy: 0.9667\n",
            "Epoch 23/30\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2716 - accuracy: 0.9661 - val_loss: 0.2648 - val_accuracy: 0.9667\n",
            "Epoch 24/30\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2756 - accuracy: 0.9604 - val_loss: 0.2525 - val_accuracy: 0.9667\n",
            "Epoch 25/30\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2472 - accuracy: 0.9571 - val_loss: 0.2598 - val_accuracy: 0.9500\n",
            "Epoch 26/30\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2700 - accuracy: 0.9664 - val_loss: 0.2401 - val_accuracy: 0.9667\n",
            "Epoch 27/30\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2428 - accuracy: 0.9672 - val_loss: 0.2296 - val_accuracy: 0.9667\n",
            "Epoch 28/30\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2386 - accuracy: 0.9677 - val_loss: 0.2229 - val_accuracy: 0.9667\n",
            "Epoch 29/30\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2250 - accuracy: 0.9625 - val_loss: 0.2529 - val_accuracy: 0.9333\n",
            "Epoch 30/30\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2949 - accuracy: 0.8919 - val_loss: 0.2100 - val_accuracy: 0.9750\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.1567 - accuracy: 1.0000\n",
            "Loss Function: 16.0 %\n",
            "Accuracy of Training: 100.0 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzZMFxAAUOP1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c058eeb3-f8ce-4edf-b20a-2cf4d7bf886c"
      },
      "source": [
        "import os, re, time, json\r\n",
        "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "print(\"Tensorflow version \" + tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version 2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8E0W_Wfn0_p"
      },
      "source": [
        "#@title visualization utilities [RUN ME]\r\n",
        "\"\"\"\r\n",
        "This cell contains helper functions used for visualization\r\n",
        "and downloads only. You can skip reading it. There is very\r\n",
        "little useful Keras/Tensorflow code here.\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "# Matplotlib config\r\n",
        "plt.rc('image', cmap='gray_r')\r\n",
        "plt.rc('grid', linewidth=0)\r\n",
        "plt.rc('xtick', top=False, bottom=False, labelsize='large')\r\n",
        "plt.rc('ytick', left=False, right=False, labelsize='large')\r\n",
        "plt.rc('axes', facecolor='F8F8F8', titlesize=\"large\", edgecolor='white')\r\n",
        "plt.rc('text', color='a8151a')\r\n",
        "plt.rc('figure', facecolor='F0F0F0')# Matplotlib fonts\r\n",
        "MATPLOTLIB_FONT_DIR = os.path.join(os.path.dirname(plt.__file__), \"mpl-data/fonts/ttf\")\r\n",
        "\r\n",
        "# pull a batch from the datasets. This code is not very nice, it gets much better in eager mode (TODO)\r\n",
        "def dataset_to_numpy_util(training_dataset, validation_dataset, N):\r\n",
        "  \r\n",
        "  # get one batch from each: 10000 validation digits, N training digits\r\n",
        "  batch_train_ds = training_dataset.unbatch().batch(N)\r\n",
        "  \r\n",
        "  # eager execution: loop through datasets normally\r\n",
        "  for validation_digits, validation_labels in validation_dataset:\r\n",
        "    validation_digits = validation_digits.numpy()\r\n",
        "    validation_labels = validation_labels.numpy()\r\n",
        "    break\r\n",
        "  for training_digits, training_labels in batch_train_ds:\r\n",
        "    training_digits = training_digits.numpy()\r\n",
        "    training_labels = training_labels.numpy()\r\n",
        "    break\r\n",
        "  \r\n",
        "  # these were one-hot encoded in the dataset\r\n",
        "  validation_labels = np.argmax(validation_labels, axis=1)\r\n",
        "  training_labels = np.argmax(training_labels, axis=1)\r\n",
        "  \r\n",
        "  return (training_digits, training_labels,\r\n",
        "          validation_digits, validation_labels)\r\n",
        "\r\n",
        "# create digits from local fonts for testing\r\n",
        "def create_digits_from_local_fonts(n):\r\n",
        "  font_labels = []\r\n",
        "  img = PIL.Image.new('LA', (28*n, 28), color = (0,255)) # format 'LA': black in channel 0, alpha in channel 1\r\n",
        "  font1 = PIL.ImageFont.truetype(os.path.join(MATPLOTLIB_FONT_DIR, 'DejaVuSansMono-Oblique.ttf'), 25)\r\n",
        "  font2 = PIL.ImageFont.truetype(os.path.join(MATPLOTLIB_FONT_DIR, 'STIXGeneral.ttf'), 25)\r\n",
        "  d = PIL.ImageDraw.Draw(img)\r\n",
        "  for i in range(n):\r\n",
        "    font_labels.append(i%10)\r\n",
        "    d.text((7+i*28,0 if i<10 else -4), str(i%10), fill=(255,255), font=font1 if i<10 else font2)\r\n",
        "  font_digits = np.array(img.getdata(), np.float32)[:,0] / 255.0 # black in channel 0, alpha in channel 1 (discarded)\r\n",
        "  font_digits = np.reshape(np.stack(np.split(np.reshape(font_digits, [28, 28*n]), n, axis=1), axis=0), [n, 28*28])\r\n",
        "  return font_digits, font_labels\r\n",
        "\r\n",
        "# utility to display a row of digits with their predictions\r\n",
        "def display_digits(digits, predictions, labels, title, n):\r\n",
        "  plt.figure(figsize=(13,3))\r\n",
        "  digits = np.reshape(digits, [n, 28, 28])\r\n",
        "  digits = np.swapaxes(digits, 0, 1)\r\n",
        "  digits = np.reshape(digits, [28, 28*n])\r\n",
        "  plt.yticks([])\r\n",
        "  plt.xticks([28*x+14 for x in range(n)], predictions)\r\n",
        "  for i,t in enumerate(plt.gca().xaxis.get_ticklabels()):\r\n",
        "    if predictions[i] != labels[i]: t.set_color('red') # bad predictions in red\r\n",
        "  plt.imshow(digits)\r\n",
        "  plt.grid(None)\r\n",
        "  plt.title(title)\r\n",
        "  \r\n",
        "# utility to display multiple rows of digits, sorted by unrecognized/recognized status\r\n",
        "def display_top_unrecognized(digits, predictions, labels, n, lines):\r\n",
        "  idx = np.argsort(predictions==labels) # sort order: unrecognized first\r\n",
        "  for i in range(lines):\r\n",
        "    display_digits(digits[idx][i*n:(i+1)*n], predictions[idx][i*n:(i+1)*n], labels[idx][i*n:(i+1)*n],\r\n",
        "                   \"{} sample validation digits out of {} with bad predictions in red and sorted first\".format(n*lines, len(digits)) if i==0 else \"\", n)\r\n",
        "    \r\n",
        "# utility to display training and validation curves\r\n",
        "def display_training_curves(training, validation, title, subplot):\r\n",
        "  if subplot%10==1: # set up the subplots on the first call\r\n",
        "    plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\r\n",
        "    plt.tight_layout()\r\n",
        "  ax = plt.subplot(subplot)\r\n",
        "  ax.grid(linewidth=1, color='white')\r\n",
        "  ax.plot(training)\r\n",
        "  ax.plot(validation)\r\n",
        "  ax.set_title('model '+ title)\r\n",
        "  ax.set_ylabel(title)\r\n",
        "  ax.set_xlabel('epoch')\r\n",
        "  ax.legend(['train', 'valid.'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgN-pcp_l7s7",
        "outputId": "45ffbb6a-d04e-4754-98e2-600334d86f35"
      },
      "source": [
        "try:\r\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\r\n",
        "  tf.config.experimental_connect_to_cluster(tpu)\r\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\r\n",
        "  strategy = tf.distribute.experimental.TPUStrategy(tpu)\r\n",
        "except ValueError:\r\n",
        "  strategy = tf.distribute.MirroredStrategy()\r\n",
        "\r\n",
        "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
            "Number of accelerators:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1Kuvt0tlzgs",
        "outputId": "9e75da71-528a-420b-f697-370a58454164"
      },
      "source": [
        "try: # detect TPUs\r\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\r\n",
        "  tf.config.experimental_connect_to_cluster(tpu)\r\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\r\n",
        "  strategy = tf.distribute.experimental.TPUStrategy(tpu)\r\n",
        "except ValueError: # detect GPUs\r\n",
        "  strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\r\n",
        "  #strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\r\n",
        "  #strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() # for clusters of multi-GPU machines\r\n",
        "\r\n",
        "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
            "Number of accelerators:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xB4RdEtGiyU"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import keras\r\n",
        "from keras.datasets import mnist\r\n",
        "from keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\r\n",
        "from keras.layers import BatchNormalization, Dropout\r\n",
        "from keras import utils\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import seaborn as sns\r\n",
        "from keras.models import Sequential\r\n",
        "\r\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\r\n",
        "\r\n",
        "\r\n",
        "# Visualizing the data\r\n",
        "for x in range(9):\r\n",
        "    plt.subplot(3,3,x+1)\r\n",
        "    plt.tight_layout()\r\n",
        "    plt.xticks([])\r\n",
        "    plt.yticks([])\r\n",
        "    plt.title(y_train[x])\r\n",
        "    plt.imshow(x_train[x])\r\n",
        "\r\n",
        "#Preprocessing\r\n",
        "x_train = x_train.reshape(-1,28,28,1).astype('float32')/255.0\r\n",
        "x_test = x_test.reshape(-1,28,28,1).astype('float32')/255.0\r\n",
        "y_train = utils.to_categorical(y_train)\r\n",
        "y_test = utils.to_categorical(y_test)\r\n",
        "\r\n",
        "model=Sequential()\r\n",
        "model.add(Conv2D(16,3,input_shape=(28,28,1), padding='same', kernel_regularizer='l2', activation='relu'))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(Conv2D(32,3, activation='relu'))\r\n",
        "model.add(MaxPooling2D(2,2))\r\n",
        "model.add(Conv2D(64,3, padding='same', activation='relu'))\r\n",
        "model.add(MaxPooling2D(2,2))\r\n",
        "model.add(Conv2D(128,3,activation='relu'))\r\n",
        "model.add(Conv2D(256,3, padding='same', activation='relu'))\r\n",
        "model.add(MaxPooling2D(2,2))\r\n",
        "model.add(Dropout(0.25))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(100, activation='relu'))\r\n",
        "model.add(Dense(50, activation='relu'))\r\n",
        "model.add(Dense(10, activation='softmax'))\r\n",
        "model.summary()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy',\r\n",
        "              metrics=['accuracy'])\r\n",
        "history= model.fit(x_train, y_train, epochs=50,batch_size=500,validation_data=(x_test, y_test), verbose=1)\r\n",
        "\r\n",
        "training=model.evaluate(x_train,y_train)\r\n",
        "print('Training Loss Function:',round(training[0],2)*100)\r\n",
        "print('Accuracy of Training:',round(training[1], 2)*100)\r\n",
        "\r\n",
        "testing=model.evaluate(x_test,y_test)\r\n",
        "print('Testing Loss Function:',round(testing[0],2)*100)\r\n",
        "print('Accuracy of Testing:',round(testing[1], 2)*100)\r\n",
        "\r\n",
        "plt.plot(history.history['loss'], label='train')\r\n",
        "plt.plot(history.history['val_loss'], label='test')\r\n",
        "plt.legend()\r\n",
        "plt.show()\r\n",
        "\r\n",
        "from sklearn.metrics import classification_report,confusion_matrix\r\n",
        "# Evaluating the Accuracy and Loss\r\n",
        "# print('loss Function: ', round(model.evaluate(x_train,y_train, verbose=1)[0],2)*100,'%')\r\n",
        "# print('Accuracy Funciton: ',round(model.evaluate(x_train,y_train, verbose=1)[1],2)*100,'%')\r\n",
        "# print('Validaiton Loss function', round(model.evaluate(x_test, y_test, verbose=1)[0], 2)*100,'%')\r\n",
        "# print('Accuracy of Validation set', round(model.evaluate(x_test, y_test, verbose=1)[1], 2)*100, '%')\r\n",
        "\r\n",
        "# Instead of probabilities it provides class labels\r\n",
        "y_pred_classes = model.predict_classes(x_test)\r\n",
        "\r\n",
        "# Reverting one-hot encoding on true validation output labels\r\n",
        "y_test_classes = np.argmax(y_test,axis=1)\r\n",
        "print(\"################ CLASSIFICATION REPORT ################\")\r\n",
        "print(classification_report(y_test_classes,y_pred_classes),\"\\n\\n\")\r\n",
        "print(\"################ CONFUSION MATRIX ################\")\r\n",
        "plt.figure(figsize=(8,8))\r\n",
        "sns.heatmap(confusion_matrix(y_test_classes,y_pred_classes),linewidths=.5,cmap=\"YlGnBu\",annot=True,cbar=False,fmt='d')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "\r\n",
        "print('done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOp3sDJHGj3i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37N6P969Gj-A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-REKk7wpGkHU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QGsYjjCGkM1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}