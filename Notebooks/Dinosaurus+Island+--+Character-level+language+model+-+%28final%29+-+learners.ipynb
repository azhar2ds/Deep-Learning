{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character level language model - Dinosaurus land\n",
    "\n",
    "Welcome to Dinosaurus Island! 65 million years ago, dinosaurs existed, and in this assignment they are back. You are in charge of a special task. Leading biology researchers are creating new breeds of dinosaurs and bringing them to life on earth, and your job is to give names to these dinosaurs. If a dinosaur does not like its name, it might go beserk, so choose wisely! \n",
    "\n",
    "<table>\n",
    "<td>\n",
    "<img src=\"images/dino.jpg\" style=\"width:250;height:300px;\">\n",
    "\n",
    "</td>\n",
    "\n",
    "</table>\n",
    "\n",
    "Luckily you have learned some deep learning and you will use it to save the day. Your assistant has collected a list of all the dinosaur names they could find, and compiled them into this [dataset](dinos.txt). (Feel free to take a look by clicking the previous link.) To create new dinosaur names, you will build a character level language model to generate new names. Your algorithm will learn the different name patterns, and randomly generate new names. Hopefully this algorithm will keep you and your team safe from the dinosaurs' wrath! \n",
    "\n",
    "By completing this assignment you will learn:\n",
    "\n",
    "- How to store text data for processing using an RNN \n",
    "- How to synthesize data, by sampling predictions at each time step and passing it to the next RNN-cell unit\n",
    "- How to build a character-level text generation recurrent neural network\n",
    "- Why clipping the gradients is important\n",
    "\n",
    "We will begin by loading in some functions that we have provided for you in `rnn_utils`. Specifically, you have access to functions such as `rnn_forward` and `rnn_backward` which are equivalent to those you've implemented in the previous assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import *\n",
    "import random\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1 - Problem Statement\n",
    "\n",
    "### 1.1 - Dataset and Preprocessing\n",
    "\n",
    "Run the following cell to read the dataset of dinosaur names, create a list of unique characters (such as a-z), and compute the dataset and vocabulary size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = open('dinos.txt', 'r').read()\n",
    "data= data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The characters are a-z (26 characters) plus the \"\\n\" (or newline character), which in this assignment plays a role similar to the `<EOS>` (or \"End of sentence\") token we had discussed in lecture, only here it indicates the end of the dinosaur name rather than the end of a sentence. In the cell below, we create a python dictionary (i.e., a hash table) to map each character to an index from 0-26. We also create a second python dictionary that maps each index back to the corresponding character character. This will help you figure out what index corresponds to what character in the probability distribution output of the softmax layer. Below, `char_to_ix` and `ix_to_char` are the python dictionaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Overview of the model\n",
    "\n",
    "Your model will have the following structure: \n",
    "\n",
    "- Initialize parameters \n",
    "- Run the optimization loop\n",
    "    - Forward propagation to compute the loss function\n",
    "    - Backward propagation to compute the gradients with respect to the loss function\n",
    "    - Clip the gradients to avoid exploding gradients\n",
    "    - Using the gradients, update your parameter with the gradient descent update rule.\n",
    "- Return the learned parameters \n",
    "    \n",
    "<img src=\"images/rnn.png\" style=\"width:450;height:300px;\">\n",
    "<caption><center> **Figure 1**: Recurrent Neural Network, similar to what you had built in the previous notebook \"Building a RNN - Step by Step\".  </center></caption>\n",
    "\n",
    "At each time-step, the RNN tries to predict what is the next character given the previous characters. The dataset $X = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$ is a list of characters in the training set, while $Y = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$ is such that at every time-step $t$, we have $y^{\\langle t \\rangle} = x^{\\langle t+1 \\rangle}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Building blocks of the model\n",
    "\n",
    "In this part, you will build two important blocks of the overall model:\n",
    "- Gradient clipping: to avoid exploding gradients\n",
    "- Sampling: a technique used to generate characters\n",
    "\n",
    "You will then apply these two functions to build the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Clipping the gradients in the optimization loop\n",
    "\n",
    "In this section you will implement the `clip` function that you will call inside of your optimization loop. Recall that your overall loop structure usually consists of a forward pass, a cost computation, a backward pass, and a parameter update. Before updating the parameters, you will perform gradient clipping when needed to make sure that your gradients are not \"exploding,\" meaning taking on overly large values. \n",
    "\n",
    "In the exercise below, you will implement a function `clip` that takes in a dictionary of gradients and returns a clipped version of gradients if needed. There are different ways to clip gradients; we will use a simple element-wise clipping procedure, in which every element of the gradient vector is clipped to lie between some range [-N, N]. More generally, you will provide a `maxValue` (say 10). In this example, if any component of the gradient vector is greater than 10, it would be set to 10; and if any component of the gradient vector is less than -10, it would be set to -10. If it is between -10 and 10, it is left alone. \n",
    "\n",
    "<img src=\"images/clip.png\" style=\"width:400;height:150px;\">\n",
    "<caption><center> **Figure 2**: Visualization of gradient descent with and without gradient clipping, in a case where the network is running into slight \"exploding gradient\" problems. </center></caption>\n",
    "\n",
    "**Exercise**: Implement the function below to return the clipped gradients of your dictionary `gradients`. Your function takes in a maximum threshold and returns the clipped versions of your gradients. You can check out this [hint](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.clip.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED FUNCTION: clip\n",
    "\n",
    "def clip(gradients, maxValue):\n",
    "    '''\n",
    "    Clips the gradients' values between minimum and maximum.\n",
    "    \n",
    "    Arguments:\n",
    "    gradients -- a dictionary containing the gradients \"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\"\n",
    "    maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue\n",
    "    \n",
    "    Returns: \n",
    "    gradients -- a dictionary with the clipped gradients.\n",
    "    '''\n",
    "    \n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "   \n",
    "    ### START CODE HERE ###\n",
    "    # clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby]. (â‰ˆ2 lines)\n",
    "    for gradient in [dWax, dWaa, dWya, db, dby]:\n",
    "        None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "dWax = np.random.randn(5,3)*10\n",
    "dWaa = np.random.randn(5,5)*10\n",
    "dWya = np.random.randn(2,5)*10\n",
    "db = np.random.randn(5,1)*10\n",
    "dby = np.random.randn(2,1)*10\n",
    "gradients = {\"dWax\": dWax, \"dWaa\": dWaa, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "gradients = clip(gradients, 10)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
    "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
    "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
    "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected output:**\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td> \n",
    "    **gradients[\"dWaa\"][1][2] **\n",
    "    </td>\n",
    "    <td> \n",
    "    10.0\n",
    "    </td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "    <td> \n",
    "    **gradients[\"dWax\"][3][1]**\n",
    "    </td>\n",
    "    <td> \n",
    "    -10.0\n",
    "    </td>\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **gradients[\"dWya\"][1][2]**\n",
    "    </td>\n",
    "    <td> \n",
    "0.29713815361\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **gradients[\"db\"][4]**\n",
    "    </td>\n",
    "    <td> \n",
    "[ 10.]\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **gradients[\"dby\"][1]**\n",
    "    </td>\n",
    "    <td> \n",
    "[ 8.45833407]\n",
    "    </td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Sampling\n",
    "\n",
    "Now assume that your model is trained. You would like to generate new text (characters). The process of generation is explained in the picture below:\n",
    "\n",
    "<img src=\"images/dinos3.png\" style=\"width:500;height:300px;\">\n",
    "<caption><center> **Figure 3**: In this picture, we assume the model is already trained. We pass in $x^{\\langle 1\\rangle} = \\vec{0}$ at the first time step, and have the network then sample one character at a time. </center></caption>\n",
    "\n",
    "**Exercise**: Implement the `sample` function below to sample characters. You need to carry out 4 steps:\n",
    "\n",
    "- **Step 1**: Pass the network the first \"dummy\" input $x^{\\langle 1 \\rangle} = \\vec{0}$ (the vector of zeros). This is the default input before we've generated any characters. We also set $a^{\\langle 0 \\rangle} = \\vec{0}$\n",
    "\n",
    "- **Step 2**: Run one step of forward propagation to get $a^{\\langle 1 \\rangle}$ and $\\hat{y}^{\\langle 1 \\rangle}$. Here are the equations:\n",
    "\n",
    "$$ a^{\\langle t+1 \\rangle} = \\tanh(W_{ax}  x^{\\langle t \\rangle } + W_{aa} a^{\\langle t \\rangle } + b)\\tag{1}$$\n",
    "\n",
    "$$ z^{\\langle t + 1 \\rangle } = W_{ya}  a^{\\langle t + 1 \\rangle } + b_y \\tag{2}$$\n",
    "\n",
    "$$ \\hat{y}^{\\langle t+1 \\rangle } = softmax(z^{\\langle t + 1 \\rangle })\\tag{3}$$\n",
    "\n",
    "Note that $\\hat{y}^{\\langle t+1 \\rangle }$ is a (softmax) probability vector (its entries are between 0 and 1 and sum to 1). $\\hat{y}^{\\langle t+1 \\rangle}_i$ represents the probability that the character indexed by \"i\" is the next character.  We have provided a `softmax()` function that you can use.\n",
    "\n",
    "- **Step 3**: Carry out sampling: Pick the next character's index according to the probability distribution specified by $\\hat{y}^{\\langle t+1 \\rangle }$. This means that if $\\hat{y}^{\\langle t+1 \\rangle }_i = 0.16$, you will pick the index \"i\" with 16% probability. To implement it, you can use [`np.random.choice`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.random.choice.html).\n",
    "\n",
    "Here is an example of how to use `np.random.choice()`:\n",
    "```python\n",
    "np.random.seed(0)\n",
    "p = np.array([0.1, 0.0, 0.7, 0.2])\n",
    "index = np.random.choice([0, 1, 2, 3], p = p.ravel())\n",
    "```\n",
    "This means that you will pick the `index` according to the distribution: \n",
    "$P(index = 0) = 0.1, P(index = 1) = 0.0, P(index = 2) = 0.7, P(index = 3) = 0.2$.\n",
    "\n",
    "- **Step 4**: The last step to implement in `sample()` is to overwrite the variable `x`, which currently stores $x^{\\langle t \\rangle }$, with the value of $x^{\\langle t + 1 \\rangle }$. You will represent $x^{\\langle t + 1 \\rangle }$ by creating a one-hot vector corresponding to the character you've chosen as your prediction. You will then forward propagate $x^{\\langle t + 1 \\rangle }$ in Step 1 and keep repeating the process until you get a \"\\n\" character, indicating you've reached the end of the dinosaur name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sample\n",
    "\n",
    "def sample(parameters, char_to_ix, seed):\n",
    "    \"\"\"\n",
    "    Sample a sequence of characters according to a sequence of probability distributions output of the RNN\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing the parameters Waa, Wax, Wya, by, and b. \n",
    "    char_to_ix -- python dictionary mapping each character to an index.\n",
    "    seed -- used for grading purposes. Do not worry about it.\n",
    "\n",
    "    Returns:\n",
    "    indices -- a list of length n containing the indexes of the sampled characters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve parameters and relevant shapes from \"parameters\" dictionary\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Create the one-hot vector x for the first character (initializing the sequence generation). (â‰ˆ1 line)\n",
    "    x = None\n",
    "    # Step 1': Initialize a_prev as zeros (â‰ˆ1 line)\n",
    "    a_prev = None\n",
    "    \n",
    "    # Create an empty list of indices, this is the list which will contain the list of indexes of the characters to generate (â‰ˆ1 line)\n",
    "    indices = None\n",
    "    \n",
    "    # Idx is a flag to detect a newline character, we initialize it to -1\n",
    "    idx = -1 \n",
    "    \n",
    "    # Loop over time-steps t. At each time-step, sample a character from a probability distribution and append \n",
    "    # its index to \"indexes\". We'll stop if we reach 50 characters (which should be very unlikely with a well \n",
    "    # trained model), which helps debugging and prevents entering an infinite loop. \n",
    "    counter = 0\n",
    "    newline_character = char_to_ix['\\n']\n",
    "    \n",
    "    while (idx != newline_character and counter != 50):\n",
    "        \n",
    "        # Step 2: Forward propagate x using the equations (1), (2) and (3)\n",
    "        a = None\n",
    "        z = None\n",
    "        y = None\n",
    "        \n",
    "        # for grading purposes\n",
    "        np.random.seed(counter+seed) \n",
    "        \n",
    "        # Step 3: Sample the index of a character within the vocabulary from the probability distribution y\n",
    "        idx = None\n",
    "\n",
    "        # Append the index to \"indices\"\n",
    "        None\n",
    "        \n",
    "        # Step 4: Overwrite the input character as the one corresponding to the sampled index.\n",
    "        x = None\n",
    "        x[None] = None\n",
    "        \n",
    "        # Update \"a_prev\" to be \"a\"\n",
    "        a_prev = None\n",
    "        \n",
    "        # for grading purposes\n",
    "        seed += 1\n",
    "        counter +=1\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    if (counter == 50):\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "n, n_a = 20, 100\n",
    "a0 = np.random.randn(n_a, 1)\n",
    "i0 = 1 # first character is ix_to_char[i0]\n",
    "Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
    "b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
    "\n",
    "\n",
    "indexes = sample(parameters, char_to_ix, 0)\n",
    "print(\"Sampling:\")\n",
    "print(\"list of sampled indices:\", indexes)\n",
    "print(\"list of sampled characters:\", [ix_to_char[i] for i in indexes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected output:**\n",
    "<table>\n",
    "<tr>\n",
    "    <td> \n",
    "    **list of sampled indices:**\n",
    "    </td>\n",
    "    <td> \n",
    "    [18, 2, 26, 0]\n",
    "    </td>\n",
    "    </tr><tr>\n",
    "    <td> \n",
    "    **list of sampled characters:**\n",
    "    </td>\n",
    "    <td> \n",
    "    ['r', 'b', 'z', '\\n']\n",
    "    </td>\n",
    "    \n",
    "        \n",
    "    \n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Building the language model \n",
    "\n",
    "It is time to build the character-level language model for text generation. \n",
    "\n",
    "\n",
    "### 3.1 - Gradient descent \n",
    "\n",
    "In this section you will implement a function performing one step of stochastic gradient descent (with clipped gradients). You will go through the training examples one at a time, so the optimization algorithm will be stochastic gradient descent. As a reminder, here are the steps of a common optimization loop for an RNN:\n",
    "\n",
    "- Forward propagate through the RNN to compute the loss\n",
    "- Backward propagate through time to compute the gradients of the loss with respect to the parameters\n",
    "- Clip the gradients if necessary \n",
    "- Update your parameters using gradient descent \n",
    "\n",
    "**Exercise**: Implement this optimization process (one step of stochastic gradient descent). \n",
    "\n",
    "We provide you with the following functions: \n",
    "\n",
    "```python\n",
    "def rnn_forward(X, Y, a_prev, parameters):\n",
    "    \"\"\" Performs the forward propagation through the RNN and computes the cross-entropy loss.\n",
    "    It returns the loss' value as well as a \"cache\" storing values to be used in the backpropagation.\"\"\"\n",
    "    ....\n",
    "    return loss, cache\n",
    "    \n",
    "def rnn_backward(X, Y, parameters, cache):\n",
    "    \"\"\" Performs the backward propagation through time to compute the gradients of the loss with respect\n",
    "    to the parameters. It returns also all the hidden states.\"\"\"\n",
    "    ...\n",
    "    return gradients, a\n",
    "\n",
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    \"\"\" Updates parameters using the Gradient Descent Update Rule.\"\"\"\n",
    "    ...\n",
    "    return parameters\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: optimize\n",
    "\n",
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
    "    \"\"\"\n",
    "    Execute one step of the optimization to train the model.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- list of integers, where each integer is a number that maps to a character in the vocabulary.\n",
    "    Y -- list of integers, exactly the same as X but shifted one index to the left.\n",
    "    a_prev -- previous hidden state.\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        b --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    learning_rate -- learning rate for the model.\n",
    "    \n",
    "    Returns:\n",
    "    loss -- value of the loss function (cross-entropy)\n",
    "    gradients -- python dictionary containing:\n",
    "                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
    "                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
    "                        dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)\n",
    "                        db -- Gradients of bias vector, of shape (n_a, 1)\n",
    "                        dby -- Gradients of output bias vector, of shape (n_y, 1)\n",
    "    a[len(X)-1] -- the last hidden state, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Forward propagate through time (â‰ˆ1 line)\n",
    "    loss, cache = None\n",
    "    \n",
    "    # Backpropagate through time (â‰ˆ1 line)\n",
    "    gradients, a = None\n",
    "    \n",
    "    # Clip your gradients between -5 (min) and 5 (max) (â‰ˆ1 line)\n",
    "    gradients = None\n",
    "    \n",
    "    # Update parameters (â‰ˆ1 line)\n",
    "    parameters = None\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return loss, gradients, a[len(X)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "vocab_size, n_a = 27, 100\n",
    "a_prev = np.random.randn(n_a, 1)\n",
    "Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
    "b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
    "X = [12,3,5,11,22,3]\n",
    "Y = [4,14,11,22,25, 26]\n",
    "\n",
    "loss, gradients, a_last = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\n",
    "print(\"Loss =\", loss)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"np.argmax(gradients[\\\"dWax\\\"]) =\", np.argmax(gradients[\"dWax\"]))\n",
    "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
    "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
    "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])\n",
    "print(\"a_last[4] =\", a_last[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected output:**\n",
    "\n",
    "<table>\n",
    "\n",
    "\n",
    "<tr>\n",
    "    <td> \n",
    "    **Loss **\n",
    "    </td>\n",
    "    <td> \n",
    "    126.503975722\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **gradients[\"dWaa\"][1][2]**\n",
    "    </td>\n",
    "    <td> \n",
    "    0.194709315347\n",
    "    </td>\n",
    "<tr>\n",
    "    <td> \n",
    "    **np.argmax(gradients[\"dWax\"])**\n",
    "    </td>\n",
    "    <td> 93\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **gradients[\"dWya\"][1][2]**\n",
    "    </td>\n",
    "    <td> -0.007773876032\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **gradients[\"db\"][4]**\n",
    "    </td>\n",
    "    <td> [-0.06809825]\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **gradients[\"dby\"][1]**\n",
    "    </td>\n",
    "    <td>[ 0.01538192]\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **a_last[4]**\n",
    "    </td>\n",
    "    <td> [-1.]\n",
    "    </td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.2 - Training the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the dataset of dinosaur names, we use each line of the dataset (one name) as one training example. Every 100 steps of stochastic gradient descent, you will sample 10 randomly chosen names to see how the algorithm is doing. Remember to shuffle the dataset, so that stochastic gradient descent visits the examples in random order. \n",
    "\n",
    "**Exercise**: Follow the instructions and implement `model()`. When `examples[index]` contains one dinosaur name (string), to create an example (X, Y), you can use this:\n",
    "```python\n",
    "        index = j % len(examples)\n",
    "        X = [None] + [char_to_ix[ch] for ch in examples[index]] \n",
    "        Y = X[1:] + [char_to_ix[\"\\n\"]]\n",
    "```\n",
    "Note that we use: `index= j % len(examples)`, where `j = 1....num_iterations`, to make sure that `examples[index]` is always a valid statement (`index` is smaller than `len(examples)`).\n",
    "The first entry of `X` being `None` will be interpreted by `rnn_forward()` as setting $x^{\\langle 0 \\rangle} = \\vec{0}$. Further, this ensures that `Y` is equal to `X` but shifted one step to the left, and with an additional \"\\n\" appended to signify the end of the dinosaur name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(data, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27):\n",
    "    \"\"\"\n",
    "    Trains the model and generates dinosaur names. \n",
    "    \n",
    "    Arguments:\n",
    "    data -- text corpus\n",
    "    ix_to_char -- dictionary that maps the index to a character\n",
    "    char_to_ix -- dictionary that maps a character to an index\n",
    "    num_iterations -- number of iterations to train the model for\n",
    "    n_a -- number of hidden neurons in the softmax layer\n",
    "    dino_names -- number of dinosaur names you want to sample at each iteration. \n",
    "    vocab_size -- number of unique characters found in the text, size of the vocabulary\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- learned parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve n_x and n_y from vocab_size\n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "    \n",
    "    # Initialize loss (this is required because we want to smooth our loss, don't worry about it)\n",
    "    loss = get_initial_loss(vocab_size, dino_names)\n",
    "    \n",
    "    # Build list of all dinosaur names (training examples).\n",
    "    with open(\"dinos.txt\") as f:\n",
    "        examples = f.readlines()\n",
    "    examples = [x.lower().strip() for x in examples]\n",
    "    \n",
    "    # Shuffle list of all dinosaur names\n",
    "    shuffle(examples)\n",
    "    \n",
    "    # Initialize the hidden state of your LSTM\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # Optimization loop\n",
    "    for j in range(num_iterations):\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        # Use the hint above to define one training example (X,Y) (â‰ˆ 2 lines)\n",
    "        index = None\n",
    "        X = None\n",
    "        Y = None\n",
    "        \n",
    "        # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters\n",
    "        # Choose a learning rate of 0.01\n",
    "        curr_loss, gradients, a_prev = None\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Use a latency trick to keep the loss smooth. It happens here to accelerate the training.\n",
    "        loss = smooth(loss, curr_loss)\n",
    "\n",
    "        # Every 2000 Iteration, generate \"n\" characters thanks to sample() to check if the model is learning properly\n",
    "        if j % 2000 == 0:\n",
    "            \n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "            \n",
    "            # The number of dinosaur names to print\n",
    "            seed = 0\n",
    "            for name in range(dino_names):\n",
    "                \n",
    "                # Sample indexes and print them\n",
    "                sampled_indexes = sample(parameters, char_to_ix, seed)\n",
    "                print_sample(sampled_indexes, ix_to_char)\n",
    "                \n",
    "                seed += 1  # To get the same result for grading purposed, increment the seed by one. \n",
    "      \n",
    "            print('\\n')\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell, you should observe your model outputting random-looking characters at the first iteration. After a few thousand iterations, your model should learn to generate reasonable-looking names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = model(data, ix_to_char, char_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "You can see that your algorithm has started to generate plausible dinosaur names towards the end of the training. At first, it was generating random characters, but towards the end you could see dinosaur names with cool endings. Feel free to run the algorithm even longer and play with hyperparameters to see if you can get even better results. Our implemetation generated some really cool names like `maconucon`, `marloralus` and `macingsersaurus`. Your model hopefully also learned that dinosaur names tend to end in `saurus`, `don`, `aura`, `tor`, etc.\n",
    "\n",
    "If your model generates some non-cool names, don't blame the model entirely--not all actual dinosaur names sound cool. (For example, `dromaeosauroides` is an actual dinosaur name and is in the training set.) But this model should give you a set of candidates from which you can pick the coolest! \n",
    "\n",
    "This assignment had used a relatively small dataset, so that you could train an RNN quickly on a CPU. Training a model of the english language requires a much bigger dataset, and usually needs much more computation, and could run for many hours on GPUs. We ran our dinosaur name for quite some time, and so far our favoriate name is the great, undefeatable, and fierce: Mangosaurus!\n",
    "\n",
    "<img src=\"images/mangosaurus.jpeg\" style=\"width:250;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Writing like Shakespeare\n",
    "\n",
    "The rest of this notebook is optional and is not graded, but we hope you'll do it anyway since it's quite fun and informative. \n",
    "\n",
    "A similar (but more complicated) task is to generate Shakespeare poems. Instead of learning from a dataset of Dinosaur names you can use a collection of Shakespearian poems. Using LSTM cells, you can learn longer term dependencies that span many characters in the text--e.g., where a character appearing somewhere a sequence can influence what should be a different character much much later in ths sequence. These long term dependencies were less important with dinosaur names, since the names were quite short. \n",
    "\n",
    "\n",
    "<img src=\"images/shakespeare.jpg\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Let's become poets! </center></caption>\n",
    "\n",
    "We have implemented a Shakespeare poem generator with Keras. Run the following cell to load the required packages and models. This may take a few minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Masking\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from shakespeare_utils import *\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save you some time, we have already trained a model for ~1000 epochs on a collection of Shakespearian poems called [*\"The Sonnets\"*](shakespeare.txt). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model for one more epoch. When it finishes training for an epoch---this will also take a few minutes---you can run `generate_output`, which will prompt asking you for an input (`<`40 characters). The poem will start with your sentence, and our RNN-Shakespeare will complete the rest of the poem for you! For example, try \"Forsooth this maketh no sense \" (don't enter the quotation marks). Depending on whether you include the space at the end, your results might also differ--try it both ways, and try other inputs as well. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y, batch_size=128, epochs=1, callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this cell to try with different inputs without having to re-train the model \n",
    "generate_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is EA07-3B73\n",
      "\n",
      " Directory of C:\\Users\\Azhar\n",
      "\n",
      "09/13/2020  11:49 AM    <DIR>          .\n",
      "09/13/2020  11:49 AM    <DIR>          ..\n",
      "01/01/2020  02:16 PM    <DIR>          .anaconda\n",
      "08/31/2019  09:19 AM    <DIR>          .android\n",
      "01/14/2020  07:09 PM    <DIR>          .astropy\n",
      "02/22/2019  12:03 AM    <DIR>          .atom\n",
      "09/25/2019  12:08 AM               126 .defaults-0.1.0.ini\n",
      "11/16/2018  05:44 PM    <DIR>          .eclipse\n",
      "01/24/2018  01:54 AM    <DIR>          .idlerc\n",
      "09/13/2020  11:32 AM    <DIR>          .ipynb_checkpoints\n",
      "01/24/2018  01:57 AM                91 .irbrc\n",
      "07/19/2019  03:22 PM    <DIR>          .ivy2\n",
      "03/02/2018  10:05 PM    <DIR>          .jmc\n",
      "01/24/2019  05:46 PM    <DIR>          .liclipse\n",
      "11/15/2018  09:51 PM    <DIR>          .nbi\n",
      "02/22/2019  12:00 AM    <DIR>          .node-gyp\n",
      "03/22/2018  02:53 PM    <DIR>          .oracle_jre_usage\n",
      "09/02/2019  04:25 PM    <DIR>          .p2\n",
      "01/24/2018  01:42 AM                 0 .perlcriticrc\n",
      "06/06/2020  04:05 PM    <DIR>          .PyCharm2019.3\n",
      "06/26/2019  12:23 PM    <DIR>          .RapidMiner\n",
      "07/19/2019  03:23 PM    <DIR>          .sbt\n",
      "07/19/2019  12:29 PM    <DIR>          .scalaide\n",
      "04/23/2019  03:43 PM    <DIR>          .spss\n",
      "01/01/2020  02:15 PM    <DIR>          .spyder-py3\n",
      "01/24/2019  01:35 PM    <DIR>          .sqldeveloper\n",
      "11/15/2018  10:34 PM    <DIR>          .tooling\n",
      "01/07/2019  07:01 PM    <DIR>          .VirtualBox\n",
      "12/02/2019  03:13 PM    <DIR>          .vscode\n",
      "01/26/2020  03:56 PM    <DIR>          2015lab4-master\n",
      "01/03/2020  03:54 PM             1,480 a.txt\n",
      "01/06/2020  04:47 PM               459 ab.txt\n",
      "07/31/2020  01:51 AM            20,472 Access DB2 on Cloud using Python.ipynb\n",
      "09/13/2020  10:26 AM           769,692 Art+Generation+with+Neural+Style+Transfer+-+v2.ipynb\n",
      "07/21/2020  09:21 PM         1,024,850 Assesment.ipynb\n",
      "09/13/2020  10:22 AM            69,649 Autonomous+driving+application+-+Car+detection+-+v3.ipynb\n",
      "02/07/2020  03:51 PM    <DIR>          azh\n",
      "02/06/2020  05:58 PM             2,453 Azharopencv_notebook.ipynb\n",
      "02/04/2020  05:52 PM            46,400 banknote.txt\n",
      "02/04/2020  07:33 PM            58,050 banknotes.csv\n",
      "01/13/2020  12:54 PM             7,294 Basic Python.ipynb\n",
      "02/27/2020  04:46 PM           154,046 bcdata.csv\n",
      "02/15/2020  04:25 PM            26,737 Boston_house_Ranforest.ipynb\n",
      "02/27/2020  04:48 PM           140,124 Breast_cancer_cleandata.csv\n",
      "02/15/2020  11:45 AM            45,362 Breast_cancer_detectionRanForest.ipynb\n",
      "11/13/2007  12:31 PM            22,016 broiler.xls\n",
      "09/13/2020  10:52 AM            81,114 Building+a+Recurrent+Neural+Network+-+Step+by+Step+-+v3.ipynb\n",
      "09/12/2020  05:30 PM            56,045 Building+your+Deep+Neural+Network+-+Step+by+Step+v8.ipynb\n",
      "08/10/2020  05:54 PM             1,651 capstone_project.ipynb\n",
      "02/27/2020  04:05 PM             2,377 Confusion_matrix.ipynb\n",
      "01/02/2018  04:31 PM    <DIR>          Contacts\n",
      "09/13/2020  09:38 AM            55,146 Convolution+model+-+Application+-+v1.ipynb\n",
      "09/13/2020  09:38 AM            58,078 Convolution+model+-+Step+by+Step+-+v2.ipynb\n",
      "08/03/2020  01:53 AM           123,936 DA0101EN-Review-Data-Wrangling.ipynb\n",
      "08/03/2020  03:37 AM           302,173 DA0101EN-Review-Exploratory-Data-Analysis.ipynb\n",
      "08/03/2020  12:21 AM            35,971 DA0101EN-Review-Introduction.ipynb\n",
      "08/03/2020  04:53 AM           216,493 DA0101EN-Review-Model-Development.ipynb\n",
      "08/03/2020  04:04 PM            41,394 DA0101EN-Review-Model-Evaluation-and-Refinement.ipynb\n",
      "02/06/2020  05:56 PM    <DIR>          data\n",
      "08/02/2020  11:15 PM            17,328 datasets.ipynb.txt\n",
      "07/31/2020  01:35 AM            14,969 DB0201EN-Week3-1-2-Querying-v4-py.ipynb\n",
      "07/31/2020  01:53 AM            12,155 DB0201EN-Week3-1-3-SQLmagic-v3-py.ipynb\n",
      "07/31/2020  04:24 AM            20,284 DB0201EN-Week4-1-1-RealDataPractice-v4-py.ipynb\n",
      "07/31/2020  04:29 AM            11,428 DB0201EN-Week4-2-2-PeerAssign-v5-py.ipynb\n",
      "09/12/2020  05:38 PM           310,076 Deep+Neural+Network+-+Application+v8.ipynb\n",
      "09/05/2020  01:03 PM    <DIR>          Desktop\n",
      "02/04/2020  04:49 PM                 0 diabetes.png\n",
      "09/13/2020  11:18 AM            37,650 Dinosaurus+Island+--+Character-level+language+model+-+%28final%29+-+learners.ipynb\n",
      "09/07/2020  09:02 PM    <DIR>          Documents\n",
      "09/13/2020  01:33 PM    <DIR>          Downloads\n",
      "08/03/2020  09:10 PM            46,832 DV0101EN-1-1-1-Introduction-to-Matplotlib-and-Line-Plots-py-v2.0.ipynb\n",
      "08/03/2020  09:34 PM            52,664 DV0101EN-3-4-1-Waffle-Charts-Word-Clouds-and-Regression-Plots-py-v2.0.ipynb\n",
      "08/04/2020  11:40 AM            58,138 DV0101EN-3-5-1-Generating-Maps-in-Python-py-v2.0.ipynb\n",
      "08/06/2019  09:54 AM    <DIR>          eclipse\n",
      "09/13/2020  11:13 AM            68,385 Emojify+-+v2.ipynb\n",
      "10/04/2019  09:04 PM               335 employee.csv\n",
      "09/13/2020  10:33 AM            31,566 Face+Recognition+for+the+Happy+House+-+v3.ipynb\n",
      "01/02/2018  04:31 PM    <DIR>          Favorites\n",
      "06/26/2019  02:37 PM                25 File.dat\n",
      "06/23/2019  02:40 PM                 5 file.txt\n",
      "09/10/2019  01:53 PM                17 file_path.LST\n",
      "08/10/2020  03:57 PM            35,378 Final Project Notebook.ipynb\n",
      "08/10/2020  03:55 PM         1,725,350 Final Project.ipynb.txt\n",
      "07/17/2019  08:08 PM    <DIR>          Focus Training python\n",
      "03/03/2019  06:23 AM            31,858 friends.jpg\n",
      "01/03/2020  03:31 PM             3,947 Generators.ipynb\n",
      "09/12/2020  07:55 PM            27,110 Gradient+Checking+v1.ipynb\n",
      "02/14/2020  08:39 PM            24,812 Handwritten Digit Classification.ipynb\n",
      "08/03/2020  05:36 PM            43,028 House Sales_in_King_Count_USA.ipynb\n",
      "10/29/2019  12:21 AM               966 How to download youtube videos using Python.ipynb\n",
      "11/29/2018  04:47 PM    <DIR>          IdeaProjects\n",
      "02/05/2020  05:37 PM           186,458 image.png\n",
      "09/13/2020  11:04 AM         1,797,460 Improvise+a+Jazz+Solo+with+an+LSTM+Network+-+v3.ipynb\n",
      "09/12/2020  07:44 PM           213,114 Initialization.ipynb\n",
      "12/18/2019  12:17 PM    <DIR>          Installed lubuntu\n",
      "09/26/2019  03:04 PM             4,073 isinstance Function.ipynb\n",
      "03/09/2019  01:07 PM    <DIR>          Java Workspace\n",
      "08/04/2020  01:42 PM     1,706,054,661 java_error_in_pycharm.hprof\n",
      "01/26/2020  12:09 PM            28,130 java_error_in_pycharm_5820.log\n",
      "03/27/2020  08:20 PM             1,982 kmeans_clustering_tkinter.py\n",
      "04/25/2020  12:26 AM            79,027 kmeans_iris_clustering.ipynb\n",
      "02/21/2020  07:32 PM             9,140 Knn-iris-classifier.ipynb\n",
      "01/02/2018  04:31 PM    <DIR>          Links\n",
      "01/30/2020  09:57 PM           305,481 LinReg_withLosfuncInJupyterNB.ipynb\n",
      "03/10/2020  02:59 PM            70,000 Lin_reg_3methods_visuals.ipynb\n",
      "08/06/2019  11:05 AM             2,141 Login prompt.ipynb\n",
      "09/12/2020  04:45 PM           199,026 Logistic+Regression+with+a+Neural+Network+mindset+v5.ipynb\n",
      "02/06/2020  02:15 PM             8,726 machine.data\n",
      "08/08/2020  07:07 PM            31,026 ML0101EN-Proj-Loan-py-v1.ipynb\n",
      "12/06/2019  12:33 AM             4,334 Mobile Carrier Finder.ipynb\n",
      "01/05/2018  10:14 AM    <DIR>          Music\n",
      "07/27/2020  04:03 AM             3,073 My Notebook.ipynb\n",
      "09/11/2019  10:59 AM               430 myfile2.csv\n",
      "09/13/2020  11:30 AM            52,364 Neural+machine+translation+with+attention+-+v4.ipynb\n",
      "09/21/2019  10:01 PM            18,467 numpy I.ipynb\n",
      "09/25/2019  12:23 AM            12,021 numpy session 2 .ipynb\n",
      "08/07/2020  07:32 PM            64,123 Numpy_Exercise_workbook.ipynb.txt\n",
      "02/06/2020  07:56 PM    <DIR>          OpenCV\n",
      "07/03/2020  11:50 PM            18,385 opencv.ipynb\n",
      "09/13/2020  11:07 AM            30,312 Operations+on+word+vectors+-+v2.ipynb\n",
      "09/13/2020  08:27 AM            61,284 Optimization+methods.ipynb\n",
      "05/22/2019  04:08 PM    <DIR>          Oracle\n",
      "10/15/2019  03:14 PM             4,186 Pandas Exercise Workbook.ipynb\n",
      "10/04/2019  09:04 PM            35,746 pandas+1 (1).ipynb\n",
      "10/02/2019  11:18 AM           101,752 pandas_3 Query like operations.ipynb\n",
      "01/02/2018  04:31 PM    <DIR>          Pictures\n",
      "10/28/2017  04:37 AM         8,726,035 Pink Panther Maplestory's 1-Minute Introduction-qiZkQlt5xlo.mp4\n",
      "09/12/2020  05:09 PM           312,713 Planar+data+classification+with+one+hidden+layer+v5.ipynb\n",
      "08/17/2020  02:53 PM    <DIR>          PycharmProjects\n",
      "07/13/2019  12:18 PM                 0 python\n",
      "02/15/2020  12:53 PM            21,210 Ra.ipynb\n",
      "02/13/2020  12:45 PM            29,493 RandomForest_irisDS.ipynb\n",
      "02/15/2020  11:41 AM            45,672 random_iris_visualizattion2.ipynb\n",
      "09/21/2019  09:32 AM                17 README.md\n",
      "09/12/2020  07:50 PM           273,585 Regularization+-+v2.ipynb\n",
      "09/13/2020  10:01 AM            48,373 Residual+Networks+-+v2.ipynb\n",
      "10/02/2019  12:47 PM             8,848 sales.csv.xlsx\n",
      "10/02/2019  12:49 PM               592 sales2.csv\n",
      "10/04/2019  09:43 PM               592 sales99.csv\n",
      "03/11/2018  03:30 PM    <DIR>          Saved Games\n",
      "08/31/2019  10:02 AM    <DIR>          Searches\n",
      "08/12/2020  12:24 PM             9,856 Segmenting-and-Clustering-Neighborhoods-in-Toronto.ipynb\n",
      "08/12/2020  01:28 PM           224,938 Segmenting-and-Clustering-Neighborhoods-in-Toronto2.ipynb\n",
      "03/06/2020  10:33 PM            10,941 SimpleLinReg_Boston_stats_modelapi.py.ipynb\n",
      "03/10/2020  06:15 PM            31,138 Simple_linReg.ipynb\n",
      "03/27/2020  07:58 PM               139 sys_restart.py\n",
      "03/27/2020  08:02 PM               293 sys_restart_options.py\n",
      "03/27/2020  07:59 PM               141 sys_shutdown.py\n",
      "09/13/2020  08:38 AM            81,079 Tensorflow+Tutorial.ipynb\n",
      "08/08/2020  03:48 PM            63,794 test_notebook_final.ipynb\n",
      "04/25/2020  01:53 AM            63,499 Titanic_Kmeans.ipynb\n",
      "08/12/2020  12:45 PM             7,590 toronto.csv\n",
      "09/27/2019  12:04 AM            61,194 train.csv\n",
      "09/13/2020  11:49 AM         9,588,823 Trigger+word+detection+-+v1.ipynb\n",
      "09/10/2019  01:23 PM    <DIR>          Untitled Folder 1\n",
      "10/05/2019  09:27 PM            24,152 Untitled.ipynb\n",
      "06/26/2019  04:52 PM             1,780 Untitled1.ipynb\n",
      "06/29/2019  02:56 PM             1,869 Untitled10.ipynb\n",
      "08/13/2020  08:49 PM            16,903 Untitled100.ipynb\n",
      "08/26/2020  03:22 PM            23,311 Untitled101.ipynb\n",
      "09/09/2020  04:14 PM            12,743 Untitled102.ipynb\n",
      "09/12/2020  03:49 PM               555 Untitled103.ipynb\n",
      "07/02/2019  01:04 PM             1,256 Untitled11.ipynb\n",
      "07/02/2019  05:01 PM             2,339 Untitled12.ipynb\n",
      "07/02/2019  06:53 PM             2,422 Untitled13.ipynb\n",
      "07/03/2019  12:48 PM             1,398 Untitled14.ipynb\n",
      "07/03/2019  04:21 PM             1,740 Untitled15.ipynb\n",
      "07/03/2019  06:55 PM               904 Untitled16.ipynb\n",
      "07/04/2019  01:08 PM             5,385 Untitled17.ipynb\n",
      "07/04/2019  03:26 PM             2,006 Untitled18.ipynb\n",
      "07/05/2019  12:41 PM             4,133 Untitled19.ipynb\n",
      "06/26/2019  06:25 PM             1,304 Untitled2.ipynb\n",
      "07/06/2019  12:42 PM             2,346 Untitled20.ipynb\n",
      "07/06/2019  04:39 PM             6,856 Untitled21.ipynb\n",
      "07/15/2019  12:31 AM            15,607 Untitled22.ipynb\n",
      "07/15/2019  04:46 PM             1,251 Untitled23.ipynb\n",
      "07/16/2019  03:26 PM             1,822 Untitled24.ipynb\n",
      "07/16/2019  04:28 PM             3,348 Untitled25.ipynb\n",
      "07/17/2019  03:05 PM             1,330 Untitled26.ipynb\n",
      "08/06/2019  09:53 AM                72 Untitled27.ipynb\n",
      "09/10/2019  12:59 PM                72 Untitled28.ipynb\n",
      "09/10/2019  01:00 PM                72 Untitled29.ipynb\n",
      "06/27/2019  12:20 PM             2,596 Untitled3.ipynb\n",
      "09/10/2019  01:01 PM                72 Untitled30.ipynb\n",
      "09/25/2019  12:22 AM                72 Untitled31.ipynb\n",
      "10/02/2019  01:30 PM            25,039 Untitled32.ipynb\n",
      "10/14/2019  04:03 PM               989 Untitled33.ipynb\n",
      "10/15/2019  11:54 PM           126,395 Untitled34.ipynb\n",
      "12/09/2019  11:56 AM             7,742 Untitled35.ipynb\n",
      "12/10/2019  03:27 PM             9,758 Untitled36.ipynb\n",
      "12/09/2019  11:57 AM               555 Untitled37.ipynb\n",
      "01/03/2020  03:31 PM             3,947 Untitled38.ipynb\n",
      "01/03/2020  04:04 PM               888 Untitled39.ipynb\n",
      "06/27/2019  02:51 PM             1,759 Untitled4.ipynb\n",
      "01/05/2020  02:03 PM               555 Untitled40.ipynb\n",
      "01/06/2020  02:36 PM             7,337 Untitled41.ipynb\n",
      "01/07/2020  04:20 PM           201,069 Untitled42.ipynb\n",
      "01/07/2020  08:29 PM               555 Untitled43.ipynb\n",
      "01/08/2020  05:28 PM             3,854 Untitled44.ipynb\n",
      "01/09/2020  02:11 PM             3,509 Untitled45.ipynb\n",
      "01/14/2020  06:59 PM             1,199 Untitled46.ipynb\n",
      "01/23/2020  02:23 PM            17,327 Untitled47.ipynb\n",
      "01/27/2020  02:08 PM             1,878 Untitled48.ipynb\n",
      "01/30/2020  03:30 PM           279,096 Untitled49.ipynb\n",
      "06/27/2019  03:09 PM               945 Untitled5.ipynb\n",
      "02/01/2020  12:59 PM            13,041 Untitled50.ipynb\n",
      "02/02/2020  03:05 PM               555 Untitled51.ipynb\n",
      "02/03/2020  02:01 PM               555 Untitled52.ipynb\n",
      "02/03/2020  09:48 PM             5,784 Untitled53.ipynb\n",
      "02/04/2020  09:00 PM            11,042 Untitled54.ipynb\n",
      "02/05/2020  05:34 PM             1,917 Untitled55.ipynb\n",
      "02/05/2020  07:43 PM               761 Untitled56.ipynb\n",
      "02/08/2020  02:04 PM             1,351 Untitled57.ipynb\n",
      "02/09/2020  05:54 PM             2,672 Untitled58.ipynb\n",
      "02/14/2020  08:42 PM            10,356 Untitled59.ipynb\n",
      "06/27/2019  04:58 PM             1,510 Untitled6.ipynb\n",
      "02/16/2020  04:05 PM           198,008 Untitled60.ipynb\n",
      "02/16/2020  04:17 PM             3,422 Untitled61.ipynb\n",
      "02/16/2020  05:12 PM             8,659 Untitled62.ipynb\n",
      "02/17/2020  01:25 PM             5,035 Untitled63.ipynb\n",
      "02/17/2020  03:08 PM             4,180 Untitled64.ipynb\n",
      "02/19/2020  04:30 PM               555 Untitled65.ipynb\n",
      "02/20/2020  09:45 AM               555 Untitled66.ipynb\n",
      "02/21/2020  09:55 PM            15,545 Untitled67.ipynb\n",
      "02/21/2020  10:59 PM            52,661 Untitled68.ipynb\n",
      "02/22/2020  11:54 AM             8,010 Untitled69.ipynb\n",
      "09/16/2019  03:15 PM               835 Untitled7.ipynb\n",
      "02/26/2020  05:51 PM             7,081 Untitled70.ipynb\n",
      "02/27/2020  05:46 PM            42,762 Untitled71.ipynb\n",
      "03/01/2020  02:48 PM             1,145 Untitled72.ipynb\n",
      "03/16/2020  04:53 PM             5,635 Untitled73.ipynb\n",
      "03/04/2020  07:35 PM            13,531 Untitled74.ipynb\n",
      "03/06/2020  02:37 PM             7,309 Untitled75.ipynb\n",
      "03/06/2020  02:23 PM               555 Untitled76.ipynb\n",
      "03/06/2020  02:23 PM               555 Untitled77.ipynb\n",
      "03/27/2020  09:11 PM            45,258 Untitled78.ipynb\n",
      "04/14/2020  03:51 PM               555 Untitled79.ipynb\n",
      "06/28/2019  04:00 PM             2,483 Untitled8.ipynb\n",
      "04/25/2020  11:23 PM               555 Untitled80.ipynb\n",
      "05/09/2020  02:08 AM             9,496 Untitled81.ipynb\n",
      "05/15/2020  11:38 PM               555 Untitled82.ipynb\n",
      "05/21/2020  02:31 AM             1,529 Untitled83.ipynb\n",
      "06/04/2020  11:07 PM               555 Untitled84.ipynb\n",
      "06/07/2020  12:21 AM             1,281 Untitled85.ipynb\n",
      "06/23/2020  01:25 AM             2,404 Untitled86.ipynb\n",
      "07/01/2020  11:54 PM               555 Untitled87.ipynb\n",
      "07/02/2020  02:25 PM               555 Untitled88.ipynb\n",
      "08/12/2020  05:19 PM             3,516 Untitled89.ipynb\n",
      "06/29/2019  01:02 PM                72 Untitled9.ipynb\n",
      "07/10/2020  03:54 AM             2,351 Untitled90.ipynb\n",
      "07/13/2020  12:28 AM               555 Untitled91.ipynb\n",
      "07/24/2020  12:10 AM               555 Untitled92.ipynb\n",
      "07/28/2020  04:32 PM               555 Untitled93.ipynb\n",
      "07/31/2020  01:47 AM               917 Untitled94.ipynb\n",
      "08/03/2020  10:49 PM            80,613 Untitled95.ipynb\n",
      "08/04/2020  11:32 AM               555 Untitled96.ipynb\n",
      "08/07/2020  06:22 PM            49,301 Untitled97.ipynb\n",
      "08/07/2020  08:47 PM            25,484 Untitled98.ipynb\n",
      "08/13/2020  07:21 PM            54,317 Untitled99.ipynb\n",
      "09/18/2019  03:15 PM    <DIR>          Videos\n",
      "01/07/2019  05:05 PM    <DIR>          VirtualBox VMs\n",
      "             211 File(s)  1,736,489,728 bytes\n",
      "              50 Dir(s)   3,296,092,160 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RNN-Shakespeare model is very similar to the one you have built for dinosaur names. The only major differences are:\n",
    "- LSTMs instead of the basic RNN to capture longer-range dependencies\n",
    "- The model is a deeper, stacked LSTM model (2 layer)\n",
    "- Using Keras instead of python to simplify the code \n",
    "\n",
    "If you want to learn more, you can also check out the Keras Team's text generation implementation on GitHub: https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py.\n",
    "\n",
    "Congratulations on finishing this notebook! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**:\n",
    "- This exercise took inspiration from Andrej Karpathy's implementation: https://gist.github.com/karpathy/d4dee566867f8291f086. To learn more about text generation, also check out Karpathy's [blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).\n",
    "- For the Shakespearian poem generator, our implementation was based on the implementation of an LSTM text generator by the Keras team: https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
